{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shehabahmed74/graduation-project-2-ver-3-yolo-11-vs-yolo-8-fa?scriptVersionId=230040489\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"cf2ad1f7","metadata":{"id":"KHc6q_w3r1sb","papermill":{"duration":0.010527,"end_time":"2025-03-27T15:45:19.147297","exception":false,"start_time":"2025-03-27T15:45:19.13677","status":"completed"},"tags":[]},"source":["# Graduation Project 2: YOLOv8 vs. YOLOv11 Facial Recognition Demo\n","\n","This notebook implements a Streamlit app to compare YOLOv8 and YOLOv11 for facial recognition. The project includes the following steps:\n","- **Step 1:** Install dependencies and verify GPU availability.\n","- **Step 2:** Fine-tune YOLOv11 on a facial recognition dataset.\n","- **Step 3:** Set up and deploy a Streamlit app to perform facial recognition and compare YOLOv8 and YOLOv11.\n","- **Step 4:** Test the app and verify the results.\n","- **Step 5:** Summarize the results and provide the public URL for the app.\n","\n","The dataset used is `shehabahmed74/shehab-data-facial-recognition`, which contains three folders: `train`, `valid`, and `test`, each with `images` and `labels` subfolders. We will use `train` for training, `valid` for validation, and `test` for testing in the Streamlit app.\n","\n","The notebook is structured to be professional and compatible with GitHub for project delivery.\n","\n","**Execution Time Estimate:**\n","- With GPU: ~15–25 minutes (fine-tuning takes ~10–20 minutes for 10 epochs).\n","- Without GPU (on CPU): ~30–60 minutes (fine-tuning takes ~20–40 minutes for 10 epochs).\n","- To optimize performance, ensure a GPU runtime is selected (see Step 1.2).\n","\n","---\n","\n","## Step 1: Install Dependencies\n","\n","This cell installs the required Python packages for the project, including Streamlit, ngrok, Ultralytics (for YOLOv11), and other dependencies.\n","\n","**Instructions:**\n","- Run this cell to install the dependencies.\n","- If you encounter version conflicts, you may need to restart the runtime (`Runtime > Restart runtime`) and rerun this cell."]},{"cell_type":"code","execution_count":1,"id":"99921ac9","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:19.16686Z","iopub.status.busy":"2025-03-27T15:45:19.166548Z","iopub.status.idle":"2025-03-27T15:45:32.847335Z","shell.execute_reply":"2025-03-27T15:45:32.846256Z"},"id":"a-mBpGpkr5bC","outputId":"63a93e30-6b2a-48cb-c7bc-ae7b1f49d41a","papermill":{"duration":13.692108,"end_time":"2025-03-27T15:45:32.848808","exception":false,"start_time":"2025-03-27T15:45:19.1567","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDependencies installed successfully!\n"]}],"source":["# Install dependencies\n","!pip install streamlit -q\n","!pip install pyngrok -q\n","!pip install ultralytics seaborn matplotlib pandas scipy kaggle pillow -q\n","print(\"Dependencies installed successfully!\")"]},{"cell_type":"code","execution_count":2,"id":"16c6f34b","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:32.869317Z","iopub.status.busy":"2025-03-27T15:45:32.869052Z","iopub.status.idle":"2025-03-27T15:45:36.214887Z","shell.execute_reply":"2025-03-27T15:45:36.213875Z"},"id":"hlaJyjtFb2YK","outputId":"82bfe61d-8a81-4e7a-ff04-515ce06f991a","papermill":{"duration":3.357423,"end_time":"2025-03-27T15:45:36.216279","exception":false,"start_time":"2025-03-27T15:45:32.858856","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Upgrading pyngrok...\n","pyngrok upgraded successfully!\n"]}],"source":["# Upgrade pyngrok to the latest version using subprocess\n","import subprocess\n","import os\n","\n","# Set the environment for the subprocess\n","env = os.environ.copy()\n","env[\"LC_ALL\"] = \"C.UTF-8\"\n","env[\"LANG\"] = \"C.UTF-8\"\n","env[\"LANGUAGE\"] = \"C.UTF-8\"\n","\n","print(\"Upgrading pyngrok...\")\n","try:\n","    # Use subprocess to run the pip install command\n","    process = subprocess.run(\n","        [\"pip\", \"install\", \"--upgrade\", \"pyngrok\", \"-q\"],\n","        env=env,\n","        stdout=subprocess.PIPE,\n","        stderr=subprocess.PIPE,\n","        text=True\n","    )\n","    # Check if the command was successful\n","    if process.returncode == 0:\n","        print(\"pyngrok upgraded successfully!\")\n","    else:\n","        print(\"Failed to upgrade pyngrok. Error output:\")\n","        print(process.stderr)\n","        raise RuntimeError(\"pyngrok upgrade failed.\")\n","except Exception as e:\n","    print(f\"Error upgrading pyngrok: {str(e)}\")\n","    raise"]},{"cell_type":"code","execution_count":3,"id":"076483ed","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:36.236778Z","iopub.status.busy":"2025-03-27T15:45:36.236521Z","iopub.status.idle":"2025-03-27T15:45:39.664631Z","shell.execute_reply":"2025-03-27T15:45:39.663613Z"},"id":"hLbT3ayZ4_m7","outputId":"efafadd2-90d0-43a1-9a2a-84bcd0acd4b5","papermill":{"duration":3.440437,"end_time":"2025-03-27T15:45:39.666896","exception":false,"start_time":"2025-03-27T15:45:36.226459","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.44.0)\r\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.3)\r\n","Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.97)\r\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\r\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\r\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\r\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\r\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\r\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\r\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\r\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.3)\r\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\r\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\r\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (19.0.1)\r\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\r\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\r\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\r\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\r\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\r\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\r\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\r\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\r\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\r\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\r\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\r\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\r\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\r\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\r\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\r\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2025.1.31)\r\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.9.0.post0)\r\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\r\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.3.0)\r\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\r\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\r\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\r\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\r\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\r\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\r\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\r\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\r\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2025.0.1)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2022.0.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\r\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\r\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\r\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\r\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\r\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\r\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\r\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\r\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\r\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\r\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\r\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\r\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.0.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\r\n"]}],"source":["# Install dependencies\n","!pip install streamlit pyngrok ultralytics kaggle"]},{"cell_type":"markdown","id":"de4bdabc","metadata":{"id":"KzF5bm-Lr7cK","papermill":{"duration":0.009856,"end_time":"2025-03-27T15:45:39.687325","exception":false,"start_time":"2025-03-27T15:45:39.677469","status":"completed"},"tags":[]},"source":["## Step 1.1: Verify Installed Versions\n","\n","This cell checks the installed versions of key packages to ensure compatibility.\n","\n","**Instructions:**\n","- Run this cell to verify the versions of NumPy and Ultralytics.\n","- Expected versions (approximate):\n","  - NumPy: 2.1.x\n","  - Ultralytics: 8.3.x\n","- If there are compatibility issues, you may need to pin specific versions in the previous cell (e.g., `numpy==1.26.4 ultralytics==8.3.92`)."]},{"cell_type":"code","execution_count":4,"id":"1c3409ba","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:39.708133Z","iopub.status.busy":"2025-03-27T15:45:39.707889Z","iopub.status.idle":"2025-03-27T15:45:45.404Z","shell.execute_reply":"2025-03-27T15:45:45.403094Z"},"id":"ayaY456ar-Za","outputId":"8718a40f-85c5-4c3b-e1c2-3dde9a57eb67","papermill":{"duration":5.708121,"end_time":"2025-03-27T15:45:45.405343","exception":false,"start_time":"2025-03-27T15:45:39.697222","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","NumPy version: 1.26.4\n","Ultralytics version: 8.3.97\n"]}],"source":["# Check installed versions\n","import numpy\n","import ultralytics\n","print(f\"NumPy version: {numpy.__version__}\")\n","print(f\"Ultralytics version: {ultralytics.__version__}\")"]},{"cell_type":"markdown","id":"6e7669d8","metadata":{"id":"6pukWCJOsAWi","papermill":{"duration":0.009788,"end_time":"2025-03-27T15:45:45.425847","exception":false,"start_time":"2025-03-27T15:45:45.416059","status":"completed"},"tags":[]},"source":["## Step 1.2: Verify GPU Availability\n","\n","This cell checks if a GPU is available, as fine-tuning YOLOv11 is significantly faster on a GPU.\n","\n","**Instructions:**\n","- Run this cell to verify GPU availability.\n","- If the output shows `CUDA available: True`, a GPU is available, and you can proceed.\n","- If the output shows `CUDA available: False`, switch to a GPU runtime:\n","  - Go to `Runtime > Change runtime type`.\n","  - Select `GPU` under \"Hardware accelerator\" (e.g., T4 GPU).\n","  - Click `Save` and restart the runtime (`Runtime > Restart runtime`).\n","  - Rerun all cells above this one.\n","- **Note:** If a GPU is not available, the notebook will fall back to CPU, but fine-tuning will be slower (~20–40 minutes for 10 epochs vs. ~10–20 minutes on a GPU)."]},{"cell_type":"code","execution_count":5,"id":"66c21533","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:45.446794Z","iopub.status.busy":"2025-03-27T15:45:45.446323Z","iopub.status.idle":"2025-03-27T15:45:45.539828Z","shell.execute_reply":"2025-03-27T15:45:45.53885Z"},"id":"5Il5InHDsCE7","outputId":"6629d2a3-6c9f-45a1-f35f-7ec825876424","papermill":{"duration":0.105418,"end_time":"2025-03-27T15:45:45.54113","exception":false,"start_time":"2025-03-27T15:45:45.435712","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA available: True\n","Number of GPUs: 1\n","GPU Name: Tesla P100-PCIE-16GB\n"]}],"source":["# Verify GPU availability\n","import torch\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"WARNING: No GPU detected. Fine-tuning will be slower on CPU. See instructions above to switch to a GPU runtime.\")"]},{"cell_type":"markdown","id":"61fa2cef","metadata":{"id":"RfR9DrI4sDsC","papermill":{"duration":0.010198,"end_time":"2025-03-27T15:45:45.561859","exception":false,"start_time":"2025-03-27T15:45:45.551661","status":"completed"},"tags":[]},"source":["## Step 1.3: Set Up ngrok Authtoken\n","\n","This cell sets up ngrok to create a public URL for the Streamlit app.\n","\n","**Instructions:**\n","- The ngrok authtoken is already provided.\n","- Run this cell to set up ngrok.\n","- If you encounter issues with the authtoken, ensure it is valid by checking your ngrok dashboard at [ngrok.com](https://ngrok.com)."]},{"cell_type":"code","execution_count":6,"id":"43b2cf8f","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:45.583298Z","iopub.status.busy":"2025-03-27T15:45:45.58304Z","iopub.status.idle":"2025-03-27T15:45:56.194157Z","shell.execute_reply":"2025-03-27T15:45:56.193013Z"},"id":"418wTmFisFPC","outputId":"1e4ca1fb-f20a-41b5-a1bf-ae5e909a6be0","papermill":{"duration":10.62349,"end_time":"2025-03-27T15:45:56.195535","exception":false,"start_time":"2025-03-27T15:45:45.572045","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ngrok authtoken set up successfully!\n"]}],"source":["# Set up ngrok authtoken\n","from pyngrok import ngrok\n","\n","# Set the ngrok authtoken\n","ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n","\n","print(\"ngrok authtoken set up successfully!\")"]},{"cell_type":"markdown","id":"2f2168a5","metadata":{"id":"xtWeveSssH2C","papermill":{"duration":0.014582,"end_time":"2025-03-27T15:45:56.225916","exception":false,"start_time":"2025-03-27T15:45:56.211334","status":"completed"},"tags":[]},"source":["## Step 2: Fine-Tune YOLOv11 for Facial Recognition\n","\n","This step fine-tunes the YOLOv11 nano model (`yolo11n.pt`) on the facial recognition dataset (`shehabahmed74/shehab-data-facial-recognition`). The fine-tuned model will be saved as `yolo11n_finetuned.pt` and used in the Streamlit app for more specific facial recognition labels (e.g., \"face\" instead of \"person\").\n","\n","The dataset contains three folders: `train`, `valid`, and `test`. We will use:\n","- `train` for training.\n","- `valid` for validation.\n","- `test` for testing in the Streamlit app.\n","\n","The dataset paths, number of classes (`nc`), and class names (`names`) are pre-configured based on prior testing. We assume:\n","- There is one class (`face`), so `nc: 1` and `names: ['face']`.\n","\n","**This cell will:**\n","- Download the dataset (`shehabahmed74/shehab-data-facial-recognition`) using `kagglehub`.\n","- Unzip the dataset and verify its structure.\n","- Dynamically set the `train` and `val` paths in `data.yaml` based on the actual dataset structure.\n","- Fine-tune the YOLOv11 model.\n","\n","**Instructions:**\n","- Run the next cell (Cell 10) to download the dataset and fine-tune the model.\n","- This step takes the longest:\n","  - With GPU: ~10–20 minutes for 10 epochs.\n","  - Without GPU (on CPU): ~20–40 minutes for 10 epochs.\n","- If you encounter errors (e.g., missing dataset paths), the dataset structure may differ from the expected layout. The cell will print the dataset structure to help you debug. You can also manually check the structure by adding a new code cell with:\n","  ```python\n","  !ls -R /root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1"]},{"cell_type":"code","execution_count":null,"id":"b888f287","metadata":{"id":"JFk7vSFGn0hl","papermill":{"duration":0.014166,"end_time":"2025-03-27T15:45:56.25446","exception":false,"start_time":"2025-03-27T15:45:56.240294","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"id":"767a73f5","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:56.284196Z","iopub.status.busy":"2025-03-27T15:45:56.283935Z","iopub.status.idle":"2025-03-27T15:45:56.289224Z","shell.execute_reply":"2025-03-27T15:45:56.288631Z"},"id":"loUaDgbm5YU8","jupyter":{"source_hidden":true},"outputId":"c81a2db5-9c05-4579-dc9e-58938e6d83a9","papermill":{"duration":0.021761,"end_time":"2025-03-27T15:45:56.290442","exception":false,"start_time":"2025-03-27T15:45:56.268681","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 2: Fine-tune YOLOv11\n","# from ultralytics import YOLO\n","# import os\n","# import torch\n","# import kagglehub\n","# import zipfile\n","# import glob\n","# import shutil\n","\n","# # Function to print directory structure using Python\n","# def print_directory_structure(directory, indent=0, max_files=5):\n","#     \"\"\"Recursively print the directory structure with a limit on the number of files displayed per directory.\"\"\"\n","#     try:\n","#         print(\"  \" * indent + f\"Directory: {directory}\")\n","#         items = sorted(os.listdir(directory))\n","#         for item in items:\n","#             item_path = os.path.join(directory, item)\n","#             if os.path.isdir(item_path):\n","#                 print(\"  \" * (indent + 1) + f\"Subdirectory: {item}\")\n","#                 print_directory_structure(item_path, indent + 2, max_files)\n","#             else:\n","#                 if max_files > 0:\n","#                     print(\"  \" * (indent + 1) + f\"File: {item}\")\n","#                     max_files -= 1\n","#                 elif max_files == 0:\n","#                     print(\"  \" * (indent + 1) + \"... (additional files omitted)\")\n","#                     break\n","#     except Exception as e:\n","#         print(\"  \" * (indent + 1) + f\"Error accessing directory: {str(e)}\")\n","\n","# # Function to download and process dataset\n","# def download_and_process_dataset():\n","#     print(\"Downloading and Processing Dataset...\")\n","#     try:\n","#         # Download dataset\n","#         dataset_path = kagglehub.dataset_download(\"shehabahmed74/shehab-data-facial-recognition\")\n","#         print(f\"Dataset downloaded to: {dataset_path}\")\n","\n","#         # Process dataset (unzip if necessary)\n","#         if os.path.isdir(dataset_path):\n","#             processed_path = dataset_path\n","#         elif os.path.isfile(dataset_path) and dataset_path.endswith('.zip'):\n","#             unzip_dir = os.path.join(os.path.dirname(dataset_path), \"unzipped_dataset\")\n","#             os.makedirs(unzip_dir, exist_ok=True)\n","#             with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n","#                 zip_ref.extractall(unzip_dir)\n","#             processed_path = unzip_dir\n","#         else:\n","#             raise ValueError(\"Dataset is neither a recognized zip file nor a usable directory.\")\n","\n","#         print(\"Dataset processing completed!\")\n","#         return processed_path\n","#     except Exception as e:\n","#         print(f\"Error downloading or processing dataset: {str(e)}\")\n","#         raise\n","\n","# # Download and process the dataset\n","# dataset_path = download_and_process_dataset()\n","\n","# # Verify the dataset structure using Python\n","# print(\"\\nDataset structure:\")\n","# print_directory_structure(dataset_path)\n","\n","# # Search for image files in the dataset\n","# print(\"\\nSearching for image files in the dataset...\")\n","# image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n","# image_files = []\n","# for ext in image_extensions:\n","#     image_files.extend(glob.glob(os.path.join(dataset_path, \"**\", ext), recursive=True))\n","\n","# if not image_files:\n","#     raise FileNotFoundError(\"No image files found in the dataset. Check the dataset structure above.\")\n","# else:\n","#     print(f\"Found {len(image_files)} image files. Examples:\")\n","#     for img in image_files[:5]:  # Print first 5 image paths\n","#         print(img)\n","\n","# # Search for label files in the dataset\n","# print(\"\\nSearching for label files in the dataset...\")\n","# label_files = glob.glob(os.path.join(dataset_path, \"**\", \"*.txt\"), recursive=True)\n","# if not label_files:\n","#     print(\"Warning: No label files found in the dataset. Check the dataset structure above.\")\n","# else:\n","#     print(f\"Found {len(label_files)} label files. Examples:\")\n","#     for lbl in label_files[:5]:  # Print first 5 label paths\n","#         print(lbl)\n","\n","# # Create a dictionary mapping image filenames (without extension) to their paths\n","# image_dict = {os.path.splitext(os.path.basename(img))[0]: img for img in image_files}\n","# label_dict = {os.path.splitext(os.path.basename(lbl))[0]: lbl for lbl in label_files}\n","\n","# # Find images that have corresponding labels\n","# paired_files = []\n","# for img_name in image_dict.keys():\n","#     if img_name in label_dict:\n","#         paired_files.append((image_dict[img_name], label_dict[img_name]))\n","\n","# if not paired_files:\n","#     raise ValueError(\"No images have corresponding label files. Check the dataset structure.\")\n","\n","# print(f\"\\nFound {len(paired_files)} image-label pairs.\")\n","\n","# # Split the paired files into training and validation sets (80% train, 20% val)\n","# paired_files.sort()  # Sort to ensure consistent splitting\n","# split_idx = int(0.8 * len(paired_files))\n","# train_pairs = paired_files[:split_idx]\n","# val_pairs = paired_files[split_idx:]\n","\n","# # Create directories for training and validation\n","# train_images_dir = os.path.join(dataset_path, \"custom_train\", \"images\")\n","# train_labels_dir = os.path.join(dataset_path, \"custom_train\", \"labels\")\n","# val_images_dir = os.path.join(dataset_path, \"custom_valid\", \"images\")\n","# val_labels_dir = os.path.join(dataset_path, \"custom_valid\", \"labels\")\n","\n","# # Remove existing directories to avoid conflicts\n","# print(\"\\nRemoving existing custom_train and custom_valid directories (if any)...\")\n","# for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n","#     if os.path.exists(dir_path):\n","#         shutil.rmtree(dir_path)\n","\n","# # Recreate the directories\n","# os.makedirs(train_images_dir, exist_ok=True)\n","# os.makedirs(train_labels_dir, exist_ok=True)\n","# os.makedirs(val_images_dir, exist_ok=True)\n","# os.makedirs(val_labels_dir, exist_ok=True)\n","\n","# # Copy images and labels to the respective directories (symbolic links to save space)\n","# print(\"\\nOrganizing images and labels into train and valid directories...\")\n","# for img_path, lbl_path in train_pairs:\n","#     img_name = os.path.basename(img_path)\n","#     lbl_name = os.path.basename(lbl_path)\n","#     img_symlink = os.path.join(train_images_dir, img_name)\n","#     lbl_symlink = os.path.join(train_labels_dir, lbl_name)\n","#     # Create symbolic links, overwriting if they exist\n","#     if os.path.exists(img_symlink):\n","#         os.remove(img_symlink)\n","#     if os.path.exists(lbl_symlink):\n","#         os.remove(lbl_symlink)\n","#     os.symlink(img_path, img_symlink)\n","#     os.symlink(lbl_path, lbl_symlink)\n","\n","# for img_path, lbl_path in val_pairs:\n","#     img_name = os.path.basename(img_path)\n","#     lbl_name = os.path.basename(lbl_path)\n","#     img_symlink = os.path.join(val_images_dir, img_name)\n","#     lbl_symlink = os.path.join(val_labels_dir, lbl_name)\n","#     # Create symbolic links, overwriting if they exist\n","#     if os.path.exists(img_symlink):\n","#         os.remove(img_symlink)\n","#     if os.path.exists(lbl_symlink):\n","#         os.remove(lbl_symlink)\n","#     os.symlink(img_path, img_symlink)\n","#     os.symlink(lbl_path, lbl_symlink)\n","\n","# # Define the paths for data.yaml\n","# train_images_path = train_images_dir\n","# val_images_path = val_images_dir\n","\n","# # Verify the new directories\n","# print(f\"\\nNew training images path: {train_images_path}\")\n","# print(f\"Number of training images: {len(glob.glob(os.path.join(train_images_path, '*')))}\")\n","# print(f\"New training labels path: {train_labels_dir}\")\n","# print(f\"Number of training labels: {len(glob.glob(os.path.join(train_labels_dir, '*')))}\")\n","# print(f\"New validation images path: {val_images_path}\")\n","# print(f\"Number of validation images: {len(glob.glob(os.path.join(val_images_path, '*')))}\")\n","# print(f\"New validation labels path: {val_labels_dir}\")\n","# print(f\"Number of validation labels: {len(glob.glob(os.path.join(val_labels_dir, '*')))}\")\n","\n","# # Check if the paths exist and are not empty\n","# if not os.path.exists(train_images_path) or not glob.glob(os.path.join(train_images_path, '*')):\n","#     raise FileNotFoundError(f\"Training images path is empty or not found: {train_images_path}\")\n","# if not os.path.exists(val_images_path) or not glob.glob(os.path.join(val_images_path, '*')):\n","#     raise FileNotFoundError(f\"Validation images path is empty or not found: {val_images_path}\")\n","# if not os.path.exists(train_labels_dir) or not glob.glob(os.path.join(train_labels_dir, '*')):\n","#     raise FileNotFoundError(f\"Training labels path is empty or not found: {train_labels_dir}\")\n","# if not os.path.exists(val_labels_dir) or not glob.glob(os.path.join(val_labels_dir, '*')):\n","#     raise FileNotFoundError(f\"Validation labels path is empty or not found: {val_labels_dir}\")\n","\n","# # Load the pre-trained YOLOv11 nano model\n","# model = YOLO(\"yolo11n.pt\")\n","\n","# # Define the data.yaml with the correct paths\n","# data_yaml = f\"\"\"\n","# train: {train_images_path}\n","# val: {val_images_path}\n","# nc: 1  # Number of classes (assumed to be 1 for 'face')\n","# names: ['face']  # Class names (assumed to be 'face')\n","# \"\"\"\n","\n","# # Save the data.yaml file\n","# with open(\"data.yaml\", \"w\") as f:\n","#     f.write(data_yaml)\n","\n","# # Determine the device (GPU if available, else CPU)\n","# device = 0 if torch.cuda.is_available() else 'cpu'\n","# print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n","\n","# # Fine-tune the model\n","# try:\n","#     model.train(\n","#         data=\"data.yaml\",\n","#         epochs=10,  # Use fewer epochs for a quick test; increase for better results\n","#         imgsz=640,\n","#         batch=16,\n","#         device=device  # Use GPU if available, else CPU\n","#     )\n","# except Exception as e:\n","#     print(f\"Error during fine-tuning: {str(e)}\")\n","#     raise\n","\n","# # Save the fine-tuned model\n","# model.save(\"yolo11n_finetuned.pt\")\n","# print(\"Fine-tuning complete! Model saved as 'yolo11n_finetuned.pt'\")"]},{"cell_type":"code","execution_count":8,"id":"6279e638","metadata":{"execution":{"iopub.execute_input":"2025-03-27T15:45:56.319905Z","iopub.status.busy":"2025-03-27T15:45:56.319696Z","iopub.status.idle":"2025-03-27T16:00:25.561206Z","shell.execute_reply":"2025-03-27T16:00:25.560294Z"},"id":"2N3CX4eG5ZTU","papermill":{"duration":869.257917,"end_time":"2025-03-27T16:00:25.562607","exception":false,"start_time":"2025-03-27T15:45:56.30469","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Step 1: Dataset copied to writable directory\n","Step 2: Directory paths defined\n","Step 3: Found 25262 images, 25262 labels\n","Step 4: Found 23129 image-label pairs after deduplication\n","Step 5: Split into 18503 training pairs, 4626 validation pairs\n","Step 6: Moved 18503 images, 18503 labels to training directories, skipped 0 pairs\n","Step 7: Moved 4626 images, 4626 labels to validation directories, skipped 0 pairs\n","Step 8: Verified 18503 pairs in custom_train, 4626 pairs in custom_val\n"]}],"source":["### Cell: Step 2\n","import os\n","import shutil\n","import random\n","\n","# Step 1: Copy dataset to writable directory\n","source_dataset_dir = \"/kaggle/input/shehab-data-facial-recognition\"\n","working_dataset_dir = \"/kaggle/working/shehab-data-facial-recognition\"\n","if os.path.exists(working_dataset_dir):\n","    shutil.rmtree(working_dataset_dir)\n","shutil.copytree(source_dataset_dir, working_dataset_dir)\n","print(\"Step 1: Dataset copied to writable directory\")\n","\n","# Step 2: Define directory paths\n","train_images_dir = os.path.join(working_dataset_dir, \"custom_train\", \"images\")\n","train_labels_dir = os.path.join(working_dataset_dir, \"custom_train\", \"labels\")\n","val_images_dir = os.path.join(working_dataset_dir, \"custom_val\", \"images\")\n","val_labels_dir = os.path.join(working_dataset_dir, \"custom_val\", \"labels\")\n","os.makedirs(train_images_dir, exist_ok=True)\n","os.makedirs(train_labels_dir, exist_ok=True)\n","os.makedirs(val_images_dir, exist_ok=True)\n","os.makedirs(val_labels_dir, exist_ok=True)\n","print(\"Step 2: Directory paths defined\")\n","\n","# Step 3: Find images and labels\n","image_extensions = (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\")\n","all_images = []\n","all_labels = []\n","for root, dirs, files in os.walk(working_dataset_dir):\n","    if \"custom_train\" in root or \"custom_val\" in root:\n","        continue\n","    for file in files:\n","        if file.lower().endswith(image_extensions):\n","            all_images.append(os.path.join(root, file))\n","        elif file.endswith(\".txt\"):\n","            all_labels.append(os.path.join(root, file))\n","print(f\"Step 3: Found {len(all_images)} images, {len(all_labels)} labels\")\n","\n","# Step 4: Pair images with labels and deduplicate\n","image_label_pairs = []\n","seen_names = set()\n","for image_path in all_images:\n","    image_name = os.path.splitext(os.path.basename(image_path))[0]\n","    if image_name in seen_names:\n","        continue  # Skip duplicates\n","    for label_path in all_labels:\n","        label_name = os.path.splitext(os.path.basename(label_path))[0]\n","        if image_name == label_name and os.path.exists(image_path) and os.path.exists(label_path):\n","            image_label_pairs.append((image_path, label_path))\n","            seen_names.add(image_name)\n","            break\n","print(f\"Step 4: Found {len(image_label_pairs)} image-label pairs after deduplication\")\n","\n","# Step 5: Split into train and validation sets\n","random.shuffle(image_label_pairs)\n","split_idx = int(0.8 * len(image_label_pairs))\n","train_pairs = image_label_pairs[:split_idx]\n","val_pairs = image_label_pairs[split_idx:]\n","print(f\"Step 5: Split into {len(train_pairs)} training pairs, {len(val_pairs)} validation pairs\")\n","\n","# Step 6: Move files to training directories\n","train_images_moved = 0\n","train_labels_moved = 0\n","train_skipped = 0\n","for image_path, label_path in train_pairs:\n","    if os.path.exists(image_path) and os.path.exists(label_path):\n","        shutil.move(image_path, os.path.join(train_images_dir, os.path.basename(image_path)))\n","        shutil.move(label_path, os.path.join(train_labels_dir, os.path.basename(label_path)))\n","        train_images_moved += 1\n","        train_labels_moved += 1\n","    else:\n","        train_skipped += 1\n","print(f\"Step 6: Moved {train_images_moved} images, {train_labels_moved} labels to training directories, skipped {train_skipped} pairs\")\n","\n","# Step 7: Move files to validation directories\n","val_images_moved = 0\n","val_labels_moved = 0\n","val_skipped = 0\n","for image_path, label_path in val_pairs:\n","    if os.path.exists(image_path) and os.path.exists(label_path):\n","        shutil.move(image_path, os.path.join(val_images_dir, os.path.basename(image_path)))\n","        shutil.move(label_path, os.path.join(val_labels_dir, os.path.basename(label_path)))\n","        val_images_moved += 1\n","        val_labels_moved += 1\n","    else:\n","        val_skipped += 1\n","print(f\"Step 7: Moved {val_images_moved} images, {val_labels_moved} labels to validation directories, skipped {val_skipped} pairs\")\n","\n","# Step 8: Verify pairing\n","train_images = os.listdir(train_images_dir)\n","train_labels = os.listdir(train_labels_dir)\n","val_images = os.listdir(val_images_dir)\n","val_labels = os.listdir(val_labels_dir)\n","train_pairs_verified = sum(1 for img in train_images if f\"{os.path.splitext(img)[0]}.txt\" in train_labels)\n","val_pairs_verified = sum(1 for img in val_images if f\"{os.path.splitext(img)[0]}.txt\" in val_labels)\n","print(f\"Step 8: Verified {train_pairs_verified} pairs in custom_train, {val_pairs_verified} pairs in custom_val\")"]},{"cell_type":"code","execution_count":9,"id":"72bcf6a4","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:00:25.594124Z","iopub.status.busy":"2025-03-27T16:00:25.59386Z","iopub.status.idle":"2025-03-27T16:00:25.600772Z","shell.execute_reply":"2025-03-27T16:00:25.599954Z"},"id":"KhDvcsFy25CU","jupyter":{"source_hidden":true},"papermill":{"duration":0.023944,"end_time":"2025-03-27T16:00:25.601945","exception":false,"start_time":"2025-03-27T16:00:25.578001","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3: Create the Streamlit app\n","# import os\n","\n","# # Define the Streamlit app code\n","# streamlit_app_code = \"\"\"\n","# import streamlit as st\n","# import os\n","# import glob\n","# import random\n","# from PIL import Image\n","# from ultralytics import YOLO\n","\n","# # Load the fine-tuned YOLOv11 model\n","# @st.cache_resource\n","# def load_model():\n","#     return YOLO(\"yolo11n_finetuned.pt\")\n","\n","# model = load_model()\n","\n","# # Streamlit app\n","# st.title(\"YOLOv11 Facial Recognition Demo\")\n","\n","# # Step 1: Dataset Path Verification\n","# st.header(\"Step 1: Dataset Path Verification\")\n","# dataset_path = st.text_input(\"Enter the dataset path from Step 2:\", value=\"/root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1\")\n","# if dataset_path:\n","#     # Check for 'test' directory\n","#     test_path = os.path.join(dataset_path, \"test\")\n","#     if not os.path.exists(test_path):\n","#         st.error(f\"'test' directory not found: {test_path}. Ensure Step 2 completed successfully and enter the correct path.\")\n","#         test_images = []\n","#     else:\n","#         # Initialize test images list\n","#         test_images = []\n","#         # Check for nested 'test/test/images' directory (where images are located)\n","#         test_images_path = os.path.join(test_path, \"test\", \"images\")\n","#         st.write(f\"Checking for images in: {test_images_path}\")\n","#         if os.path.exists(test_images_path):\n","#             # Look for images with any case variation of .jpg, .png, .jpeg\n","#             test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n","#                           glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n","#                           glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n","#             st.write(f\"Found {len(test_images)} images in 'test/test/images'\")\n","#             if test_images:\n","#                 st.success(f\"Test images directory found: {test_images_path}\")\n","#                 st.write(f\"Found {len(test_images)} test images.\")\n","#                 st.write(f\"Sample images: {test_images[:5]}\")\n","#                 # Verify that the first image is readable\n","#                 try:\n","#                     with open(test_images[0], \"rb\") as f:\n","#                         f.read(1)\n","#                     st.write(f\"Successfully read first image: {test_images[0]}\")\n","#                 except Exception as e:\n","#                     st.error(f\"Error reading image {test_images[0]}: {str(e)}\")\n","#                     test_images = []\n","#             else:\n","#                 st.warning(f\"Test images directory exists but no .jpg, .png, or .jpeg images found in: {test_images_path}\")\n","#                 st.write(f\"Files in 'test/test/images': {os.listdir(test_images_path)[:5]}\")\n","#         else:\n","#             st.warning(f\"'test/test/images' directory not found: {test_images_path}\")\n","#             st.write(f\"Contents of 'test': {os.listdir(test_path)[:5]}\")\n","#             # Fallback: Check for images directly in 'test/test'\n","#             test_images_path = os.path.join(test_path, \"test\")\n","#             st.write(f\"Checking for images directly in: {test_images_path}\")\n","#             if os.path.exists(test_images_path):\n","#                 test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n","#                               glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n","#                               glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n","#                 st.write(f\"Found {len(test_images)} images directly in 'test/test'\")\n","#                 if test_images:\n","#                     st.success(f\"Test images found in nested 'test/test' directory: {test_images_path}\")\n","#                     st.write(f\"Found {len(test_images)} test images.\")\n","#                     st.write(f\"Sample images: {test_images[:5]}\")\n","#                 else:\n","#                     st.warning(f\"No .jpg, .png, or .jpeg images found in {test_images_path}\")\n","#                     st.write(f\"Files in 'test/test': {os.listdir(test_images_path)[:5]}\")\n","#             else:\n","#                 st.warning(f\"'test/test' directory not found: {test_images_path}\")\n","\n","#         # Final check: If no images are found, notify the user\n","#         if not test_images:\n","#             st.info(\"No test images found. You can still upload an external image for inference below.\")\n","\n","# # Step 2: Test YOLOv11 Facial Recognition\n","# st.header(\"Step 2: Test YOLOv11 Facial Recognition\")\n","\n","# # Option to select image source\n","# image_source_options = [\"Use a random image from the test dataset\", \"Upload an external image\"]\n","# image_source = st.radio(\n","#     \"Select image source for facial recognition:\",\n","#     image_source_options,\n","#     index=0 if test_images else 1  # Default to random if test images exist, otherwise upload\n","# )\n","\n","# # Disable the random image option if no test images are found\n","# if not test_images and image_source == \"Use a random image from the test dataset\":\n","#     st.warning(\"No test images found. Please select 'Upload an external image' instead.\")\n","#     image_source = \"Upload an external image\"\n","\n","# # Initialize image_path\n","# image_path = None\n","\n","# # Handle random image selection\n","# if image_source == \"Use a random image from the test dataset\" and test_images:\n","#     image_path = random.choice(test_images)\n","#     st.write(f\"Selected random image: {os.path.basename(image_path)}\")\n","\n","# # Always show the file uploader for external images\n","# if image_source == \"Upload an external image\":\n","#     uploaded_file = st.file_uploader(\"Upload an external image for facial recognition\", type=[\"jpg\", \"jpeg\", \"png\"])\n","#     if uploaded_file is not None:\n","#         # Save the uploaded file temporarily\n","#         image_path = os.path.join(\"temp\", uploaded_file.name)\n","#         st.write(f\"Saving uploaded file to: {image_path}\")\n","#         try:\n","#             os.makedirs(\"temp\", exist_ok=True)\n","#             with open(image_path, \"wb\") as f:\n","#                 f.write(uploaded_file.getbuffer())\n","#             st.write(f\"Uploaded image: {uploaded_file.name}\")\n","#             # Verify the file was saved\n","#             if os.path.exists(image_path):\n","#                 st.write(f\"File successfully saved at: {image_path}\")\n","#             else:\n","#                 st.error(f\"Failed to save the uploaded file at: {image_path}\")\n","#                 image_path = None\n","#         except Exception as e:\n","#             st.error(f\"Error saving uploaded file: {str(e)}\")\n","#             image_path = None\n","#     else:\n","#         st.info(\"Please upload an image to proceed.\")\n","\n","# # Run inference if an image is selected\n","# if st.button(\"Run Inference\"):\n","#     if image_path:\n","#         # Load and display the image\n","#         try:\n","#             image = Image.open(image_path)\n","#             st.image(image, caption=\"Input Image\", use_column_width=True)\n","#         except Exception as e:\n","#             st.error(f\"Error loading image {image_path}: {str(e)}\")\n","#             image_path = None\n","\n","#         if image_path:\n","#             # Run YOLOv11 inference\n","#             with st.spinner(\"Running facial recognition...\"):\n","#                 try:\n","#                     results = model(image)\n","#                     # Display the results\n","#                     st.header(\"Step 3: Results\")\n","#                     # Plot the results\n","#                     annotated_image = results[0].plot()  # Get the annotated image with bounding boxes\n","#                     st.image(annotated_image, caption=\"Detected Faces\", use_column_width=True)\n","\n","#                     # Display detection details\n","#                     detections = results[0].boxes\n","#                     st.write(f\"Number of faces detected: {len(detections)}\")\n","#                     for i, det in enumerate(detections):\n","#                         conf = det.conf.item()\n","#                         bbox = det.xyxy[0].tolist()\n","#                         st.write(f\"Face {i+1}: Confidence = {conf:.2f}, Bounding Box = {bbox}\")\n","#                 except Exception as e:\n","#                     st.error(f\"Error during inference: {str(e)}\")\n","#     else:\n","#         st.error(\"Please select a random test image or upload an external image to run inference.\")\n","\n","# # Step 4: Summary and Conclusion\n","# st.header(\"Step 4: Summary and Conclusion\")\n","# st.write(\"This demo showcases the YOLOv11 model fine-tuned for facial recognition.\")\n","# st.write(\"The model was fine-tuned on the dataset provided in Step 2 and tested on either a random test image or an uploaded external image.\")\n","# st.write(\"For detailed performance metrics and comparisons, refer to the notebook (Step 3.1).\")\n","# \"\"\"\n","\n","# # Write the Streamlit app code to app.py\n","# with open(\"app.py\", \"w\") as f:\n","#     f.write(streamlit_app_code)\n","\n","# # Verify that app.py was created\n","# if os.path.exists(\"app.py\"):\n","#     print(\"Streamlit app created successfully at app.py!\")\n","# else:\n","#     raise FileNotFoundError(\"Failed to create app.py\")"]},{"cell_type":"code","execution_count":null,"id":"df17473e","metadata":{"id":"_gbzxrty25-7","papermill":{"duration":0.014281,"end_time":"2025-03-27T16:00:25.630924","exception":false,"start_time":"2025-03-27T16:00:25.616643","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"58f95c75","metadata":{"id":"4FnX3ZJTsMp6","papermill":{"duration":0.014358,"end_time":"2025-03-27T16:00:25.659912","exception":false,"start_time":"2025-03-27T16:00:25.645554","status":"completed"},"tags":[]},"source":["## Step 3: Set Up the Streamlit App\n","\n","This step creates the Streamlit app (`app.py`) that compares YOLOv8 and YOLOv11 for facial recognition. The app includes:\n","- **Dataset Path Verification:** Uses the dataset already downloaded in Step 2.\n","- **YOLOv11 Inference:** Performs facial recognition using the fine-tuned model on the `test` dataset or an uploaded image.\n","- **Performance Comparison:** Generates plots and a table comparing YOLOv8 and YOLOv11.\n","- **Summary:** Provides a conclusion and recommendation.\n","\n","The app uses the `test` folder for inference to evaluate the fine-tuned model on unseen data.\n","\n","**Instructions:**\n","- Run this cell to create `app.py`.\n","- Then, proceed to Step 3.1 to deploy the app.\n","- Note: The dataset was already downloaded in Step 2. The app will use the same dataset path."]},{"cell_type":"markdown","id":"4d7d8a1b","metadata":{"papermill":{"duration":0.014419,"end_time":"2025-03-27T16:00:25.688955","exception":false,"start_time":"2025-03-27T16:00:25.674536","status":"completed"},"tags":[]},"source":["# Step 10: Fine-tune YOLOv11 model"]},{"cell_type":"code","execution_count":10,"id":"a33a493a","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:00:25.720148Z","iopub.status.busy":"2025-03-27T16:00:25.719862Z","iopub.status.idle":"2025-03-27T16:00:29.161678Z","shell.execute_reply":"2025-03-27T16:00:29.160619Z"},"papermill":{"duration":3.459312,"end_time":"2025-03-27T16:00:29.163396","exception":false,"start_time":"2025-03-27T16:00:25.704084","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.97)\r\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\r\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\r\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\r\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\r\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\r\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\r\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\r\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\r\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\r\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\r\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\r\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\r\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\r\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\r\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\r\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\r\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\r\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\r\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\r\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\r\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n","Ultralytics installed successfully\n"]}],"source":["# Step 0: Install required libraries\n","!pip install ultralytics\n","print(\"Ultralytics installed successfully\")"]},{"cell_type":"code","execution_count":11,"id":"e07f8010","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:00:29.195726Z","iopub.status.busy":"2025-03-27T16:00:29.195398Z","iopub.status.idle":"2025-03-27T16:00:32.671229Z","shell.execute_reply":"2025-03-27T16:00:32.670217Z"},"papermill":{"duration":3.493291,"end_time":"2025-03-27T16:00:32.672544","exception":false,"start_time":"2025-03-27T16:00:29.179253","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.97)\r\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\r\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\r\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\r\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\r\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\r\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\r\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\r\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\r\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\r\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\r\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\r\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\r\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\r\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\r\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\r\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\r\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\r\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\r\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\r\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\r\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n","Ultralytics installed successfully\n"]}],"source":["# Step 0: Install required libraries\n","!pip install ultralytics\n","print(\"Ultralytics installed successfully\")"]},{"cell_type":"markdown","id":"8d9af810","metadata":{"papermill":{"duration":0.015121,"end_time":"2025-03-27T16:00:32.703822","exception":false,"start_time":"2025-03-27T16:00:32.688701","status":"completed"},"tags":[]},"source":["To fix warning message which appeared when run cell 10"]},{"cell_type":"code","execution_count":12,"id":"3b386b31","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:00:32.736142Z","iopub.status.busy":"2025-03-27T16:00:32.73588Z","iopub.status.idle":"2025-03-27T16:00:35.137477Z","shell.execute_reply":"2025-03-27T16:00:35.136434Z"},"papermill":{"duration":2.419235,"end_time":"2025-03-27T16:00:35.138843","exception":false,"start_time":"2025-03-27T16:00:32.719608","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Fixed 18503 training labels and 4626 validation labels\n"]}],"source":["# Step 10.5: Fix label files to ensure class ID is 0\n","import os\n","\n","# Define directories\n","train_labels_dir = \"/kaggle/working/shehab-data-facial-recognition/custom_train/labels\"\n","val_labels_dir = \"/kaggle/working/shehab-data-facial-recognition/custom_val/labels\"\n","\n","# Function to fix a single label file\n","def fix_label_file(label_path):\n","    try:\n","        with open(label_path, \"r\") as f:\n","            lines = f.readlines()\n","        # Fix class ID to 0\n","        fixed_lines = []\n","        for line in lines:\n","            parts = line.strip().split()\n","            if len(parts) >= 5:  # Ensure the line has at least 5 parts (class_id, x, y, w, h)\n","                parts[0] = \"0\"  # Set class ID to 0\n","                fixed_lines.append(\" \".join(parts) + \"\\n\")\n","        # Write the fixed lines back to the file\n","        with open(label_path, \"w\") as f:\n","            f.writelines(fixed_lines)\n","    except Exception as e:\n","        print(f\"Error fixing {label_path}: {str(e)}\")\n","\n","# Fix training labels\n","train_fixed = 0\n","for label_file in os.listdir(train_labels_dir):\n","    if label_file.endswith(\".txt\"):\n","        fix_label_file(os.path.join(train_labels_dir, label_file))\n","        train_fixed += 1\n","\n","# Fix validation labels\n","val_fixed = 0\n","for label_file in os.listdir(val_labels_dir):\n","    if label_file.endswith(\".txt\"):\n","        fix_label_file(os.path.join(val_labels_dir, label_file))\n","        val_fixed += 1\n","\n","print(f\"Fixed {train_fixed} training labels and {val_fixed} validation labels\")"]},{"cell_type":"code","execution_count":13,"id":"a57b3205","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:00:35.171267Z","iopub.status.busy":"2025-03-27T16:00:35.170976Z","iopub.status.idle":"2025-03-27T16:56:13.132188Z","shell.execute_reply":"2025-03-27T16:56:13.131227Z"},"id":"IUQ4Qg_M09BF","papermill":{"duration":3337.978823,"end_time":"2025-03-27T16:56:13.133646","exception":false,"start_time":"2025-03-27T16:00:35.154823","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Created data.yaml\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5.35M/5.35M [00:00<00:00, 111MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Ultralytics 8.3.97 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/kaggle/working/shehab-data-facial-recognition/data.yaml, epochs=10, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=/kaggle/working/runs/train, name=yolo11n_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/kaggle/working/runs/train/yolo11n_finetune\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 755k/755k [00:00<00:00, 23.0MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n"," 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n"," 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n"," 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n","YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n","\n","Transferred 448/499 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /kaggle/working/runs/train/yolo11n_finetune', view at http://localhost:6006/\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/shehab-data-facial-recognition/custom_train/labels... 18503 images, 0 backgrounds, 0 corrupt: 100%|██████████| 18503/18503 [00:14<00:00, 1260.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/shehab-data-facial-recognition/custom_train/labels.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/shehab-data-facial-recognition/custom_val/labels... 4626 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4626/4626 [00:03<00:00, 1245.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/shehab-data-facial-recognition/custom_val/labels.cache\n","Plotting labels to /kaggle/working/runs/train/yolo11n_finetune/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 4 dataloader workers\n","Logging results to \u001b[1m/kaggle/working/runs/train/yolo11n_finetune\u001b[0m\n","Starting training for 10 epochs...\n","Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       1/10      1.35G     0.1233     0.4377     0.9609          7        640: 100%|██████████| 2313/2313 [05:15<00:00,  7.34it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:28<00:00, 10.10it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       2/10      1.54G    0.07245      0.085     0.9209          7        640: 100%|██████████| 2313/2313 [05:00<00:00,  7.71it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:27<00:00, 10.71it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       3/10      1.54G    0.06548    0.07061     0.9167          7        640: 100%|██████████| 2313/2313 [04:56<00:00,  7.80it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.89it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       4/10      1.54G    0.05011     0.0564     0.9067          7        640: 100%|██████████| 2313/2313 [04:54<00:00,  7.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 11.14it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       5/10      1.54G    0.04184    0.04789     0.9026          7        640: 100%|██████████| 2313/2313 [04:54<00:00,  7.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.85it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       6/10      1.54G    0.03605    0.04207     0.9023          7        640: 100%|██████████| 2313/2313 [04:55<00:00,  7.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.86it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       7/10      1.54G    0.03151    0.03746     0.9025          7        640: 100%|██████████| 2313/2313 [04:55<00:00,  7.82it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.79it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       8/10      1.54G    0.02734    0.03264        0.9          7        640: 100%|██████████| 2313/2313 [04:56<00:00,  7.81it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.93it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       9/10      1.54G    0.02354     0.0282     0.9008          7        640: 100%|██████████| 2313/2313 [04:56<00:00,  7.81it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.78it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      10/10      1.54G    0.02006    0.02376     0.9004          7        640: 100%|██████████| 2313/2313 [04:56<00:00,  7.80it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:26<00:00, 10.87it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","10 epochs completed in 0.905 hours.\n","Optimizer stripped from /kaggle/working/runs/train/yolo11n_finetune/weights/last.pt, 5.5MB\n","Optimizer stripped from /kaggle/working/runs/train/yolo11n_finetune/weights/best.pt, 5.5MB\n","\n","Validating /kaggle/working/runs/train/yolo11n_finetune/weights/best.pt...\n","Ultralytics 8.3.97 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n","YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"]},{"name":"stderr","output_type":"stream","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:27<00:00, 10.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all       4626       4626          1          1      0.995      0.995\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n","  xa[xa < 0] = -1\n","/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n","  xa[xa < 0] = -1\n"]},{"name":"stdout","output_type":"stream","text":["Speed: 0.2ms preprocess, 2.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n","Results saved to \u001b[1m/kaggle/working/runs/train/yolo11n_finetune\u001b[0m\n","Fine-tuning complete. Model saved as 'yolo11n_finetuned.pt'\n"]}],"source":["# Step 10: Fine-tune YOLOv11 model\n","from ultralytics import YOLO\n","import os\n","\n","# Create data.yaml if it doesn't exist\n","data_yaml_path = \"/kaggle/working/shehab-data-facial-recognition/data.yaml\"\n","if not os.path.exists(data_yaml_path):\n","    data_yaml = \"\"\"\n","    train: /kaggle/working/shehab-data-facial-recognition/custom_train/images\n","    val: /kaggle/working/shehab-data-facial-recognition/custom_val/images\n","    nc: 1\n","    names: ['face']\n","    \"\"\"\n","    with open(data_yaml_path, \"w\") as f:\n","        f.write(data_yaml)\n","    print(\"Created data.yaml\")\n","\n","# Load the pre-trained YOLOv11 model\n","model = YOLO(\"yolo11n.pt\")  # This will download the model if not present\n","\n","# Fine-tune the model on your dataset\n","model.train(\n","    data=data_yaml_path,\n","    epochs=10,\n","    imgsz=640,\n","    batch=8,\n","    device=0,\n","    project=\"/kaggle/working/runs/train\",\n","    name=\"yolo11n_finetune\"\n",")\n","\n","# Save the fine-tuned model\n","model.save(\"/kaggle/working/yolo11n_finetuned.pt\")\n","print(\"Fine-tuning complete. Model saved as 'yolo11n_finetuned.pt'\")"]},{"cell_type":"markdown","id":"d5530bdb","metadata":{"papermill":{"duration":2.467491,"end_time":"2025-03-27T16:56:17.965722","exception":false,"start_time":"2025-03-27T16:56:15.498231","status":"completed"},"tags":[]},"source":[]},{"cell_type":"code","execution_count":14,"id":"e266d03e","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:56:22.873803Z","iopub.status.busy":"2025-03-27T16:56:22.873432Z","iopub.status.idle":"2025-03-27T16:56:22.883622Z","shell.execute_reply":"2025-03-27T16:56:22.882743Z"},"id":"clzCoV2PGeWC","papermill":{"duration":2.564426,"end_time":"2025-03-27T16:56:22.884952","exception":false,"start_time":"2025-03-27T16:56:20.320526","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Streamlit app created successfully at app.py!\n"]}],"source":["# Step 3: Create the Streamlit app\n","import os\n","\n","# Check if the fine-tuned model file exists\n","model_file = \"yolo11n_finetuned.pt\"\n","if not os.path.exists(model_file):\n","    raise FileNotFoundError(f\"The fine-tuned model file '{model_file}' does not exist. Please run the fine-tuning step (Cell 10) to create it.\")\n","\n","# Define the Streamlit app code\n","streamlit_app_code = \"\"\"\n","import streamlit as st\n","import os\n","import glob\n","import random\n","from PIL import Image\n","from ultralytics import YOLO\n","\n","# Load the fine-tuned YOLOv11 model\n","@st.cache_resource\n","def load_model():\n","    return YOLO(\"yolo11n_finetuned.pt\")\n","\n","model = load_model()\n","\n","# Initialize session state for image_path\n","if 'image_path' not in st.session_state:\n","    st.session_state.image_path = None\n","\n","# Streamlit app\n","st.title(\"YOLOv11 Facial Recognition Demo\")\n","\n","# Step 1: Dataset Path Verification\n","st.header(\"Step 1: Dataset Path Verification\")\n","dataset_path = st.text_input(\"Enter the dataset path from Step 2:\", value=\"/root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1\")\n","if dataset_path:\n","    # Check for 'test' directory\n","    test_path = os.path.join(dataset_path, \"test\")\n","    if not os.path.exists(test_path):\n","        st.error(f\"'test' directory not found: {test_path}. Ensure Step 2 completed successfully and enter the correct path.\")\n","        test_images = []\n","    else:\n","        # Initialize test images list\n","        test_images = []\n","        # Check for nested 'test/test/images' directory (where images are located)\n","        test_images_path = os.path.join(test_path, \"test\", \"images\")\n","        st.write(f\"Checking for images in: {test_images_path}\")\n","        if os.path.exists(test_images_path):\n","            # Look for images with any case variation of .jpg, .png, .jpeg\n","            test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n","                          glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n","                          glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n","            st.write(f\"Found {len(test_images)} images in 'test/test/images'\")\n","            if test_images:\n","                st.success(f\"Test images directory found: {test_images_path}\")\n","                st.write(f\"Found {len(test_images)} test images.\")\n","                st.write(f\"Sample images: {test_images[:5]}\")\n","                # Verify that the first image is readable\n","                try:\n","                    with open(test_images[0], \"rb\") as f:\n","                        f.read(1)\n","                    st.write(f\"Successfully read first image: {test_images[0]}\")\n","                except Exception as e:\n","                    st.error(f\"Error reading image {test_images[0]}: {str(e)}\")\n","                    test_images = []\n","            else:\n","                st.warning(f\"Test images directory exists but no .jpg, .png, or .jpeg images found in: {test_images_path}\")\n","                st.write(f\"Files in 'test/test/images': {os.listdir(test_images_path)[:5]}\")\n","        else:\n","            st.warning(f\"'test/test/images' directory not found: {test_images_path}\")\n","            st.write(f\"Contents of 'test': {os.listdir(test_path)[:5]}\")\n","            # Fallback: Check for images directly in 'test/test'\n","            test_images_path = os.path.join(test_path, \"test\")\n","            st.write(f\"Checking for images directly in: {test_images_path}\")\n","            if os.path.exists(test_images_path):\n","                test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n","                              glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n","                              glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n","                st.write(f\"Found {len(test_images)} images directly in 'test/test'\")\n","                if test_images:\n","                    st.success(f\"Test images found in nested 'test/test' directory: {test_images_path}\")\n","                    st.write(f\"Found {len(test_images)} test images.\")\n","                    st.write(f\"Sample images: {test_images[:5]}\")\n","                else:\n","                    st.warning(f\"No .jpg, .png, or .jpeg images found in {test_images_path}\")\n","                    st.write(f\"Files in 'test/test': {os.listdir(test_images_path)[:5]}\")\n","            else:\n","                st.warning(f\"'test/test' directory not found: {test_images_path}\")\n","\n","        # Final check: If no images are found, notify the user\n","        if not test_images:\n","            st.info(\"No test images found. You can still upload an external image for inference below.\")\n","\n","# Step 2: Test YOLOv11 Facial Recognition\n","st.header(\"Step 2: Test YOLOv11 Facial Recognition\")\n","\n","# Option to select image source\n","image_source_options = [\"Use a random image from the test dataset\", \"Upload an external image\"]\n","image_source = st.radio(\n","    \"Select image source for facial recognition:\",\n","    image_source_options,\n","    index=0 if test_images else 1  # Default to random if test images exist, otherwise upload\n",")\n","\n","# Disable the random image option if no test images are found\n","if not test_images and image_source == \"Use a random image from the test dataset\":\n","    st.warning(\"No test images found. Please select 'Upload an external image' instead.\")\n","    image_source = \"Upload an external image\"\n","\n","# Initialize local image_path for this run\n","image_path = None\n","\n","# Handle random image selection\n","if image_source == \"Use a random image from the test dataset\" and test_images:\n","    image_path = random.choice(test_images)\n","    st.write(f\"Selected random image: {os.path.basename(image_path)}\")\n","    st.session_state.image_path = image_path\n","\n","# Handle external image upload\n","if image_source == \"Upload an external image\":\n","    uploaded_file = st.file_uploader(\"Upload an external image for facial recognition\", type=[\"jpg\", \"jpeg\", \"png\"])\n","    if uploaded_file is not None:\n","        # Save the uploaded file temporarily\n","        image_path = os.path.join(\"temp\", uploaded_file.name)\n","        st.write(f\"Saving uploaded file to: {image_path}\")\n","        try:\n","            os.makedirs(\"temp\", exist_ok=True)\n","            with open(image_path, \"wb\") as f:\n","                f.write(uploaded_file.getbuffer())\n","            st.write(f\"Uploaded image: {uploaded_file.name}\")\n","            # Verify the file was saved\n","            if os.path.exists(image_path):\n","                st.write(f\"File successfully saved at: {image_path}\")\n","                st.session_state.image_path = image_path  # Store in session state\n","            else:\n","                st.error(f\"Failed to save the uploaded file at: {image_path}\")\n","                st.session_state.image_path = None\n","        except Exception as e:\n","            st.error(f\"Error saving uploaded file: {str(e)}\")\n","            st.session_state.image_path = None\n","    else:\n","        st.info(\"Please upload an image to proceed.\")\n","\n","    # Display current uploaded image status\n","    if st.session_state.image_path and image_source == \"Upload an external image\":\n","        st.success(f\"Image ready for inference: {os.path.basename(st.session_state.image_path)}\")\n","        if st.button(\"Clear Uploaded Image\"):\n","            st.session_state.image_path = None\n","            st.experimental_rerun()\n","\n","# Run inference if an image is selected\n","if st.button(\"Run Inference\"):\n","    # Use the session state image_path if available, otherwise use the local image_path\n","    current_image_path = st.session_state.image_path if st.session_state.image_path else image_path\n","    if current_image_path:\n","        # Load and display the image\n","        try:\n","            image = Image.open(current_image_path)\n","            st.image(image, caption=\"Input Image\", use_column_width=True)\n","        except Exception as e:\n","            st.error(f\"Error loading image {current_image_path}: {str(e)}\")\n","            st.session_state.image_path = None\n","            current_image_path = None\n","\n","        if current_image_path:\n","            # Run YOLOv11 inference\n","            with st.spinner(\"Running facial recognition...\"):\n","                try:\n","                    results = model(image)\n","                    # Display the results\n","                    st.header(\"Step 3: Results\")\n","                    # Plot the results\n","                    annotated_image = results[0].plot()  # Get the annotated image with bounding boxes\n","                    st.image(annotated_image, caption=\"Detected Faces\", use_column_width=True)\n","\n","                    # Display detection details\n","                    detections = results[0].boxes\n","                    st.write(f\"Number of faces detected: {len(detections)}\")\n","                    for i, det in enumerate(detections):\n","                        conf = det.conf.item()\n","                        bbox = det.xyxy[0].tolist()\n","                        st.write(f\"Face {i+1}: Confidence = {conf:.2f}, Bounding Box = {bbox}\")\n","                except Exception as e:\n","                    st.error(f\"Error during inference: {str(e)}\")\n","    else:\n","        st.error(\"Please select a random test image or upload an external image to run inference.\")\n","\n","# Step 4: Summary and Conclusion\n","st.header(\"Step 4: Summary and Conclusion\")\n","st.write(\"This demo showcases the YOLOv11 model fine-tuned for facial recognition.\")\n","st.write(\"The model was fine-tuned on the dataset provided in Step 2 and tested on either a random test image or an uploaded external image.\")\n","st.write(\"For detailed performance metrics and comparisons, refer to the notebook (Step 3.1).\")\n","\"\"\"\n","\n","# Write the Streamlit app code to app.py\n","with open(\"app.py\", \"w\") as f:\n","    f.write(streamlit_app_code)\n","\n","# Verify that app.py was created\n","if os.path.exists(\"app.py\"):\n","    print(\"Streamlit app created successfully at app.py!\")\n","else:\n","    raise FileNotFoundError(\"Failed to create app.py\")"]},{"cell_type":"code","execution_count":null,"id":"f1cfe1b1","metadata":{"papermill":{"duration":2.376094,"end_time":"2025-03-27T16:56:27.683389","exception":false,"start_time":"2025-03-27T16:56:25.307295","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"55d03b5b","metadata":{"id":"M0Ci1s3TsV-k","papermill":{"duration":2.391144,"end_time":"2025-03-27T16:56:32.530919","exception":false,"start_time":"2025-03-27T16:56:30.139775","status":"completed"},"tags":[]},"source":["## Step 3.1: Deploy the Streamlit App and Download the Dataset\n","\n","<!-- This cell deploys the Streamlit app using ngrok, creating a public URL for access. It also downloads the dataset, which is required for fine-tuning and inference.\n","\n","**Instructions:**\n","- Run this cell to start the Streamlit app.\n","- Open the provided URL in your browser.\n","- Go to \"Step 1: Dataset Preparation\" in the app and click \"Download and Process Dataset.\"\n","- The public URL will be stored for the summary section.\n","- Proceed to Step 4 to test the app. -->\n","\n","\n","“# Deprecated"]},{"cell_type":"code","execution_count":15,"id":"bd15e414","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:56:37.333297Z","iopub.status.busy":"2025-03-27T16:56:37.332958Z","iopub.status.idle":"2025-03-27T16:56:37.337297Z","shell.execute_reply":"2025-03-27T16:56:37.336434Z"},"id":"NMAyiwxvbX_V","papermill":{"duration":2.452544,"end_time":"2025-03-27T16:56:37.338772","exception":false,"start_time":"2025-03-27T16:56:34.886228","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.1: Performance Comparison (YOLOv8 vs. YOLOv11)\n","# import numpy as np\n","# import pandas as pd\n","# import seaborn as sns\n","# import matplotlib.pyplot as plt\n","# import time\n","# import json\n","# import os\n","\n","# print(\"Step 3.1: Comparing YOLOv8 and YOLOv11...\")\n","# start_time = time.time()\n","\n","# # Simulate performance metrics over 50 epochs\n","# epochs = np.arange(1, 51)\n","# mAP8 = 0.75 + 0.12 * np.sin(epochs * 0.08)  # Simulated mAP50 for YOLOv8\n","# mAP11 = 0.82 + 0.10 * np.cos(epochs * 0.06)  # Simulated mAP50 for YOLOv11\n","# latency8 = 25 - 0.06 * epochs  # Simulated latency for YOLOv8\n","# latency11 = 24 - 0.07 * epochs  # Simulated latency for YOLOv11\n","# fps8 = 45 + 0.15 * epochs  # Simulated FPS for YOLOv8\n","# fps11 = 47 + 0.18 * epochs  # Simulated FPS for YOLOv11\n","\n","# # Plot mAP50 comparison\n","# plt.figure(figsize=(12, 6))\n","# sns.lineplot(x=epochs, y=mAP8, label=\"YOLOv8 mAP50\", color=\"blue\")\n","# sns.lineplot(x=epochs, y=mAP11, label=\"YOLOv11 mAP50\", color=\"orange\")\n","# plt.xlabel(\"Epochs\")\n","# plt.ylabel(\"mAP50\")\n","# plt.title(\"mAP50 Comparison: YOLOv8 vs. YOLOv11\")\n","# plt.legend()\n","# plt.grid(True)\n","# plt.savefig(\"map50_comparison.png\")  # Save the plot\n","# plt.show()\n","# print(\"Generated mAP50 comparison plot.\")\n","\n","# # Plot latency comparison\n","# plt.figure(figsize=(12, 6))\n","# sns.lineplot(x=epochs, y=latency8, label=\"YOLOv8 Latency (ms)\", color=\"blue\")\n","# sns.lineplot(x=epochs, y=latency11, label=\"YOLOv11 Latency (ms)\", color=\"orange\")\n","# plt.xlabel(\"Epochs\")\n","# plt.ylabel(\"Latency (ms)\")\n","# plt.title(\"Latency Comparison: YOLOv8 vs. YOLOv11\")\n","# plt.legend()\n","# plt.grid(True)\n","# plt.savefig(\"latency_comparison.png\")  # Save the plot\n","# plt.show()\n","# print(\"Generated latency comparison plot.\")\n","\n","# # Plot FPS comparison\n","# plt.figure(figsize=(12, 6))\n","# sns.lineplot(x=epochs, y=fps8, label=\"YOLOv8 FPS\", color=\"blue\")\n","# sns.lineplot(x=epochs, y=fps11, label=\"YOLOv11 FPS\", color=\"orange\")\n","# plt.xlabel(\"Epochs\")\n","# plt.ylabel(\"FPS\")\n","# plt.title(\"FPS Comparison: YOLOv8 vs. YOLOv11\")\n","# plt.legend()\n","# plt.grid(True)\n","# plt.savefig(\"fps_comparison.png\")  # Save the plot\n","# plt.show()\n","# print(\"Generated FPS comparison plot.\")\n","\n","# # Create comparison table\n","# comparison_data = {\n","#     \"Metric\": [\"Peak mAP50\", \"Min Latency (ms)\", \"Peak FPS\", \"Parameters (M)\", \"Inference Speedup (%)\"],\n","#     \"YOLOv8\": [np.max(mAP8), np.min(latency8), np.max(fps8), 11.2, 0],\n","#     \"YOLOv11\": [np.max(mAP11), np.min(latency11), np.max(fps11), 8.7, 2]\n","# }\n","# comparison_df = pd.DataFrame(comparison_data)\n","# print(\"\\nDetailed Comparison Table:\")\n","# print(comparison_df.to_string(index=False))\n","\n","# # Load and display the test results from the Streamlit app\n","# print(\"\\nFacial Recognition Test Results (from Streamlit App):\")\n","# if os.path.exists(\"inference_results.json\"):\n","#     with open(\"inference_results.json\", \"r\") as f:\n","#         inference_results = json.load(f)\n","#     print(f\"Image Path: {inference_results['image_path']}\")\n","#     print(f\"Number of Faces Detected: {inference_results['num_faces_detected']}\")\n","#     for detection in inference_results['detections']:\n","#         print(f\"Face {detection['face']}: Confidence = {detection['confidence']:.2f}, Bounding Box = {detection['bounding_box']}\")\n","# else:\n","#     print(\"No test results found. Please run a test in the Streamlit app (Step 3.2) first.\")\n","\n","# print(f\"Step 3.1 completed in {time.time() - start_time:.2f} seconds\")"]},{"cell_type":"markdown","id":"552f95ef","metadata":{"id":"6fHiFg1Q1pvh","papermill":{"duration":2.449192,"end_time":"2025-03-27T16:56:42.15873","exception":false,"start_time":"2025-03-27T16:56:39.709538","status":"completed"},"tags":[]},"source":["## Step 3.2: Deploy the Streamlit App\n","\n","<!-- This cell deploys the Streamlit app using ngrok, creating a public URL for access.\n","\n","**Instructions:**\n","- Run this cell to start the Streamlit app.\n","- Open the provided URL in your browser.\n","- Go to \"Step 1: Dataset Path Verification\" in the app to confirm the dataset path.\n","- Test the app by using a test image or uploading an image in \"Step 2: Test YOLOv11 Facial Recognition.\"\n","- The public URL will be stored for the summary section.\n","- Proceed to Step 4 to test the app further.\n","Cell 14 # -->\n","\n","“# Deprecated"]},{"cell_type":"code","execution_count":16,"id":"dbfd7a70","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:56:46.871246Z","iopub.status.busy":"2025-03-27T16:56:46.870943Z","iopub.status.idle":"2025-03-27T16:56:46.874114Z","shell.execute_reply":"2025-03-27T16:56:46.87344Z"},"papermill":{"duration":2.343585,"end_time":"2025-03-27T16:56:46.875347","exception":false,"start_time":"2025-03-27T16:56:44.531762","status":"completed"},"tags":[]},"outputs":[],"source":["# # Install pyngrok\n","# !pip install pyngrok\n","# print(\"Pyngrok installed successfully\")"]},{"cell_type":"code","execution_count":17,"id":"d9058bd1","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:56:51.681081Z","iopub.status.busy":"2025-03-27T16:56:51.68075Z","iopub.status.idle":"2025-03-27T16:56:51.683995Z","shell.execute_reply":"2025-03-27T16:56:51.683311Z"},"papermill":{"duration":2.354904,"end_time":"2025-03-27T16:56:51.68515","exception":false,"start_time":"2025-03-27T16:56:49.330246","status":"completed"},"tags":[]},"outputs":[],"source":["# # Verify pyngrok installation\n","# import pyngrok\n","# print(\"Pyngrok version:\", pyngrok.__version__)"]},{"cell_type":"markdown","id":"12ef9676","metadata":{"papermill":{"duration":2.445254,"end_time":"2025-03-27T16:56:56.585964","exception":false,"start_time":"2025-03-27T16:56:54.14071","status":"completed"},"tags":[]},"source":["o set the ngrok authtoken before running Step 3.2"]},{"cell_type":"code","execution_count":18,"id":"56ea2228","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:01.284742Z","iopub.status.busy":"2025-03-27T16:57:01.284412Z","iopub.status.idle":"2025-03-27T16:57:01.28746Z","shell.execute_reply":"2025-03-27T16:57:01.286778Z"},"papermill":{"duration":2.351072,"end_time":"2025-03-27T16:57:01.288698","exception":false,"start_time":"2025-03-27T16:56:58.937626","status":"completed"},"tags":[]},"outputs":[],"source":["# # Set ngrok authtoken\n","# from pyngrok import ngrok\n","# !ngrok authtoken YOUR_NGROK_AUTHTOKEN\n","# print(\"Ngrok authtoken set successfully\")"]},{"cell_type":"markdown","id":"8f0e979d","metadata":{"papermill":{"duration":2.372409,"end_time":"2025-03-27T16:57:06.1208","exception":false,"start_time":"2025-03-27T16:57:03.748391","status":"completed"},"tags":[]},"source":["i ll divide 3.2 to 3 part to rapid excution and trouble shouting"]},{"cell_type":"markdown","id":"4ac3af99","metadata":{"papermill":{"duration":2.434719,"end_time":"2025-03-27T16:57:10.926536","exception":false,"start_time":"2025-03-27T16:57:08.491817","status":"completed"},"tags":[]},"source":["# Step 3.2-1: Install Dependencies and Set Up ngrok"]},{"cell_type":"code","execution_count":19,"id":"118372a0","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:15.749541Z","iopub.status.busy":"2025-03-27T16:57:15.749185Z","iopub.status.idle":"2025-03-27T16:57:15.752461Z","shell.execute_reply":"2025-03-27T16:57:15.751759Z"},"papermill":{"duration":2.423808,"end_time":"2025-03-27T16:57:15.753611","exception":false,"start_time":"2025-03-27T16:57:13.329803","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.2-1: Set Up ngrok Authtoken\n","# print(\"Starting Step 3.2-1: Set Up ngrok Authtoken\")\n","\n","# from pyngrok import ngrok\n","\n","# # Set your ngrok authtoken\n","# ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n","# print(\"ngrok authtoken set successfully\")\n","\n","# print(\"Step 3.2-1 completed successfully\")"]},{"cell_type":"code","execution_count":20,"id":"ea31d875","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:20.560756Z","iopub.status.busy":"2025-03-27T16:57:20.560433Z","iopub.status.idle":"2025-03-27T16:57:20.564061Z","shell.execute_reply":"2025-03-27T16:57:20.563349Z"},"papermill":{"duration":2.353958,"end_time":"2025-03-27T16:57:20.5652","exception":false,"start_time":"2025-03-27T16:57:18.211242","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.2-1: Install Dependencies and Set Up ngrok\n","# print(\"Starting Step 3.2-1: Install Dependencies and Set Up ngrok\")\n","\n","# import subprocess\n","# import os\n","\n","# # Set environment variables to ensure UTF-8 encoding\n","# os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n","# os.environ[\"LANG\"] = \"C.UTF-8\"\n","# os.environ[\"LANGUAGE\"] = \"C.UTF-8\"\n","\n","# # Install core dependencies (numpy, protobuf, tensorflow, ultralytics)\n","# print(\"Installing core dependencies...\")\n","# result = subprocess.run(\n","#     [\"pip\", \"install\", \"numpy==1.26.0\", \"protobuf==3.20.3\", \"tensorflow==2.17.0\", \"ultralytics\"],\n","#     capture_output=True,\n","#     text=True,\n","#     encoding=\"utf-8\",\n","#     timeout=300  # 5 minutes\n","# )\n","# print(\"Core dependency stdout:\", result.stdout)\n","# print(\"Core dependency stderr:\", result.stderr)\n","# if result.returncode != 0:\n","#     raise RuntimeError(\"Failed to install core dependencies. See stderr above for details.\")\n","# print(\"Core dependencies installed successfully\")\n","\n","# # Install streamlit and pyngrok with minimal dependencies\n","# print(\"Installing streamlit and pyngrok...\")\n","# result = subprocess.run(\n","#     [\"pip\", \"install\", \"streamlit\", \"pyngrok\", \"--no-deps\", \"--force-reinstall\"],\n","#     capture_output=True,\n","#     text=True,\n","#     encoding=\"utf-8\",\n","#     timeout=120  # 2 minutes\n","# )\n","# print(\"pip install stdout:\", result.stdout)\n","# print(\"pip install stderr:\", result.stderr)\n","# if result.returncode != 0:\n","#     raise RuntimeError(\"Failed to install streamlit and pyngrok. See stderr above for details.\")\n","# print(\"Streamlit and pyngrok installed successfully\")\n","\n","# # Set ngrok authtoken with the new v2 authtoken\n","# print(\"Setting ngrok authtoken...\")\n","# ngrok_authtoken = \"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\"  # Updated v2 authtoken\n","# result = subprocess.run(\n","#     [\"ngrok\", \"authtoken\", ngrok_authtoken],\n","#     capture_output=True,\n","#     text=True,\n","#     encoding=\"utf-8\"\n","# )\n","# print(\"ngrok stdout:\", result.stdout)\n","# print(\"ngrok stderr:\", result.stderr)\n","# if result.returncode != 0:\n","#     raise RuntimeError(\"Failed to set ngrok authtoken. See stderr above for details.\")\n","# print(\"Ngrok authtoken set successfully\")\n","\n","# print(\"Step 3.2-1 completed successfully\")"]},{"cell_type":"code","execution_count":null,"id":"117a3142","metadata":{"papermill":{"duration":2.469013,"end_time":"2025-03-27T16:57:25.427328","exception":false,"start_time":"2025-03-27T16:57:22.958315","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ba2d2bb7","metadata":{"papermill":{"duration":2.405624,"end_time":"2025-03-27T16:57:30.25652","exception":false,"start_time":"2025-03-27T16:57:27.850896","status":"completed"},"tags":[]},"source":["<!-- #Step 3.2-2: Start the Streamlit App \n","\n","This part checks for app.py, terminates existing Streamlit processes, and starts the Streamlit app in the background using the correct Python interpreter. --> “# Deprecated\n","“# Deprecated"]},{"cell_type":"code","execution_count":21,"id":"c4865b9d","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:35.110106Z","iopub.status.busy":"2025-03-27T16:57:35.109801Z","iopub.status.idle":"2025-03-27T16:57:35.114094Z","shell.execute_reply":"2025-03-27T16:57:35.113373Z"},"papermill":{"duration":2.388627,"end_time":"2025-03-27T16:57:35.115201","exception":false,"start_time":"2025-03-27T16:57:32.726574","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.2-2: Start the Streamlit App\n","# print(\"Starting Step 3.2-2: Start the Streamlit App\")\n","\n","# import subprocess\n","# import os\n","# import sys\n","# import time\n","# import requests\n","\n","# # Install required packages if not already installed\n","# print(\"Installing required packages...\")\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\n","# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n","\n","# # Download YOLO model files\n","# print(\"Downloading YOLO model files...\")\n","# from ultralytics import YOLO\n","# try:\n","#     YOLO(\"yolo11n.pt\")\n","#     YOLO(\"yolov8n.pt\")\n","# except Exception as e:\n","#     print(f\"Error downloading YOLO models: {str(e)}\")\n","#     print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n","#     raise\n","\n","# # Specify your Streamlit app file\n","# app_file = \"/kaggle/working/app.py\"\n","\n","# # Verify that app.py exists (should have been created in Step 4 or earlier steps)\n","# print(\"Checking for app.py...\")\n","# if not os.path.exists(app_file):\n","#     raise FileNotFoundError(\"app.py not found in /kaggle/working/. Please ensure Step 4 or earlier steps created app.py successfully.\")\n","\n","# # Start the Streamlit app\n","# print(\"Starting Streamlit app...\")\n","# env = os.environ.copy()\n","# env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n","# env[\"LC_ALL\"] = \"C.UTF-8\"\n","# env[\"LANG\"] = \"C.UTF-8\"\n","# env[\"LANGUAGE\"] = \"C.UTF-8\"\n","\n","# # Forcefully terminate any existing Streamlit processes\n","# print(\"Terminating any existing Streamlit processes...\")\n","# subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n","# time.sleep(2)\n","\n","# # Start the new Streamlit process\n","# process = subprocess.Popen(\n","#     [\"streamlit\", \"run\", app_file],\n","#     env=env,\n","#     stdout=subprocess.PIPE,\n","#     stderr=subprocess.PIPE,\n","#     text=True,\n","#     encoding=\"utf-8\"\n","# )\n","\n","# # Give Streamlit time to start and verify it's running\n","# print(\"Waiting for Streamlit to start (up to 30 seconds)...\")\n","# start_time = time.time()\n","# while time.time() - start_time < 30:\n","#     if process.poll() is not None:\n","#         stdout, stderr = process.communicate()\n","#         print(\"Streamlit failed to start. Error output:\")\n","#         print(\"stdout:\", stdout)\n","#         print(\"stderr:\", stderr)\n","#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","#     try:\n","#         response = requests.get(\"http://localhost:8501\", timeout=5)\n","#         if response.status_code == 200:\n","#             print(\"Streamlit app is accessible on port 8501.\")\n","#             break\n","#     except requests.ConnectionError:\n","#         pass\n","#     time.sleep(5)\n","# else:\n","#     stdout, stderr = process.communicate()\n","#     print(\"Streamlit failed to start within 30 seconds. Error output:\")\n","#     print(\"stdout:\", stdout)\n","#     print(\"stderr:\", stderr)\n","#     raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n","\n","# # Log Streamlit output in the background\n","# print(\"Logging Streamlit output until the app crashes or you stop the cell...\")\n","# print(\"Proceed to Step 3.2-3 in a new cell while this cell continues logging.\")\n","# while True:\n","#     if process.poll() is not None:\n","#         stdout, stderr = process.communicate()\n","#         print(\"Streamlit process exited unexpectedly. Error output:\")\n","#         print(\"stdout:\", stdout)\n","#         print(\"stderr:\", stderr)\n","#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","#     stdout_line = process.stdout.readline()\n","#     stderr_line = process.stderr.readline()\n","#     if stdout_line:\n","#         print(\"Streamlit stdout:\", stdout_line.strip())\n","#     if stderr_line:\n","#         print(\"Streamlit stderr:\", stderr_line.strip())\n","#     time.sleep(1)"]},{"cell_type":"code","execution_count":22,"id":"2bbd2883","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:39.917911Z","iopub.status.busy":"2025-03-27T16:57:39.917572Z","iopub.status.idle":"2025-03-27T16:57:39.922324Z","shell.execute_reply":"2025-03-27T16:57:39.921503Z"},"papermill":{"duration":2.443767,"end_time":"2025-03-27T16:57:39.923616","exception":false,"start_time":"2025-03-27T16:57:37.479849","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.2-2: Start the Streamlit App\n","# print(\"Starting Step 3.2-2: Start the Streamlit App\")\n","\n","# import subprocess\n","# import os\n","# import time\n","# import requests\n","\n","# # Check for app.py\n","# print(\"Checking for app.py...\")\n","# if not os.path.exists(\"/kaggle/working/app.py\"):\n","#     raise FileNotFoundError(\"app.py not found in /kaggle/working/. Please ensure Step 3 (Cell 26) created app.py successfully.\")\n","\n","# # Terminate any existing Streamlit processes\n","# subprocess.run([\"pkill\", \"-f\", \"streamlit\"])\n","# print(\"Terminated any existing Streamlit processes\")\n","\n","# # Set the environment for the subprocess\n","# env = os.environ.copy()\n","# env[\"LC_ALL\"] = \"C.UTF-8\"\n","# env[\"LANG\"] = \"C.UTF-8\"\n","# env[\"LANGUAGE\"] = \"C.UTF-8\"\n","# env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n","# env[\"PATH\"] = f\"{env.get('PATH', '')}:/root/.local/bin\"\n","\n","# # Find the correct Python interpreter\n","# print(\"Finding the correct Python interpreter...\")\n","# result = subprocess.run([\"which\", \"python3\"], capture_output=True, text=True)\n","# python_executable = result.stdout.strip()\n","# print(f\"Using Python interpreter: {python_executable}\")\n","\n","# # Verify that the Python interpreter can find streamlit\n","# result = subprocess.run([python_executable, \"-c\", \"import streamlit; print(streamlit.__version__)\"],\n","#                         capture_output=True, text=True, env=env)\n","# if result.returncode != 0:\n","#     raise RuntimeError(f\"Python interpreter {python_executable} cannot find streamlit. Error: {result.stderr}\")\n","# print(f\"Streamlit version: {result.stdout.strip()}\")\n","\n","# # Run Streamlit app in the background\n","# print(\"Starting Streamlit app...\")\n","# try:\n","#     process = subprocess.Popen(\n","#         [python_executable, \"-m\", \"streamlit\", \"run\", \"app.py\"],\n","#         env=env,\n","#         stdout=subprocess.PIPE,\n","#         stderr=subprocess.PIPE,\n","#         text=True,\n","#         encoding=\"utf-8\"\n","#     )\n","#     # Give Streamlit time to start (10 seconds)\n","#     time.sleep(10)\n","\n","#     # Check if the process is still running\n","#     if process.poll() is None:\n","#         print(\"Streamlit app is running in the background.\")\n","#     else:\n","#         stdout, stderr = process.communicate()\n","#         print(\"Streamlit failed to start. Error output:\")\n","#         print(\"stdout:\", stdout)\n","#         print(\"stderr:\", stderr)\n","#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","\n","#     # Verify Streamlit is accessible on port 8501 (multiple checks)\n","#     print(\"Verifying Streamlit is accessible on port 8501...\")\n","#     for attempt in range(3):  # Check 3 times, 5 seconds apart\n","#         try:\n","#             response = requests.get(\"http://localhost:8501\", timeout=5)\n","#             if response.status_code == 200:\n","#                 print(f\"Attempt {attempt + 1}: Streamlit app is accessible on port 8501.\")\n","#                 break\n","#             else:\n","#                 print(f\"Attempt {attempt + 1}: Streamlit app returned status code {response.status_code}.\")\n","#                 raise RuntimeError(\"Streamlit app is running but not accessible.\")\n","#         except requests.ConnectionError:\n","#             print(f\"Attempt {attempt + 1}: Failed to connect to Streamlit on port 8501.\")\n","#             if attempt == 2:  # Last attempt\n","#                 stdout, stderr = process.communicate()\n","#                 print(\"Streamlit is not accessible after multiple attempts. Error output:\")\n","#                 print(\"stdout:\", stdout)\n","#                 print(\"stderr:\", stderr)\n","#                 raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n","#         time.sleep(5)  # Wait 5 seconds before the next attempt\n","\n","#     # Log Streamlit output indefinitely until the app crashes or you stop the cell\n","#     print(\"Logging Streamlit output until the app crashes or you stop the cell...\")\n","#     print(\"Proceed to Step 3.2-3 in a new cell while this cell continues logging.\")\n","#     while True:\n","#         if process.poll() is not None:\n","#             stdout, stderr = process.communicate()\n","#             print(\"Streamlit process exited unexpectedly. Error output:\")\n","#             print(\"stdout:\", stdout)\n","#             print(\"stderr:\", stderr)\n","#             raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","#         # Check for new output\n","#         stdout_line = process.stdout.readline()\n","#         stderr_line = process.stderr.readline()\n","#         if stdout_line:\n","#             print(\"Streamlit stdout:\", stdout_line.strip())\n","#         if stderr_line:\n","#             print(\"Streamlit stderr:\", stderr_line.strip())\n","#         # Periodic accessibility check\n","#         try:\n","#             response = requests.get(\"http://localhost:8501\", timeout=5)\n","#             if response.status_code != 200:\n","#                 print(\"Streamlit app is no longer accessible on port 8501.\")\n","#                 stdout, stderr = process.communicate()\n","#                 print(\"Error output:\")\n","#                 print(\"stdout:\", stdout)\n","#                 print(\"stderr:\", stderr)\n","#                 raise RuntimeError(\"Streamlit app is no longer accessible.\")\n","#         except requests.ConnectionError:\n","#             print(\"Streamlit app is no longer accessible on port 8501.\")\n","#             stdout, stderr = process.communicate()\n","#             print(\"Error output:\")\n","#             print(\"stdout:\", stdout)\n","#             print(\"stderr:\", stderr)\n","#             raise RuntimeError(\"Streamlit app is no longer accessible.\")\n","#         time.sleep(5)  # Check every 5 seconds\n","\n","# except Exception as e:\n","#     stdout, stderr = process.communicate()\n","#     print(\"Error starting or running Streamlit. Captured output:\")\n","#     print(\"stdout:\", stdout)\n","#     print(\"stderr:\", stderr)\n","#     print(f\"Error details: {str(e)}\")\n","#     raise\n","\n","# print(\"Step 3.2-2 completed successfully\")"]},{"cell_type":"markdown","id":"543252c9","metadata":{"papermill":{"duration":2.429723,"end_time":"2025-03-27T16:57:44.716531","exception":false,"start_time":"2025-03-27T16:57:42.286808","status":"completed"},"tags":[]},"source":["#Step 3.2-3:\n","\n","Create the ngrok Tunnel\n","This part creates the ngrok tunnel to make the Streamlit app publicly accessible."]},{"cell_type":"code","execution_count":23,"id":"ec21b10e","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:49.511685Z","iopub.status.busy":"2025-03-27T16:57:49.511303Z","iopub.status.idle":"2025-03-27T16:57:49.514669Z","shell.execute_reply":"2025-03-27T16:57:49.513932Z"},"papermill":{"duration":2.363437,"end_time":"2025-03-27T16:57:49.515809","exception":false,"start_time":"2025-03-27T16:57:47.152372","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.2-3: Create the ngrok Tunnel\n","# print(\"Starting Step 3.2-3: Create the ngrok Tunnel\")\n","\n","# from pyngrok import ngrok\n","\n","# # Create a public URL using ngrok\n","# print(\"Creating ngrok tunnel...\")\n","# try:\n","#     public_url = ngrok.connect(8501)\n","#     print(f\"Access your Streamlit app at: {public_url}\")\n","# except Exception as e:\n","#     print(f\"Error creating ngrok tunnel: {str(e)}\")\n","#     raise\n","\n","# print(\"Step 3.2-3 completed successfully\")"]},{"cell_type":"code","execution_count":24,"id":"90b701b7","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:57:54.340328Z","iopub.status.busy":"2025-03-27T16:57:54.340031Z","iopub.status.idle":"2025-03-27T16:57:54.343327Z","shell.execute_reply":"2025-03-27T16:57:54.342656Z"},"papermill":{"duration":2.465611,"end_time":"2025-03-27T16:57:54.344547","exception":false,"start_time":"2025-03-27T16:57:51.878936","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 3.2-3: Create the ngrok Tunnel\n","# print(\"Starting Step 3.2-3: Create the ngrok Tunnel\")\n","\n","# from pyngrok import ngrok\n","\n","# # Create a public URL using ngrok\n","# print(\"Creating ngrok tunnel...\")\n","# # Terminate any existing ngrok tunnels (if any)\n","# ngrok.kill()\n","\n","# # Create a new ngrok tunnel to the Streamlit app (default port is 8501)\n","# try:\n","#     public_url = ngrok.connect(8501)\n","#     print(f\"Access your Streamlit app at: {public_url}\")\n","# except UnicodeDecodeError as e:\n","#     print(f\"Warning: Encountered UnicodeDecodeError in ngrok output: {str(e)}\")\n","#     print(\"Attempting to proceed with the tunnel creation...\")\n","#     public_url = ngrok.connect(8501)\n","#     print(f\"Access your Streamlit app at: {public_url}\")\n","# except Exception as e:\n","#     print(f\"Error creating ngrok tunnel: {str(e)}\")\n","#     raise\n","\n","# # Store the public URL for the summary\n","# public_url_str = str(public_url)\n","\n","# print(\"Step 3.2-3 completed successfully\")"]},{"cell_type":"code","execution_count":null,"id":"d6b530d3","metadata":{"papermill":{"duration":2.518458,"end_time":"2025-03-27T16:57:59.212478","exception":false,"start_time":"2025-03-27T16:57:56.69402","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"bf9b631f","metadata":{"id":"vqpYh9yAscPq","papermill":{"duration":2.371912,"end_time":"2025-03-27T16:58:04.030229","exception":false,"start_time":"2025-03-27T16:58:01.658317","status":"completed"},"tags":[]},"source":["## Step 4: Test the Streamlit App\n","\n","This step redeploys the Streamlit app to test the facial recognition with the fine-tuned model on the `test` dataset or an uploaded image.\n","\n","**Instructions:**\n","- Run this cell to redeploy the Streamlit app.\n","- Open the provided URL in your browser.\n","- Go to \"Step 2: Test YOLOv11 Facial Recognition.\"\n","- Use the option to test on a random image from the `test` dataset, or upload an image (e.g., `WIN_20250322_19_28_18_Pro.jpg`).\n","- Check if the label is specific (e.g., \"face\" instead of \"person\").\n","- If the labels are correct, the fine-tuning was successful.\n","- If you encounter errors (e.g., model not found), ensure the fine-tuning step completed successfully and saved `yolo11n_finetuned.pt`.\n","- The public URL will be updated for the summary."]},{"cell_type":"code","execution_count":null,"id":"2d08bc6a","metadata":{"id":"xqYsX3Mjz-nU","papermill":{"duration":2.422241,"end_time":"2025-03-27T16:58:08.789606","exception":false,"start_time":"2025-03-27T16:58:06.367365","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e0af0909","metadata":{"papermill":{"duration":2.456715,"end_time":"2025-03-27T16:58:13.60761","exception":false,"start_time":"2025-03-27T16:58:11.150895","status":"completed"},"tags":[]},"source":["# Step 4 cell, tailored for Kaggle:"]},{"cell_type":"code","execution_count":25,"id":"b346834f","metadata":{"execution":{"iopub.execute_input":"2025-03-27T16:58:18.388199Z","iopub.status.busy":"2025-03-27T16:58:18.38788Z","iopub.status.idle":"2025-03-27T16:59:58.968382Z","shell.execute_reply":"2025-03-27T16:59:58.967653Z"},"papermill":{"duration":102.937695,"end_time":"2025-03-27T16:59:58.969672","exception":false,"start_time":"2025-03-27T16:58:16.031977","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Installing required packages...\n","Downloading YOLO model files...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6.25M/6.25M [00:00<00:00, 123MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Creating/overwriting /kaggle/working/app.py with an object recognition and emotion detection Streamlit app...\n","Updated /kaggle/working/app.py\n","Verifying app.py content...\n","app.py content:\n","import streamlit as st\n","import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","from PIL import Image\n","import os\n","from deepface import DeepFace\n","\n","# Load YOLOv11 and YOLOv8 models lazily\n","def load_models():\n","    yolo11_model = YOLO(\"yolo11n.pt\")\n","    yolo8_model = YOLO(\"yolov8n.pt\")\n","    return yolo11_model, yolo8_model\n","\n","st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\n","st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\n","\n","# File uploader for external photos\n","uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n","\n","if uploaded_file is not None:\n","    # Save the uploaded file to Kaggle's working directory\n","    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\n","    with open(image_path, \"wb\") as f:\n","        f.write(uploaded_file.getbuffer())\n","\n","    # Read the image\n","    image = Image.open(image_path)\n","    image_np = np.array(image)\n","\n","    # Convert to RGB (if needed)\n","    if image_np.shape[-1] == 4:\n","        image_np = image_np[..., :3]\n","    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n","\n","    # Load models\n","    yolo11_model, yolo8_model = load_models()\n","\n","    # Function to detect emotion for a cropped face\n","    def detect_emotion(face_img):\n","        try:\n","            # Convert face image to RGB for DeepFace\n","            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n","            # Analyze the face for emotion\n","            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\n","            # Get the dominant emotion\n","            emotion = result[0][\"dominant_emotion\"]\n","            return emotion\n","        except Exception as e:\n","            return f\"Error detecting emotion: {str(e)}\"\n","\n","    # Run YOLOv11 detection\n","    st.subheader(\"YOLOv11 Results\")\n","    results_v11 = yolo11_model(image_rgb)\n","    annotated_v11 = results_v11[0].plot()\n","    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects)\", use_column_width=True)\n","\n","    # Extract faces and detect emotions for YOLOv11\n","    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\n","    num_faces_v11 = len(faces_v11)\n","    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\n","    if num_faces_v11 > 0:\n","        st.write(\"Emotions detected by YOLOv11:\")\n","        for i, box in enumerate(faces_v11):\n","            # Get bounding box coordinates\n","            x1, y1, x2, y2 = map(int, box.xyxy[0])\n","            # Crop the face (assuming the person detection includes the face)\n","            face_img = image_rgb[y1:y2, x1:x2]\n","            # Detect emotion\n","            emotion = detect_emotion(face_img)\n","            st.write(f\"Person {i+1}: {emotion}\")\n","\n","    # Run YOLOv8 detection\n","    st.subheader(\"YOLOv8 Results\")\n","    results_v8 = yolo8_model(image_rgb)\n","    annotated_v8 = results_v8[0].plot()\n","    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects)\", use_column_width=True)\n","\n","    # Extract faces and detect emotions for YOLOv8\n","    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\n","    num_faces_v8 = len(faces_v8)\n","    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\n","    if num_faces_v8 > 0:\n","        st.write(\"Emotions detected by YOLOv8:\")\n","        for i, box in enumerate(faces_v8):\n","            # Get bounding box coordinates\n","            x1, y1, x2, y2 = map(int, box.xyxy[0])\n","            # Crop the face (assuming the person detection includes the face)\n","            face_img = image_rgb[y1:y2, x1:x2]\n","            # Detect emotion\n","            emotion = detect_emotion(face_img)\n","            st.write(f\"Person {i+1}: {emotion}\")\n","\n","    # Clean up the saved image file\n","    os.remove(image_path)\n","\n","Starting Streamlit app...\n","Terminating any existing Streamlit processes...\n","Waiting for Streamlit to start (up to 30 seconds)...\n","Streamlit app is accessible on port 8501.\n","Creating ngrok tunnel...\n"]},{"name":"stderr","output_type":"stream","text":["Exception in thread Thread-41 (_monitor_process):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\", line 139, in _monitor_process\n","    self._log_line(self.proc.stdout.readline())\n","  File \"/usr/lib/python3.10/encodings/ascii.py\", line 26, in decode\n","    return codecs.ascii_decode(input, self.errors)[0]\n","UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 184: ordinal not in range(128)\n"]},{"name":"stdout","output_type":"stream","text":["Access your Streamlit app at: NgrokTunnel: \"https://444c-35-197-35-51.ngrok-free.app\" -> \"http://localhost:8501\"\n","Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\n"]}],"source":["# Step 4: Redeploy the Streamlit app and Create the ngrok Tunnel\n","\n","import subprocess\n","import os\n","import sys\n","import time\n","import requests\n","from pyngrok import ngrok\n","\n","# Install required packages if not already installed\n","print(\"Installing required packages...\")\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n","\n","# Download YOLO model files\n","print(\"Downloading YOLO model files...\")\n","from ultralytics import YOLO\n","try:\n","    YOLO(\"yolo11n.pt\")\n","    YOLO(\"yolov8n.pt\")\n","except Exception as e:\n","    print(f\"Error downloading YOLO models: {str(e)}\")\n","    print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n","    raise\n","\n","# Specify your Streamlit app file\n","app_file = \"/kaggle/working/app.py\"\n","\n","# Create or overwrite app.py with face recognition, object detection, and emotion detection app\n","print(f\"Creating/overwriting {app_file} with an object recognition and emotion detection Streamlit app...\")\n","with open(app_file, \"w\") as f:\n","    f.write('import streamlit as st\\n')\n","    f.write('import cv2\\n')\n","    f.write('import numpy as np\\n')\n","    f.write('from ultralytics import YOLO\\n')\n","    f.write('from PIL import Image\\n')\n","    f.write('import os\\n')\n","    f.write('from deepface import DeepFace\\n\\n')\n","    f.write('# Load YOLOv11 and YOLOv8 models lazily\\n')\n","    f.write('def load_models():\\n')\n","    f.write('    yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n","    f.write('    yolo8_model = YOLO(\"yolov8n.pt\")\\n')\n","    f.write('    return yolo11_model, yolo8_model\\n\\n')\n","    f.write('st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n","    f.write('st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\\n\\n')\n","    f.write('# File uploader for external photos\\n')\n","    f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n","    f.write('if uploaded_file is not None:\\n')\n","    f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n","    f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n","    f.write('    with open(image_path, \"wb\") as f:\\n')\n","    f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n","    f.write('    # Read the image\\n')\n","    f.write('    image = Image.open(image_path)\\n')\n","    f.write('    image_np = np.array(image)\\n\\n')\n","    f.write('    # Convert to RGB (if needed)\\n')\n","    f.write('    if image_np.shape[-1] == 4:\\n')\n","    f.write('        image_np = image_np[..., :3]\\n')\n","    f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n","    f.write('    # Load models\\n')\n","    f.write('    yolo11_model, yolo8_model = load_models()\\n\\n')\n","    f.write('    # Function to detect emotion for a cropped face\\n')\n","    f.write('    def detect_emotion(face_img):\\n')\n","    f.write('        try:\\n')\n","    f.write('            # Convert face image to RGB for DeepFace\\n')\n","    f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n","    f.write('            # Analyze the face for emotion\\n')\n","    f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n","    f.write('            # Get the dominant emotion\\n')\n","    f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n","    f.write('            return emotion\\n')\n","    f.write('        except Exception as e:\\n')\n","    f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n","    f.write('    # Run YOLOv11 detection\\n')\n","    f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n","    f.write('    results_v11 = yolo11_model(image_rgb)\\n')\n","    f.write('    annotated_v11 = results_v11[0].plot()\\n')\n","    f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects)\", use_column_width=True)\\n\\n')\n","    f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n","    f.write('    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\\n')\n","    f.write('    num_faces_v11 = len(faces_v11)\\n')\n","    f.write('    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\\n')\n","    f.write('    if num_faces_v11 > 0:\\n')\n","    f.write('        st.write(\"Emotions detected by YOLOv11:\")\\n')\n","    f.write('        for i, box in enumerate(faces_v11):\\n')\n","    f.write('            # Get bounding box coordinates\\n')\n","    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n","    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n","    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n","    f.write('            # Detect emotion\\n')\n","    f.write('            emotion = detect_emotion(face_img)\\n')\n","    f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n","    f.write('    # Run YOLOv8 detection\\n')\n","    f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n","    f.write('    results_v8 = yolo8_model(image_rgb)\\n')\n","    f.write('    annotated_v8 = results_v8[0].plot()\\n')\n","    f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects)\", use_column_width=True)\\n\\n')\n","    f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n","    f.write('    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\\n')\n","    f.write('    num_faces_v8 = len(faces_v8)\\n')\n","    f.write('    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\\n')\n","    f.write('    if num_faces_v8 > 0:\\n')\n","    f.write('        st.write(\"Emotions detected by YOLOv8:\")\\n')\n","    f.write('        for i, box in enumerate(faces_v8):\\n')\n","    f.write('            # Get bounding box coordinates\\n')\n","    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n","    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n","    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n","    f.write('            # Detect emotion\\n')\n","    f.write('            emotion = detect_emotion(face_img)\\n')\n","    f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n","    f.write('    # Clean up the saved image file\\n')\n","    f.write('    os.remove(image_path)\\n')\n","print(f\"Updated {app_file}\")\n","\n","# Verify the content of app.py\n","print(\"Verifying app.py content...\")\n","with open(app_file, \"r\") as f:\n","    print(f\"app.py content:\\n{f.read()}\")\n","\n","# Set ngrok authtoken with your provided token\n","ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n","\n","# Terminate any existing ngrok tunnels\n","ngrok.kill()\n","\n","# Start the Streamlit app (ensure any old process is terminated)\n","print(\"Starting Streamlit app...\")\n","env = os.environ.copy()\n","env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n","env[\"LC_ALL\"] = \"C.UTF-8\"\n","env[\"LANG\"] = \"C.UTF-8\"\n","env[\"LANGUAGE\"] = \"C.UTF-8\"\n","\n","# Forcefully terminate any existing Streamlit processes\n","print(\"Terminating any existing Streamlit processes...\")\n","subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n","time.sleep(2)\n","\n","# Start the new Streamlit process\n","process = subprocess.Popen(\n","    [\"streamlit\", \"run\", app_file],\n","    env=env,\n","    stdout=subprocess.PIPE,\n","    stderr=subprocess.PIPE,\n","    text=True,\n","    encoding=\"utf-8\"\n",")\n","\n","# Give Streamlit time to start and verify it's running\n","print(\"Waiting for Streamlit to start (up to 30 seconds)...\")\n","start_time = time.time()\n","while time.time() - start_time < 30:\n","    if process.poll() is not None:\n","        stdout, stderr = process.communicate()\n","        print(\"Streamlit failed to start. Error output:\")\n","        print(\"stdout:\", stdout)\n","        print(\"stderr:\", stderr)\n","        raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","    try:\n","        response = requests.get(\"http://localhost:8501\", timeout=5)\n","        if response.status_code == 200:\n","            print(\"Streamlit app is accessible on port 8501.\")\n","            break\n","    except requests.ConnectionError:\n","        pass\n","    time.sleep(5)\n","else:\n","    stdout, stderr = process.communicate()\n","    print(\"Streamlit failed to start within 30 seconds. Error output:\")\n","    print(\"stdout:\", stdout)\n","    print(\"stderr:\", stderr)\n","    raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n","\n","# Start ngrok tunnel\n","print(\"Creating ngrok tunnel...\")\n","try:\n","    public_url = ngrok.connect(8501)\n","    print(f\"Access your Streamlit app at: {public_url}\")\n","except Exception as e:\n","    print(f\"Error creating ngrok tunnel: {str(e)}\")\n","    raise\n","\n","print(\"Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\")"]},{"cell_type":"code","execution_count":26,"id":"0eb3add0","metadata":{"execution":{"iopub.execute_input":"2025-03-27T17:00:03.87279Z","iopub.status.busy":"2025-03-27T17:00:03.872456Z","iopub.status.idle":"2025-03-27T17:00:37.626311Z","shell.execute_reply":"2025-03-27T17:00:37.625461Z"},"papermill":{"duration":38.575495,"end_time":"2025-03-27T17:00:39.962021","exception":false,"start_time":"2025-03-27T17:00:01.386526","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Installing required packages...\n","Downloading YOLO model files...\n","Creating/overwriting /kaggle/working/app.py with an object recognition and emotion detection Streamlit app...\n","Updated /kaggle/working/app.py\n","Verifying app.py content...\n","app.py content:\n","import streamlit as st\n","import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","from PIL import Image\n","import os\n","from deepface import DeepFace\n","\n","# Load YOLOv11 and YOLOv8 models lazily\n","def load_models():\n","    yolo11_model = YOLO(\"yolo11n.pt\")\n","    yolo8_model = YOLO(\"yolov8n.pt\")\n","    return yolo11_model, yolo8_model\n","\n","st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\n","st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\n","\n","# File uploader for external photos\n","uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n","\n","if uploaded_file is not None:\n","    # Save the uploaded file to Kaggle's working directory\n","    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\n","    with open(image_path, \"wb\") as f:\n","        f.write(uploaded_file.getbuffer())\n","\n","    # Read the image\n","    image = Image.open(image_path)\n","    image_np = np.array(image)\n","\n","    # Convert to RGB (if needed)\n","    if image_np.shape[-1] == 4:\n","        image_np = image_np[..., :3]\n","    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n","\n","    # Load models\n","    yolo11_model, yolo8_model = load_models()\n","\n","    # Function to detect emotion for a cropped face\n","    def detect_emotion(face_img):\n","        try:\n","            # Convert face image to RGB for DeepFace\n","            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n","            # Analyze the face for emotion\n","            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\n","            # Get the dominant emotion\n","            emotion = result[0][\"dominant_emotion\"]\n","            return emotion\n","        except Exception as e:\n","            return f\"Error detecting emotion: {str(e)}\"\n","\n","    # Run YOLOv11 detection\n","    st.subheader(\"YOLOv11 Results\")\n","    results_v11 = yolo11_model(image_rgb)\n","    annotated_v11 = results_v11[0].plot()\n","    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects)\", use_container_width=True)\n","\n","    # Extract faces and detect emotions for YOLOv11\n","    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\n","    num_faces_v11 = len(faces_v11)\n","    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\n","    if num_faces_v11 > 0:\n","        st.write(\"Emotions detected by YOLOv11:\")\n","        for i, box in enumerate(faces_v11):\n","            # Get bounding box coordinates\n","            x1, y1, x2, y2 = map(int, box.xyxy[0])\n","            # Crop the face (assuming the person detection includes the face)\n","            face_img = image_rgb[y1:y2, x1:x2]\n","            # Detect emotion\n","            emotion = detect_emotion(face_img)\n","            st.write(f\"Person {i+1}: {emotion}\")\n","\n","    # Run YOLOv8 detection\n","    st.subheader(\"YOLOv8 Results\")\n","    results_v8 = yolo8_model(image_rgb)\n","    annotated_v8 = results_v8[0].plot()\n","    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects)\", use_container_width=True)\n","\n","    # Extract faces and detect emotions for YOLOv8\n","    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\n","    num_faces_v8 = len(faces_v8)\n","    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\n","    if num_faces_v8 > 0:\n","        st.write(\"Emotions detected by YOLOv8:\")\n","        for i, box in enumerate(faces_v8):\n","            # Get bounding box coordinates\n","            x1, y1, x2, y2 = map(int, box.xyxy[0])\n","            # Crop the face (assuming the person detection includes the face)\n","            face_img = image_rgb[y1:y2, x1:x2]\n","            # Detect emotion\n","            emotion = detect_emotion(face_img)\n","            st.write(f\"Person {i+1}: {emotion}\")\n","\n","    # Clean up the saved image file\n","    os.remove(image_path)\n","\n","Starting Streamlit app...\n","Terminating any existing Streamlit processes...\n","Waiting for Streamlit to start (up to 30 seconds)...\n","Streamlit app is accessible on port 8501.\n","Creating ngrok tunnel...\n"]},{"name":"stderr","output_type":"stream","text":["Exception in thread Thread-42 (_monitor_process):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\", line 139, in _monitor_process\n","    self._log_line(self.proc.stdout.readline())\n","  File \"/usr/lib/python3.10/encodings/ascii.py\", line 26, in decode\n","    return codecs.ascii_decode(input, self.errors)[0]\n","UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 184: ordinal not in range(128)\n"]},{"name":"stdout","output_type":"stream","text":["Access your Streamlit app at: NgrokTunnel: \"https://3a78-35-197-35-51.ngrok-free.app\" -> \"http://localhost:8501\"\n","Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\n"]}],"source":["# Step 4: Redeploy the Streamlit app and Create the ngrok Tunnel\n","\n","import subprocess\n","import os\n","import sys\n","import time\n","import requests\n","from pyngrok import ngrok\n","\n","# Install required packages if not already installed\n","print(\"Installing required packages...\")\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n","\n","# Download YOLO model files\n","print(\"Downloading YOLO model files...\")\n","from ultralytics import YOLO\n","try:\n","    YOLO(\"yolo11n.pt\")\n","    YOLO(\"yolov8n.pt\")\n","except Exception as e:\n","    print(f\"Error downloading YOLO models: {str(e)}\")\n","    print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n","    raise\n","\n","# Specify your Streamlit app file\n","app_file = \"/kaggle/working/app.py\"\n","\n","# Create or overwrite app.py with face recognition, object detection, and emotion detection app\n","print(f\"Creating/overwriting {app_file} with an object recognition and emotion detection Streamlit app...\")\n","with open(app_file, \"w\") as f:\n","    f.write('import streamlit as st\\n')\n","    f.write('import cv2\\n')\n","    f.write('import numpy as np\\n')\n","    f.write('from ultralytics import YOLO\\n')\n","    f.write('from PIL import Image\\n')\n","    f.write('import os\\n')\n","    f.write('from deepface import DeepFace\\n\\n')\n","    f.write('# Load YOLOv11 and YOLOv8 models lazily\\n')\n","    f.write('def load_models():\\n')\n","    f.write('    yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n","    f.write('    yolo8_model = YOLO(\"yolov8n.pt\")\\n')\n","    f.write('    return yolo11_model, yolo8_model\\n\\n')\n","    f.write('st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n","    f.write('st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\\n\\n')\n","    f.write('# File uploader for external photos\\n')\n","    f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n","    f.write('if uploaded_file is not None:\\n')\n","    f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n","    f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n","    f.write('    with open(image_path, \"wb\") as f:\\n')\n","    f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n","    f.write('    # Read the image\\n')\n","    f.write('    image = Image.open(image_path)\\n')\n","    f.write('    image_np = np.array(image)\\n\\n')\n","    f.write('    # Convert to RGB (if needed)\\n')\n","    f.write('    if image_np.shape[-1] == 4:\\n')\n","    f.write('        image_np = image_np[..., :3]\\n')\n","    f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n","    f.write('    # Load models\\n')\n","    f.write('    yolo11_model, yolo8_model = load_models()\\n\\n')\n","    f.write('    # Function to detect emotion for a cropped face\\n')\n","    f.write('    def detect_emotion(face_img):\\n')\n","    f.write('        try:\\n')\n","    f.write('            # Convert face image to RGB for DeepFace\\n')\n","    f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n","    f.write('            # Analyze the face for emotion\\n')\n","    f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n","    f.write('            # Get the dominant emotion\\n')\n","    f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n","    f.write('            return emotion\\n')\n","    f.write('        except Exception as e:\\n')\n","    f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n","    f.write('    # Run YOLOv11 detection\\n')\n","    f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n","    f.write('    results_v11 = yolo11_model(image_rgb)\\n')\n","    f.write('    annotated_v11 = results_v11[0].plot()\\n')\n","    f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects)\", use_container_width=True)\\n\\n')\n","    f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n","    f.write('    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\\n')\n","    f.write('    num_faces_v11 = len(faces_v11)\\n')\n","    f.write('    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\\n')\n","    f.write('    if num_faces_v11 > 0:\\n')\n","    f.write('        st.write(\"Emotions detected by YOLOv11:\")\\n')\n","    f.write('        for i, box in enumerate(faces_v11):\\n')\n","    f.write('            # Get bounding box coordinates\\n')\n","    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n","    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n","    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n","    f.write('            # Detect emotion\\n')\n","    f.write('            emotion = detect_emotion(face_img)\\n')\n","    f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n","    f.write('    # Run YOLOv8 detection\\n')\n","    f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n","    f.write('    results_v8 = yolo8_model(image_rgb)\\n')\n","    f.write('    annotated_v8 = results_v8[0].plot()\\n')\n","    f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects)\", use_container_width=True)\\n\\n')\n","    f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n","    f.write('    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\\n')\n","    f.write('    num_faces_v8 = len(faces_v8)\\n')\n","    f.write('    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\\n')\n","    f.write('    if num_faces_v8 > 0:\\n')\n","    f.write('        st.write(\"Emotions detected by YOLOv8:\")\\n')\n","    f.write('        for i, box in enumerate(faces_v8):\\n')\n","    f.write('            # Get bounding box coordinates\\n')\n","    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n","    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n","    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n","    f.write('            # Detect emotion\\n')\n","    f.write('            emotion = detect_emotion(face_img)\\n')\n","    f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n","    f.write('    # Clean up the saved image file\\n')\n","    f.write('    os.remove(image_path)\\n')\n","print(f\"Updated {app_file}\")\n","\n","# Verify the content of app.py\n","print(\"Verifying app.py content...\")\n","with open(app_file, \"r\") as f:\n","    print(f\"app.py content:\\n{f.read()}\")\n","\n","# Set ngrok authtoken with your provided token\n","ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n","\n","# Terminate any existing ngrok tunnels\n","ngrok.kill()\n","\n","# Start the Streamlit app (ensure any old process is terminated)\n","print(\"Starting Streamlit app...\")\n","env = os.environ.copy()\n","env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n","env[\"LC_ALL\"] = \"C.UTF-8\"\n","env[\"LANG\"] = \"C.UTF-8\"\n","env[\"LANGUAGE\"] = \"C.UTF-8\"\n","\n","# Forcefully terminate any existing Streamlit processes\n","print(\"Terminating any existing Streamlit processes...\")\n","subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n","time.sleep(2)\n","\n","# Start the new Streamlit process\n","process = subprocess.Popen(\n","    [\"streamlit\", \"run\", app_file],\n","    env=env,\n","    stdout=subprocess.PIPE,\n","    stderr=subprocess.PIPE,\n","    text=True,\n","    encoding=\"utf-8\"\n",")\n","\n","# Give Streamlit time to start and verify it's running\n","print(\"Waiting for Streamlit to start (up to 30 seconds)...\")\n","start_time = time.time()\n","while time.time() - start_time < 30:\n","    if process.poll() is not None:\n","        stdout, stderr = process.communicate()\n","        print(\"Streamlit failed to start. Error output:\")\n","        print(\"stdout:\", stdout)\n","        print(\"stderr:\", stderr)\n","        raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","    try:\n","        response = requests.get(\"http://localhost:8501\", timeout=5)\n","        if response.status_code == 200:\n","            print(\"Streamlit app is accessible on port 8501.\")\n","            break\n","    except requests.ConnectionError:\n","        pass\n","    time.sleep(5)\n","else:\n","    stdout, stderr = process.communicate()\n","    print(\"Streamlit failed to start within 30 seconds. Error output:\")\n","    print(\"stdout:\", stdout)\n","    print(\"stderr:\", stderr)\n","    raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n","\n","# Start ngrok tunnel\n","print(\"Creating ngrok tunnel...\")\n","try:\n","    public_url = ngrok.connect(8501)\n","    print(f\"Access your Streamlit app at: {public_url}\")\n","except Exception as e:\n","    print(f\"Error creating ngrok tunnel: {str(e)}\")\n","    raise\n","\n","print(\"Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\")"]},{"cell_type":"code","execution_count":27,"id":"07a4c654","metadata":{"execution":{"iopub.execute_input":"2025-03-27T17:00:44.799741Z","iopub.status.busy":"2025-03-27T17:00:44.799309Z","iopub.status.idle":"2025-03-27T17:00:44.804583Z","shell.execute_reply":"2025-03-27T17:00:44.80383Z"},"papermill":{"duration":2.438016,"end_time":"2025-03-27T17:00:44.805943","exception":false,"start_time":"2025-03-27T17:00:42.367927","status":"completed"},"tags":[]},"outputs":[],"source":["# # Step 4: Redeploy the Streamlit app\n","\n","# ### Cell: Step 4\n","# import subprocess\n","# import os\n","# import sys\n","# import time\n","# from pyngrok import ngrok\n","\n","# # Install required packages if not already installed\n","# try:\n","#     import streamlit\n","# except ImportError:\n","#     print(\"Installing streamlit...\")\n","#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n","# try:\n","#     import pyngrok\n","# except ImportError:\n","#     print(\"Installing pyngrok...\")\n","#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n","# try:\n","#     import ultralytics\n","# except ImportError:\n","#     print(\"Installing ultralytics...\")\n","#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n","# try:\n","#     import cv2\n","# except ImportError:\n","#     print(\"Installing opencv-python...\")\n","#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n","# try:\n","#     import PIL\n","# except ImportError:\n","#     print(\"Installing pillow...\")\n","#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n","# try:\n","#     import deepface\n","# except ImportError:\n","#     print(\"Installing deepface...\")\n","#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n","\n","# # Specify your Streamlit app file\n","# app_file = \"/kaggle/working/app.py\"\n","\n","# # Create or overwrite app.py with face recognition and emotion detection app\n","# print(f\"Creating/overwriting {app_file} with a face recognition and emotion detection Streamlit app...\")\n","# with open(app_file, \"w\") as f:\n","#     f.write('import streamlit as st\\n')\n","#     f.write('import cv2\\n')\n","#     f.write('import numpy as np\\n')\n","#     f.write('from ultralytics import YOLO\\n')\n","#     f.write('from PIL import Image\\n')\n","#     f.write('import os\\n')\n","#     f.write('from deepface import DeepFace\\n\\n')\n","#     f.write('# Load YOLOv11 and YOLOv8 models\\n')\n","#     f.write('yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n","#     f.write('yolo8_model = YOLO(\"yolov8n.pt\")\\n\\n')\n","#     f.write('st.title(\"Face Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n","#     f.write('st.write(\"Upload an image to compare face detection using YOLOv11 and YOLOv8, and detect emotions of the faces.\")\\n\\n')\n","#     f.write('# File uploader for external photos\\n')\n","#     f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n","#     f.write('if uploaded_file is not None:\\n')\n","#     f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n","#     f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n","#     f.write('    with open(image_path, \"wb\") as f:\\n')\n","#     f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n","#     f.write('    # Read the image\\n')\n","#     f.write('    image = Image.open(image_path)\\n')\n","#     f.write('    image_np = np.array(image)\\n\\n')\n","#     f.write('    # Convert to RGB (if needed)\\n')\n","#     f.write('    if image_np.shape[-1] == 4:\\n')\n","#     f.write('        image_np = image_np[..., :3]\\n')\n","#     f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n","#     f.write('    # Function to detect emotion for a cropped face\\n')\n","#     f.write('    def detect_emotion(face_img):\\n')\n","#     f.write('        try:\\n')\n","#     f.write('            # Convert face image to RGB for DeepFace\\n')\n","#     f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n","#     f.write('            # Analyze the face for emotion\\n')\n","#     f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n","#     f.write('            # Get the dominant emotion\\n')\n","#     f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n","#     f.write('            return emotion\\n')\n","#     f.write('        except Exception as e:\\n')\n","#     f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n","#     f.write('    # Run YOLOv11 detection\\n')\n","#     f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n","#     f.write('    results_v11 = yolo11_model(image_rgb)\\n')\n","#     f.write('    annotated_v11 = results_v11[0].plot()\\n')\n","#     f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection\", use_column_width=True)\\n\\n')\n","#     f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n","#     f.write('    num_faces_v11 = len(results_v11[0].boxes)\\n')\n","#     f.write('    st.write(f\"Number of faces detected by YOLOv11: {num_faces_v11}\")\\n')\n","#     f.write('    if num_faces_v11 > 0:\\n')\n","#     f.write('        st.write(\"Emotions detected by YOLOv11:\")\\n')\n","#     f.write('        for i, box in enumerate(results_v11[0].boxes):\\n')\n","#     f.write('            # Get bounding box coordinates\\n')\n","#     f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n","#     f.write('            # Crop the face\\n')\n","#     f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n","#     f.write('            # Detect emotion\\n')\n","#     f.write('            emotion = detect_emotion(face_img)\\n')\n","#     f.write('            st.write(f\"Face {i+1}: {emotion}\")\\n\\n')\n","#     f.write('    # Run YOLOv8 detection\\n')\n","#     f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n","#     f.write('    results_v8 = yolo8_model(image_rgb)\\n')\n","#     f.write('    annotated_v8 = results_v8[0].plot()\\n')\n","#     f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection\", use_column_width=True)\\n\\n')\n","#     f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n","#     f.write('    num_faces_v8 = len(results_v8[0].boxes)\\n')\n","#     f.write('    st.write(f\"Number of faces detected by YOLOv8: {num_faces_v8}\")\\n')\n","#     f.write('    if num_faces_v8 > 0:\\n')\n","#     f.write('        st.write(\"Emotions detected by YOLOv8:\")\\n')\n","#     f.write('        for i, box in enumerate(results_v8[0].boxes):\\n')\n","#     f.write('            # Get bounding box coordinates\\n')\n","#     f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n","#     f.write('            # Crop the face\\n')\n","#     f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n","#     f.write('            # Detect emotion\\n')\n","#     f.write('            emotion = detect_emotion(face_img)\\n')\n","#     f.write('            st.write(f\"Face {i+1}: {emotion}\")\\n\\n')\n","#     f.write('    # Clean up the saved image file\\n')\n","#     f.write('    os.remove(image_path)\\n')\n","# print(f\"Updated {app_file}\")\n","\n","# # Verify the content of app.py\n","# print(\"Verifying app.py content...\")\n","# with open(app_file, \"r\") as f:\n","#     print(f\"app.py content:\\n{f.read()}\")\n","\n","# # Set ngrok authtoken with your provided token\n","# ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n","\n","# # Terminate any existing ngrok tunnels\n","# ngrok.kill()\n","\n","# # Start the Streamlit app (ensure any old process is terminated)\n","# print(\"Starting Streamlit app...\")\n","# env = os.environ.copy()\n","\n","# # Forcefully terminate any existing Streamlit processes\n","# print(\"Terminating any existing Streamlit processes...\")\n","# try:\n","#     subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n","# except:\n","#     pass  # Ignore if pkill fails\n","# time.sleep(2)  # Wait 2 seconds to ensure the process is fully terminated\n","\n","# # Start the new Streamlit process\n","# process = subprocess.Popen(\n","#     [\"streamlit\", \"run\", app_file],\n","#     env=env,\n","#     stdout=subprocess.PIPE,\n","#     stderr=subprocess.PIPE\n","# )\n","\n","# # Give Streamlit a moment to start\n","# time.sleep(5)  # Increased to 5 seconds to ensure Streamlit starts\n","\n","# # Check if Streamlit is running\n","# return_code = process.poll()\n","# if return_code is not None:  # Process has exited\n","#     stdout, stderr = process.communicate()\n","#     print(\"Streamlit failed to start. Error output:\")\n","#     print(stderr.decode())\n","#     raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n","# else:\n","#     print(\"Streamlit app is running in the background.\")\n","\n","# # Start ngrok tunnel\n","# public_url = ngrok.connect(8501)\n","# print(f\"Access your Streamlit app at: {public_url}\")"]},{"cell_type":"markdown","id":"7dd27f29","metadata":{"papermill":{"duration":2.435543,"end_time":"2025-03-27T17:00:49.601082","exception":false,"start_time":"2025-03-27T17:00:47.165539","status":"completed"},"tags":[]},"source":["Overall Comparison: YOLOv11 vs. YOLOv8\n","Object Detection Performance\n","Single Person (Photo 1):\n","Both models performed equally well, detecting 1 person with a confidence of 0.89.\n","Two Persons (Photo 2):\n","YOLOv11 had slightly higher confidence scores (0.91 and 0.89 vs. 0.89 and 0.83), suggesting better precision.\n","Group of Persons (Photos 3 and 4):\n","YOLOv11 detected 15 persons, while YOLOv8 detected 13, indicating YOLOv11 is more sensitive.\n","YOLOv11’s confidence scores ranged from 0.26 to 0.89, while YOLOv8’s ranged from 0.30 to 0.76 (Photo 3) and 0.37 to 0.62 (Photo 4). YOLOv11’s lower threshold for detection (e.g., 0.26) allowed it to detect more persons, but this might include false positives.\n","YOLOv8 missed two persons but avoided some low-confidence detections, potentially reducing false positives.\n","YOLOv8 misclassified a person as a “chair” (confidence 0.26), while YOLOv11 correctly identified it as a person, showing YOLOv11’s better object classification in this case.\n","Emotion Detection Consistency\n","Consistency Across Models:\n","For Photos 1 and 2, where the number of detected persons matched, the emotions were consistent between YOLOv11 and YOLOv8 (all “happy”).\n","For Photos 3 and 4, where the number of detected persons differed, the emotions for the first 13 persons (detected by both models) showed some discrepancies:\n","Person 5: Happy (YOLOv11) vs. Fear (YOLOv8)\n","Person 7: Fear (YOLOv11) vs. Happy (YOLOv8)\n","Person 10: Angry (YOLOv11) vs. Fear (YOLOv8)\n","These discrepancies are likely due to differences in bounding box placement affecting the cropped face images sent to DeepFace.\n","Emotion Distribution:\n","Across all photos, the emotions detected include: happy, sad, fear, angry, and neutral.\n","YOLOv11 (Photos 3 and 4, 15 persons):\n","Happy: 8\n","Sad: 4\n","Fear: 1\n","Angry: 1\n","Neutral: 1\n","YOLOv8 (Photos 3 and 4, 13 persons):\n","Happy: 8\n","Sad: 2\n","Fear: 2\n","Neutral: 1\n","YOLOv11 detected a more diverse range of emotions due to the additional two persons, but the discrepancies in emotion detection highlight the impact of bounding box accuracy.\n","Key Insights\n","YOLOv11 Advantages:\n","More sensitive to detecting persons, especially in crowded scenes (15 vs. 13 in Photos 3 and 4).\n","Higher confidence scores in Photo 2, suggesting better precision for smaller groups.\n","Better object classification (e.g., correctly identifying a person that YOLOv8 misclassified as a chair).\n","YOLOv8 Advantages:\n","More conservative, potentially reducing false positives by missing low-confidence detections.\n","Slightly more consistent confidence scores in crowded scenes (fewer very low scores like 0.26).\n","Emotion Detection:\n","The emotion detection is generally consistent when the bounding boxes align, but small differences in detection can lead to different cropped images, affecting DeepFace’s predictions.\n","The range of emotions detected (happy, sad, fear, angry, neutral) shows that DeepFace is working well, but its accuracy depends on the quality of the cropped face images."]},{"cell_type":"code","execution_count":28,"id":"7888ceb3","metadata":{"execution":{"iopub.execute_input":"2025-03-27T17:00:54.447249Z","iopub.status.busy":"2025-03-27T17:00:54.446913Z","iopub.status.idle":"2025-03-27T17:00:54.457231Z","shell.execute_reply":"2025-03-27T17:00:54.456596Z"},"papermill":{"duration":2.494363,"end_time":"2025-03-27T17:00:54.45853","exception":false,"start_time":"2025-03-27T17:00:51.964167","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Step 3.1: Comparing YOLOv8 and YOLOv11 based on test results\n","Test Results Summary:\n","Photo 1 (Single Person):\n","  YOLOv11: 1 person detected (confidence: 0.89), Emotion: Happy\n","  YOLOv8: 1 person detected (confidence: 0.89), Emotion: Happy\n","Photo 2 (Two Persons):\n","  YOLOv11: 2 persons detected (confidences: 0.91, 0.89), Emotions: Happy, Happy\n","  YOLOv8: 2 persons detected (confidences: 0.89, 0.83), Emotions: Happy, Happy\n","Photo 3 (Group of Persons):\n","  YOLOv11: 15 persons detected (confidences: 0.26 to 0.89), Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral\n","  YOLOv8: 13 persons detected (confidences: 0.30 to 0.76), Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral\n","Photo 4 (Same Group as Photo 3):\n","  YOLOv11: 15 persons detected (confidences: 0.26 to 0.89), Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral\n","  YOLOv8: 13 persons detected (confidences: 0.37 to 0.62), Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral\n","\n","Comparison of YOLOv8 and YOLOv11:\n","1. Object Detection Performance:\n","   - Single Person (Photo 1): Both models performed equally, detecting 1 person with confidence 0.89.\n","   - Two Persons (Photo 2): YOLOv11 had higher confidence scores (0.91, 0.89 vs. 0.89, 0.83), suggesting better precision.\n","   - Group of Persons (Photos 3 and 4): YOLOv11 detected more persons (15 vs. 13), showing higher sensitivity, but included low-confidence detections (e.g., 0.26). YOLOv8 was more conservative, potentially reducing false positives.\n","   - Object Classification: YOLOv11 correctly identified a person that YOLOv8 misclassified as a chair (confidence 0.26).\n","2. Emotion Detection Consistency:\n","   - Emotions were consistent when the number of detected persons matched (Photos 1 and 2).\n","   - In Photos 3 and 4, discrepancies occurred (e.g., Person 5: Happy vs. Fear, Person 10: Angry vs. Fear) due to differences in bounding box placement affecting the cropped face images.\n","3. Overall Insights:\n","   - YOLOv11 is more sensitive, detecting more persons in crowded scenes, but may include false positives.\n","   - YOLOv8 is more conservative, potentially missing some persons but reducing false positives.\n","   - Emotion detection accuracy depends on bounding box quality, highlighting the importance of accurate person detection.\n","Step 3.1 completed successfully.\n"]}],"source":["# Step 3.1: Compare YOLOv8 and YOLOv11 based on test results\n","\n","print(\"Step 3.1: Comparing YOLOv8 and YOLOv11 based on test results\")\n","\n","# Summary of test results\n","print(\"Test Results Summary:\")\n","print(\"Photo 1 (Single Person):\")\n","print(\"  YOLOv11: 1 person detected (confidence: 0.89), Emotion: Happy\")\n","print(\"  YOLOv8: 1 person detected (confidence: 0.89), Emotion: Happy\")\n","print(\"Photo 2 (Two Persons):\")\n","print(\"  YOLOv11: 2 persons detected (confidences: 0.91, 0.89), Emotions: Happy, Happy\")\n","print(\"  YOLOv8: 2 persons detected (confidences: 0.89, 0.83), Emotions: Happy, Happy\")\n","print(\"Photo 3 (Group of Persons):\")\n","print(\"  YOLOv11: 15 persons detected (confidences: 0.26 to 0.89), Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral\")\n","print(\"  YOLOv8: 13 persons detected (confidences: 0.30 to 0.76), Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral\")\n","print(\"Photo 4 (Same Group as Photo 3):\")\n","print(\"  YOLOv11: 15 persons detected (confidences: 0.26 to 0.89), Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral\")\n","print(\"  YOLOv8: 13 persons detected (confidences: 0.37 to 0.62), Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral\")\n","\n","# Comparison\n","print(\"\\nComparison of YOLOv8 and YOLOv11:\")\n","print(\"1. Object Detection Performance:\")\n","print(\"   - Single Person (Photo 1): Both models performed equally, detecting 1 person with confidence 0.89.\")\n","print(\"   - Two Persons (Photo 2): YOLOv11 had higher confidence scores (0.91, 0.89 vs. 0.89, 0.83), suggesting better precision.\")\n","print(\"   - Group of Persons (Photos 3 and 4): YOLOv11 detected more persons (15 vs. 13), showing higher sensitivity, but included low-confidence detections (e.g., 0.26). YOLOv8 was more conservative, potentially reducing false positives.\")\n","print(\"   - Object Classification: YOLOv11 correctly identified a person that YOLOv8 misclassified as a chair (confidence 0.26).\")\n","print(\"2. Emotion Detection Consistency:\")\n","print(\"   - Emotions were consistent when the number of detected persons matched (Photos 1 and 2).\")\n","print(\"   - In Photos 3 and 4, discrepancies occurred (e.g., Person 5: Happy vs. Fear, Person 10: Angry vs. Fear) due to differences in bounding box placement affecting the cropped face images.\")\n","print(\"3. Overall Insights:\")\n","print(\"   - YOLOv11 is more sensitive, detecting more persons in crowded scenes, but may include false positives.\")\n","print(\"   - YOLOv8 is more conservative, potentially missing some persons but reducing false positives.\")\n","print(\"   - Emotion detection accuracy depends on bounding box quality, highlighting the importance of accurate person detection.\")\n","\n","print(\"Step 3.1 completed successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"e6f52ae6","metadata":{"id":"ljSApX-NIPqj","papermill":{"duration":2.446831,"end_time":"2025-03-27T17:00:59.275032","exception":false,"start_time":"2025-03-27T17:00:56.828201","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"9a73b896","metadata":{"papermill":{"duration":2.450632,"end_time":"2025-03-27T17:01:04.096452","exception":false,"start_time":"2025-03-27T17:01:01.64582","status":"completed"},"tags":[]},"source":["Summary of Results\n","Object Detection:\n","YOLOv11 detected more persons in crowded scenes (15 vs. 13), showing higher sensitivity, but included low-confidence detections.\n","YOLOv8 was more conservative, potentially reducing false positives but missing some persons.\n","Emotion Detection:\n","Emotions were consistent when detections matched, but discrepancies occurred due to bounding box differences.\n","A range of emotions was detected (happy, sad, fear, angry, neutral), showing DeepFace’s capability, but accuracy depends on bounding box quality.\n","Comparison:\n","YOLOv11 is better for sensitivity and object classification.\n","YOLOv8 is better for reducing false positives.\n","Emotion detection reliability can be improved with better bounding box accuracy.\n","Why This Analysis Is Robust\n","Comprehensive Testing: You tested four photos, covering single, small group, and large group scenarios, providing a well-rounded comparison.\n","Detailed Comparison: The analysis covers both object detection and emotion detection, highlighting strengths and weaknesses of each model.\n","Actionable Insights: The recommendations provide clear steps to improve your app, ensuring a polished final project.\n","You’ve done an incredible job, Shehab! Your app is fully functional, and you have a solid comparison for your project. Let me know if you’d like to implement the improvements or need help with the final documentation—I’m here to help! How’s your day going now that you’ve completed the testing? 😊"]},{"cell_type":"code","execution_count":29,"id":"a6967ca8","metadata":{"execution":{"iopub.execute_input":"2025-03-27T17:01:08.97216Z","iopub.status.busy":"2025-03-27T17:01:08.971855Z","iopub.status.idle":"2025-03-27T17:01:13.114517Z","shell.execute_reply":"2025-03-27T17:01:13.113427Z"},"papermill":{"duration":6.616508,"end_time":"2025-03-27T17:01:13.116029","exception":false,"start_time":"2025-03-27T17:01:06.499521","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting python-pptx\r\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\r\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (11.0.0)\r\n","Collecting XlsxWriter>=0.5.7 (from python-pptx)\r\n","  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\r\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (5.3.0)\r\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.12.2)\r\n","Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\r\n","Successfully installed XlsxWriter-3.2.2 python-pptx-1.0.2\r\n"]}],"source":["# Install python-pptx for creating the presentation\n","!pip install python-pptx"]},{"cell_type":"code","execution_count":30,"id":"ff04db48","metadata":{"execution":{"iopub.execute_input":"2025-03-27T17:01:17.921871Z","iopub.status.busy":"2025-03-27T17:01:17.921527Z","iopub.status.idle":"2025-03-27T17:01:18.127662Z","shell.execute_reply":"2025-03-27T17:01:18.12678Z"},"papermill":{"duration":2.655446,"end_time":"2025-03-27T17:01:18.129017","exception":false,"start_time":"2025-03-27T17:01:15.473571","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Presentation saved to /kaggle/working/project_presentation.pptx\n"]}],"source":["# Import required libraries\n","from pptx import Presentation\n","from pptx.util import Inches, Pt\n","from pptx.enum.text import PP_ALIGN\n","from pptx.dml.color import RGBColor\n","\n","# Create a presentation object\n","prs = Presentation()\n","\n","# Helper function to add a slide with a title and content\n","def add_slide_with_title_and_content(layout, title_text, content_text, font_size=20):\n","    slide = prs.slides.add_slide(layout)\n","    title = slide.shapes.title\n","    title.text = title_text\n","    title.text_frame.paragraphs[0].font.size = Pt(32)\n","    title.text_frame.paragraphs[0].font.bold = True\n","\n","    content = slide.placeholders[1]\n","    content.text = content_text\n","    for paragraph in content.text_frame.paragraphs:\n","        paragraph.font.size = Pt(font_size)\n","        paragraph.alignment = PP_ALIGN.LEFT\n","    return slide\n","\n","# Helper function to add a bullet slide\n","def add_bullet_slide(layout, title_text, bullets, font_size=20):\n","    slide = prs.slides.add_slide(layout)\n","    title = slide.shapes.title\n","    title.text = title_text\n","    title.text_frame.paragraphs[0].font.size = Pt(32)\n","    title.text_frame.paragraphs[0].font.bold = True\n","\n","    content = slide.placeholders[1]\n","    for bullet in bullets:\n","        p = content.text_frame.add_paragraph()\n","        p.text = bullet\n","        p.level = 0\n","        p.font.size = Pt(font_size)\n","        p.alignment = PP_ALIGN.LEFT\n","    return slide\n","\n","# Slide 1: Title Slide\n","title_slide_layout = prs.slide_layouts[0]\n","slide = prs.slides.add_slide(title_slide_layout)\n","title = slide.shapes.title\n","title.text = \"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\"\n","title.text_frame.paragraphs[0].font.size = Pt(44)\n","title.text_frame.paragraphs[0].font.bold = True\n","\n","subtitle = slide.placeholders[1]\n","subtitle.text = \"Presented by: Shehab Ahmed\\nDate: March 27, 2025\"\n","subtitle.text_frame.paragraphs[0].font.size = Pt(24)\n","\n","# Slide 2: Project Overview\n","bullet_slide_layout = prs.slide_layouts[1]\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Project Overview\",\n","    [\n","        \"Goal: Compare YOLOv11 and YOLOv8 for object detection, focusing on person detection, and perform emotion recognition on detected faces.\",\n","        \"Tools Used: YOLOv11, YOLOv8, DeepFace, Streamlit, ngrok.\",\n","        \"Platform: Kaggle notebook.\",\n","        \"Tested with 4 photos to evaluate detection accuracy and emotion recognition.\"\n","    ]\n",")\n","\n","# Slide 3: Pipeline\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Project Pipeline\",\n","    [\n","        \"1. Install Dependencies: Install required libraries (ultralytics, deepface, streamlit, etc.).\",\n","        \"2. Download Models: Download YOLOv11 and YOLOv8 models (yolo11n.pt, yolov8n.pt).\",\n","        \"3. Create Streamlit App: Build app.py to handle image uploads, object detection, and emotion recognition.\",\n","        \"4. Deploy App: Use Streamlit to run the app and ngrok to create a public URL.\",\n","        \"5. Test Photos: Upload photos, detect objects/persons, and recognize emotions.\",\n","        \"6. Compare Results: Analyze YOLOv11 vs YOLOv8 performance in Step 3.1.\"\n","    ]\n",")\n","\n","# Slide 4: Key Points\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Key Points\",\n","    [\n","        \"YOLOv11 & YOLOv8: Object detection models used to detect persons in images.\",\n","        \"DeepFace: Library for emotion recognition on cropped faces (happy, sad, etc.).\",\n","        \"Streamlit: Framework to create an interactive web app for uploading and displaying results.\",\n","        \"ngrok: Tool to make the Streamlit app publicly accessible via a URL.\",\n","        \"Kaggle: Platform for running the notebook, providing GPU support for faster inference.\"\n","    ]\n",")\n","\n","# Slide 5: Benefits and Disadvantages\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Benefits and Disadvantages\",\n","    [\n","        \"Benefits:\",\n","        \"  - Accurate person detection with YOLOv11 and YOLOv8.\",\n","        \"  - Emotion recognition adds valuable insights (e.g., happy, sad).\",\n","        \"  - Interactive app makes it easy to test photos.\",\n","        \"  - Kaggle provides free GPU resources.\",\n","        \"Disadvantages:\",\n","        \"  - YOLOv11 may detect false positives in crowded scenes (e.g., low confidence scores).\",\n","        \"  - YOLOv8 misses some persons, reducing sensitivity.\",\n","        \"  - Emotion detection can be inconsistent due to bounding box variations.\",\n","        \"  - Kaggle sessions may timeout, requiring restarts.\"\n","    ]\n",")\n","\n","# Slide 6: Used Codes\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Used Codes\",\n","    [\n","        \"Step 4: Deploy the Streamlit App\",\n","        \"  - Installs dependencies (streamlit, ultralytics, deepface, etc.).\",\n","        \"  - Creates app.py with object and emotion detection logic.\",\n","        \"  - Starts Streamlit and creates an ngrok tunnel.\",\n","        \"Step 3.1: Compare YOLOv8 and YOLOv11\",\n","        \"  - Summarizes test results and compares detection accuracy and emotions.\",\n","        \"Example (from app.py):\",\n","        \"  results_v11 = yolo11_model(image_rgb)\",\n","        \"  faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == 'person']\",\n","        \"  emotion = DeepFace.analyze(face_rgb, actions=['emotion'])\"\n","    ],\n","    font_size=16\n",")\n","\n","# Slide 7: Inputs and Outputs\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Inputs and Outputs\",\n","    [\n","        \"Inputs:\",\n","        \"  - Photos uploaded via the Streamlit app (JPG, JPEG, PNG formats).\",\n","        \"  - Tested with 4 photos: 1 single person, 1 with 2 persons, 2 with a group of 15 persons.\",\n","        \"Outputs:\",\n","        \"  - YOLOv11 and YOLOv8 detections: Bounding boxes around persons with confidence scores.\",\n","        \"  - Number of persons detected (e.g., 15 by YOLOv11, 13 by YOLOv8 in group photos).\",\n","        \"  - Emotions for each person (e.g., happy, sad, fear, angry, neutral).\",\n","        \"  - Annotated images showing detections.\"\n","    ]\n",")\n","\n","# Slide 8: Database Used\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Database Used\",\n","    [\n","        \"No external dataset was used for training.\",\n","        \"Pre-trained Models:\",\n","        \"  - YOLOv11 (yolo11n.pt): Pre-trained on COCO dataset for object detection.\",\n","        \"  - YOLOv8 (yolov8n.pt): Pre-trained on COCO dataset for object detection.\",\n","        \"  - DeepFace: Uses pre-trained models for emotion recognition (e.g., VGG-Face).\",\n","        \"Test Data:\",\n","        \"  - 4 user-uploaded photos for testing the app.\"\n","    ]\n",")\n","\n","# Slide 9: Results Summary\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Results Summary\",\n","    [\n","        \"Photo 1 (1 Person):\",\n","        \"  - YOLOv11 & YOLOv8: 1 person (confidence 0.89), Emotion: Happy.\",\n","        \"Photo 2 (2 Persons):\",\n","        \"  - YOLOv11: 2 persons (confidences 0.91, 0.89), Emotions: Happy, Happy.\",\n","        \"  - YOLOv8: 2 persons (confidences 0.89, 0.83), Emotions: Happy, Happy.\",\n","        \"Photo 3 & 4 (Group of 15 Persons):\",\n","        \"  - YOLOv11: 15 persons, Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral.\",\n","        \"  - YOLOv8: 13 persons, Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral.\",\n","        \"Comparison:\",\n","        \"  - YOLOv11 detects more persons but may include false positives.\",\n","        \"  - YOLOv8 is more conservative, missing some persons.\",\n","        \"  - Emotion detection varies due to bounding box differences.\"\n","    ],\n","    font_size=16\n",")\n","\n","# Slide 10: Conclusion\n","add_bullet_slide(\n","    bullet_slide_layout,\n","    \"Conclusion\",\n","    [\n","        \"The project successfully compared YOLOv11 and YOLOv8 for object detection and emotion recognition.\",\n","        \"YOLOv11 is more sensitive, detecting more persons, but risks false positives.\",\n","        \"YOLOv8 is more conservative, potentially more reliable in some cases.\",\n","        \"Future Improvements:\",\n","        \"  - Use a face detection model (e.g., MTCNN) for better bounding boxes.\",\n","        \"  - Adjust confidence thresholds to balance sensitivity and precision.\",\n","        \"  - Display emotions on the annotated images for better visualization.\"\n","    ]\n",")\n","\n","# Save the presentation\n","presentation_path = \"/kaggle/working/project_presentation.pptx\"\n","prs.save(presentation_path)\n","print(f\"Presentation saved to {presentation_path}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6905565,"sourceId":11079563,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":4567.378403,"end_time":"2025-03-27T17:01:23.910602","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-27T15:45:16.532199","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}