{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11079563,"sourceType":"datasetVersion","datasetId":6905565}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shehabahmed74/graduation-project-2-ver-3-yolo-11-vs-yolo-8-fa?scriptVersionId=230042782\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Graduation Project 2: YOLOv8 vs. YOLOv11 Facial Recognition Demo\n\nThis notebook implements a Streamlit app to compare YOLOv8 and YOLOv11 for facial recognition. The project includes the following steps:\n- **Step 1:** Install dependencies and verify GPU availability.\n- **Step 2:** Fine-tune YOLOv11 on a facial recognition dataset.\n- **Step 3:** Set up and deploy a Streamlit app to perform facial recognition and compare YOLOv8 and YOLOv11.\n- **Step 4:** Test the app and verify the results.\n- **Step 5:** Summarize the results and provide the public URL for the app.\n\nThe dataset used is `shehabahmed74/shehab-data-facial-recognition`, which contains three folders: `train`, `valid`, and `test`, each with `images` and `labels` subfolders. We will use `train` for training, `valid` for validation, and `test` for testing in the Streamlit app.\n\nThe notebook is structured to be professional and compatible with GitHub for project delivery.\n\n**Execution Time Estimate:**\n- With GPU: ~15–25 minutes (fine-tuning takes ~10–20 minutes for 10 epochs).\n- Without GPU (on CPU): ~30–60 minutes (fine-tuning takes ~20–40 minutes for 10 epochs).\n- To optimize performance, ensure a GPU runtime is selected (see Step 1.2).\n\n---\n\n## Step 1: Install Dependencies\n\nThis cell installs the required Python packages for the project, including Streamlit, ngrok, Ultralytics (for YOLOv11), and other dependencies.\n\n**Instructions:**\n- Run this cell to install the dependencies.\n- If you encounter version conflicts, you may need to restart the runtime (`Runtime > Restart runtime`) and rerun this cell.","metadata":{"id":"KHc6q_w3r1sb"}},{"cell_type":"code","source":"# Install dependencies\n!pip install streamlit -q\n!pip install pyngrok -q\n!pip install ultralytics seaborn matplotlib pandas scipy kaggle pillow -q\nprint(\"Dependencies installed successfully!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-mBpGpkr5bC","outputId":"63a93e30-6b2a-48cb-c7bc-ae7b1f49d41a","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:14.987174Z","iopub.execute_input":"2025-03-27T16:06:14.987528Z","iopub.status.idle":"2025-03-27T16:06:25.013154Z","shell.execute_reply.started":"2025-03-27T16:06:14.987503Z","shell.execute_reply":"2025-03-27T16:06:25.01188Z"}},"outputs":[{"name":"stdout","text":"Dependencies installed successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Upgrade pyngrok to the latest version using subprocess\nimport subprocess\nimport os\n\n# Set the environment for the subprocess\nenv = os.environ.copy()\nenv[\"LC_ALL\"] = \"C.UTF-8\"\nenv[\"LANG\"] = \"C.UTF-8\"\nenv[\"LANGUAGE\"] = \"C.UTF-8\"\n\nprint(\"Upgrading pyngrok...\")\ntry:\n    # Use subprocess to run the pip install command\n    process = subprocess.run(\n        [\"pip\", \"install\", \"--upgrade\", \"pyngrok\", \"-q\"],\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True\n    )\n    # Check if the command was successful\n    if process.returncode == 0:\n        print(\"pyngrok upgraded successfully!\")\n    else:\n        print(\"Failed to upgrade pyngrok. Error output:\")\n        print(process.stderr)\n        raise RuntimeError(\"pyngrok upgrade failed.\")\nexcept Exception as e:\n    print(f\"Error upgrading pyngrok: {str(e)}\")\n    raise","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlaJyjtFb2YK","outputId":"82bfe61d-8a81-4e7a-ff04-515ce06f991a","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:25.014571Z","iopub.execute_input":"2025-03-27T16:06:25.014863Z","iopub.status.idle":"2025-03-27T16:06:28.345688Z","shell.execute_reply.started":"2025-03-27T16:06:25.014837Z","shell.execute_reply":"2025-03-27T16:06:28.344798Z"}},"outputs":[{"name":"stdout","text":"Upgrading pyngrok...\npyngrok upgraded successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Install dependencies\n!pip install streamlit pyngrok ultralytics kaggle","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLbT3ayZ4_m7","outputId":"efafadd2-90d0-43a1-9a2a-84bcd0acd4b5","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:28.347735Z","iopub.execute_input":"2025-03-27T16:06:28.348035Z","iopub.status.idle":"2025-03-27T16:06:31.779297Z","shell.execute_reply.started":"2025-03-27T16:06:28.348014Z","shell.execute_reply":"2025-03-27T16:06:31.778468Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.44.0)\nRequirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.3)\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.97)\nRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\nRequirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\nRequirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2025.1.31)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.3.0)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Step 1.1: Verify Installed Versions\n\nThis cell checks the installed versions of key packages to ensure compatibility.\n\n**Instructions:**\n- Run this cell to verify the versions of NumPy and Ultralytics.\n- Expected versions (approximate):\n  - NumPy: 2.1.x\n  - Ultralytics: 8.3.x\n- If there are compatibility issues, you may need to pin specific versions in the previous cell (e.g., `numpy==1.26.4 ultralytics==8.3.92`).","metadata":{"id":"KzF5bm-Lr7cK"}},{"cell_type":"code","source":"# Check installed versions\nimport numpy\nimport ultralytics\nprint(f\"NumPy version: {numpy.__version__}\")\nprint(f\"Ultralytics version: {ultralytics.__version__}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayaY456ar-Za","outputId":"8718a40f-85c5-4c3b-e1c2-3dde9a57eb67","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:31.780698Z","iopub.execute_input":"2025-03-27T16:06:31.780932Z","iopub.status.idle":"2025-03-27T16:06:35.329212Z","shell.execute_reply.started":"2025-03-27T16:06:31.780912Z","shell.execute_reply":"2025-03-27T16:06:35.328345Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nNumPy version: 1.26.4\nUltralytics version: 8.3.97\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Step 1.2: Verify GPU Availability\n\nThis cell checks if a GPU is available, as fine-tuning YOLOv11 is significantly faster on a GPU.\n\n**Instructions:**\n- Run this cell to verify GPU availability.\n- If the output shows `CUDA available: True`, a GPU is available, and you can proceed.\n- If the output shows `CUDA available: False`, switch to a GPU runtime:\n  - Go to `Runtime > Change runtime type`.\n  - Select `GPU` under \"Hardware accelerator\" (e.g., T4 GPU).\n  - Click `Save` and restart the runtime (`Runtime > Restart runtime`).\n  - Rerun all cells above this one.\n- **Note:** If a GPU is not available, the notebook will fall back to CPU, but fine-tuning will be slower (~20–40 minutes for 10 epochs vs. ~10–20 minutes on a GPU).","metadata":{"id":"6pukWCJOsAWi"}},{"cell_type":"code","source":"# Verify GPU availability\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: No GPU detected. Fine-tuning will be slower on CPU. See instructions above to switch to a GPU runtime.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Il5InHDsCE7","outputId":"6629d2a3-6c9f-45a1-f35f-7ec825876424","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:35.330105Z","iopub.execute_input":"2025-03-27T16:06:35.33055Z","iopub.status.idle":"2025-03-27T16:06:35.4245Z","shell.execute_reply.started":"2025-03-27T16:06:35.330517Z","shell.execute_reply":"2025-03-27T16:06:35.423655Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 1\nGPU Name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Step 1.3: Set Up ngrok Authtoken\n\nThis cell sets up ngrok to create a public URL for the Streamlit app.\n\n**Instructions:**\n- The ngrok authtoken is already provided.\n- Run this cell to set up ngrok.\n- If you encounter issues with the authtoken, ensure it is valid by checking your ngrok dashboard at [ngrok.com](https://ngrok.com).","metadata":{"id":"RfR9DrI4sDsC"}},{"cell_type":"code","source":"# Set up ngrok authtoken\nfrom pyngrok import ngrok\n\n# Set the ngrok authtoken\nngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\nprint(\"ngrok authtoken set up successfully!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"418wTmFisFPC","outputId":"1e4ca1fb-f20a-41b5-a1bf-ae5e909a6be0","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:35.425458Z","iopub.execute_input":"2025-03-27T16:06:35.425741Z","iopub.status.idle":"2025-03-27T16:06:38.100103Z","shell.execute_reply.started":"2025-03-27T16:06:35.425709Z","shell.execute_reply":"2025-03-27T16:06:38.099192Z"}},"outputs":[{"name":"stdout","text":"ngrok authtoken set up successfully!                                                                \n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Step 2: Fine-Tune YOLOv11 for Facial Recognition\n\nThis step fine-tunes the YOLOv11 nano model (`yolo11n.pt`) on the facial recognition dataset (`shehabahmed74/shehab-data-facial-recognition`). The fine-tuned model will be saved as `yolo11n_finetuned.pt` and used in the Streamlit app for more specific facial recognition labels (e.g., \"face\" instead of \"person\").\n\nThe dataset contains three folders: `train`, `valid`, and `test`. We will use:\n- `train` for training.\n- `valid` for validation.\n- `test` for testing in the Streamlit app.\n\nThe dataset paths, number of classes (`nc`), and class names (`names`) are pre-configured based on prior testing. We assume:\n- There is one class (`face`), so `nc: 1` and `names: ['face']`.\n\n**This cell will:**\n- Download the dataset (`shehabahmed74/shehab-data-facial-recognition`) using `kagglehub`.\n- Unzip the dataset and verify its structure.\n- Dynamically set the `train` and `val` paths in `data.yaml` based on the actual dataset structure.\n- Fine-tune the YOLOv11 model.\n\n**Instructions:**\n- Run the next cell (Cell 10) to download the dataset and fine-tune the model.\n- This step takes the longest:\n  - With GPU: ~10–20 minutes for 10 epochs.\n  - Without GPU (on CPU): ~20–40 minutes for 10 epochs.\n- If you encounter errors (e.g., missing dataset paths), the dataset structure may differ from the expected layout. The cell will print the dataset structure to help you debug. You can also manually check the structure by adding a new code cell with:\n  ```python\n  !ls -R /root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1","metadata":{"id":"xtWeveSssH2C"}},{"cell_type":"code","source":"","metadata":{"id":"JFk7vSFGn0hl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 2: Fine-tune YOLOv11\n# from ultralytics import YOLO\n# import os\n# import torch\n# import kagglehub\n# import zipfile\n# import glob\n# import shutil\n\n# # Function to print directory structure using Python\n# def print_directory_structure(directory, indent=0, max_files=5):\n#     \"\"\"Recursively print the directory structure with a limit on the number of files displayed per directory.\"\"\"\n#     try:\n#         print(\"  \" * indent + f\"Directory: {directory}\")\n#         items = sorted(os.listdir(directory))\n#         for item in items:\n#             item_path = os.path.join(directory, item)\n#             if os.path.isdir(item_path):\n#                 print(\"  \" * (indent + 1) + f\"Subdirectory: {item}\")\n#                 print_directory_structure(item_path, indent + 2, max_files)\n#             else:\n#                 if max_files > 0:\n#                     print(\"  \" * (indent + 1) + f\"File: {item}\")\n#                     max_files -= 1\n#                 elif max_files == 0:\n#                     print(\"  \" * (indent + 1) + \"... (additional files omitted)\")\n#                     break\n#     except Exception as e:\n#         print(\"  \" * (indent + 1) + f\"Error accessing directory: {str(e)}\")\n\n# # Function to download and process dataset\n# def download_and_process_dataset():\n#     print(\"Downloading and Processing Dataset...\")\n#     try:\n#         # Download dataset\n#         dataset_path = kagglehub.dataset_download(\"shehabahmed74/shehab-data-facial-recognition\")\n#         print(f\"Dataset downloaded to: {dataset_path}\")\n\n#         # Process dataset (unzip if necessary)\n#         if os.path.isdir(dataset_path):\n#             processed_path = dataset_path\n#         elif os.path.isfile(dataset_path) and dataset_path.endswith('.zip'):\n#             unzip_dir = os.path.join(os.path.dirname(dataset_path), \"unzipped_dataset\")\n#             os.makedirs(unzip_dir, exist_ok=True)\n#             with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n#                 zip_ref.extractall(unzip_dir)\n#             processed_path = unzip_dir\n#         else:\n#             raise ValueError(\"Dataset is neither a recognized zip file nor a usable directory.\")\n\n#         print(\"Dataset processing completed!\")\n#         return processed_path\n#     except Exception as e:\n#         print(f\"Error downloading or processing dataset: {str(e)}\")\n#         raise\n\n# # Download and process the dataset\n# dataset_path = download_and_process_dataset()\n\n# # Verify the dataset structure using Python\n# print(\"\\nDataset structure:\")\n# print_directory_structure(dataset_path)\n\n# # Search for image files in the dataset\n# print(\"\\nSearching for image files in the dataset...\")\n# image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n# image_files = []\n# for ext in image_extensions:\n#     image_files.extend(glob.glob(os.path.join(dataset_path, \"**\", ext), recursive=True))\n\n# if not image_files:\n#     raise FileNotFoundError(\"No image files found in the dataset. Check the dataset structure above.\")\n# else:\n#     print(f\"Found {len(image_files)} image files. Examples:\")\n#     for img in image_files[:5]:  # Print first 5 image paths\n#         print(img)\n\n# # Search for label files in the dataset\n# print(\"\\nSearching for label files in the dataset...\")\n# label_files = glob.glob(os.path.join(dataset_path, \"**\", \"*.txt\"), recursive=True)\n# if not label_files:\n#     print(\"Warning: No label files found in the dataset. Check the dataset structure above.\")\n# else:\n#     print(f\"Found {len(label_files)} label files. Examples:\")\n#     for lbl in label_files[:5]:  # Print first 5 label paths\n#         print(lbl)\n\n# # Create a dictionary mapping image filenames (without extension) to their paths\n# image_dict = {os.path.splitext(os.path.basename(img))[0]: img for img in image_files}\n# label_dict = {os.path.splitext(os.path.basename(lbl))[0]: lbl for lbl in label_files}\n\n# # Find images that have corresponding labels\n# paired_files = []\n# for img_name in image_dict.keys():\n#     if img_name in label_dict:\n#         paired_files.append((image_dict[img_name], label_dict[img_name]))\n\n# if not paired_files:\n#     raise ValueError(\"No images have corresponding label files. Check the dataset structure.\")\n\n# print(f\"\\nFound {len(paired_files)} image-label pairs.\")\n\n# # Split the paired files into training and validation sets (80% train, 20% val)\n# paired_files.sort()  # Sort to ensure consistent splitting\n# split_idx = int(0.8 * len(paired_files))\n# train_pairs = paired_files[:split_idx]\n# val_pairs = paired_files[split_idx:]\n\n# # Create directories for training and validation\n# train_images_dir = os.path.join(dataset_path, \"custom_train\", \"images\")\n# train_labels_dir = os.path.join(dataset_path, \"custom_train\", \"labels\")\n# val_images_dir = os.path.join(dataset_path, \"custom_valid\", \"images\")\n# val_labels_dir = os.path.join(dataset_path, \"custom_valid\", \"labels\")\n\n# # Remove existing directories to avoid conflicts\n# print(\"\\nRemoving existing custom_train and custom_valid directories (if any)...\")\n# for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n#     if os.path.exists(dir_path):\n#         shutil.rmtree(dir_path)\n\n# # Recreate the directories\n# os.makedirs(train_images_dir, exist_ok=True)\n# os.makedirs(train_labels_dir, exist_ok=True)\n# os.makedirs(val_images_dir, exist_ok=True)\n# os.makedirs(val_labels_dir, exist_ok=True)\n\n# # Copy images and labels to the respective directories (symbolic links to save space)\n# print(\"\\nOrganizing images and labels into train and valid directories...\")\n# for img_path, lbl_path in train_pairs:\n#     img_name = os.path.basename(img_path)\n#     lbl_name = os.path.basename(lbl_path)\n#     img_symlink = os.path.join(train_images_dir, img_name)\n#     lbl_symlink = os.path.join(train_labels_dir, lbl_name)\n#     # Create symbolic links, overwriting if they exist\n#     if os.path.exists(img_symlink):\n#         os.remove(img_symlink)\n#     if os.path.exists(lbl_symlink):\n#         os.remove(lbl_symlink)\n#     os.symlink(img_path, img_symlink)\n#     os.symlink(lbl_path, lbl_symlink)\n\n# for img_path, lbl_path in val_pairs:\n#     img_name = os.path.basename(img_path)\n#     lbl_name = os.path.basename(lbl_path)\n#     img_symlink = os.path.join(val_images_dir, img_name)\n#     lbl_symlink = os.path.join(val_labels_dir, lbl_name)\n#     # Create symbolic links, overwriting if they exist\n#     if os.path.exists(img_symlink):\n#         os.remove(img_symlink)\n#     if os.path.exists(lbl_symlink):\n#         os.remove(lbl_symlink)\n#     os.symlink(img_path, img_symlink)\n#     os.symlink(lbl_path, lbl_symlink)\n\n# # Define the paths for data.yaml\n# train_images_path = train_images_dir\n# val_images_path = val_images_dir\n\n# # Verify the new directories\n# print(f\"\\nNew training images path: {train_images_path}\")\n# print(f\"Number of training images: {len(glob.glob(os.path.join(train_images_path, '*')))}\")\n# print(f\"New training labels path: {train_labels_dir}\")\n# print(f\"Number of training labels: {len(glob.glob(os.path.join(train_labels_dir, '*')))}\")\n# print(f\"New validation images path: {val_images_path}\")\n# print(f\"Number of validation images: {len(glob.glob(os.path.join(val_images_path, '*')))}\")\n# print(f\"New validation labels path: {val_labels_dir}\")\n# print(f\"Number of validation labels: {len(glob.glob(os.path.join(val_labels_dir, '*')))}\")\n\n# # Check if the paths exist and are not empty\n# if not os.path.exists(train_images_path) or not glob.glob(os.path.join(train_images_path, '*')):\n#     raise FileNotFoundError(f\"Training images path is empty or not found: {train_images_path}\")\n# if not os.path.exists(val_images_path) or not glob.glob(os.path.join(val_images_path, '*')):\n#     raise FileNotFoundError(f\"Validation images path is empty or not found: {val_images_path}\")\n# if not os.path.exists(train_labels_dir) or not glob.glob(os.path.join(train_labels_dir, '*')):\n#     raise FileNotFoundError(f\"Training labels path is empty or not found: {train_labels_dir}\")\n# if not os.path.exists(val_labels_dir) or not glob.glob(os.path.join(val_labels_dir, '*')):\n#     raise FileNotFoundError(f\"Validation labels path is empty or not found: {val_labels_dir}\")\n\n# # Load the pre-trained YOLOv11 nano model\n# model = YOLO(\"yolo11n.pt\")\n\n# # Define the data.yaml with the correct paths\n# data_yaml = f\"\"\"\n# train: {train_images_path}\n# val: {val_images_path}\n# nc: 1  # Number of classes (assumed to be 1 for 'face')\n# names: ['face']  # Class names (assumed to be 'face')\n# \"\"\"\n\n# # Save the data.yaml file\n# with open(\"data.yaml\", \"w\") as f:\n#     f.write(data_yaml)\n\n# # Determine the device (GPU if available, else CPU)\n# device = 0 if torch.cuda.is_available() else 'cpu'\n# print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n\n# # Fine-tune the model\n# try:\n#     model.train(\n#         data=\"data.yaml\",\n#         epochs=10,  # Use fewer epochs for a quick test; increase for better results\n#         imgsz=640,\n#         batch=16,\n#         device=device  # Use GPU if available, else CPU\n#     )\n# except Exception as e:\n#     print(f\"Error during fine-tuning: {str(e)}\")\n#     raise\n\n# # Save the fine-tuned model\n# model.save(\"yolo11n_finetuned.pt\")\n# print(\"Fine-tuning complete! Model saved as 'yolo11n_finetuned.pt'\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loUaDgbm5YU8","outputId":"c81a2db5-9c05-4579-dc9e-58938e6d83a9","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:38.101084Z","iopub.execute_input":"2025-03-27T16:06:38.101418Z","iopub.status.idle":"2025-03-27T16:06:38.107167Z","shell.execute_reply.started":"2025-03-27T16:06:38.101365Z","shell.execute_reply":"2025-03-27T16:06:38.10625Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"### Cell: Step 2\nimport os\nimport shutil\nimport random\n\n# Step 1: Copy dataset to writable directory\nsource_dataset_dir = \"/kaggle/input/shehab-data-facial-recognition\"\nworking_dataset_dir = \"/kaggle/working/shehab-data-facial-recognition\"\nif os.path.exists(working_dataset_dir):\n    shutil.rmtree(working_dataset_dir)\nshutil.copytree(source_dataset_dir, working_dataset_dir)\nprint(\"Step 1: Dataset copied to writable directory\")\n\n# Step 2: Define directory paths\ntrain_images_dir = os.path.join(working_dataset_dir, \"custom_train\", \"images\")\ntrain_labels_dir = os.path.join(working_dataset_dir, \"custom_train\", \"labels\")\nval_images_dir = os.path.join(working_dataset_dir, \"custom_val\", \"images\")\nval_labels_dir = os.path.join(working_dataset_dir, \"custom_val\", \"labels\")\nos.makedirs(train_images_dir, exist_ok=True)\nos.makedirs(train_labels_dir, exist_ok=True)\nos.makedirs(val_images_dir, exist_ok=True)\nos.makedirs(val_labels_dir, exist_ok=True)\nprint(\"Step 2: Directory paths defined\")\n\n# Step 3: Find images and labels\nimage_extensions = (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\")\nall_images = []\nall_labels = []\nfor root, dirs, files in os.walk(working_dataset_dir):\n    if \"custom_train\" in root or \"custom_val\" in root:\n        continue\n    for file in files:\n        if file.lower().endswith(image_extensions):\n            all_images.append(os.path.join(root, file))\n        elif file.endswith(\".txt\"):\n            all_labels.append(os.path.join(root, file))\nprint(f\"Step 3: Found {len(all_images)} images, {len(all_labels)} labels\")\n\n# Step 4: Pair images with labels and deduplicate\nimage_label_pairs = []\nseen_names = set()\nfor image_path in all_images:\n    image_name = os.path.splitext(os.path.basename(image_path))[0]\n    if image_name in seen_names:\n        continue  # Skip duplicates\n    for label_path in all_labels:\n        label_name = os.path.splitext(os.path.basename(label_path))[0]\n        if image_name == label_name and os.path.exists(image_path) and os.path.exists(label_path):\n            image_label_pairs.append((image_path, label_path))\n            seen_names.add(image_name)\n            break\nprint(f\"Step 4: Found {len(image_label_pairs)} image-label pairs after deduplication\")\n\n# Step 5: Split into train and validation sets\nrandom.shuffle(image_label_pairs)\nsplit_idx = int(0.8 * len(image_label_pairs))\ntrain_pairs = image_label_pairs[:split_idx]\nval_pairs = image_label_pairs[split_idx:]\nprint(f\"Step 5: Split into {len(train_pairs)} training pairs, {len(val_pairs)} validation pairs\")\n\n# Step 6: Move files to training directories\ntrain_images_moved = 0\ntrain_labels_moved = 0\ntrain_skipped = 0\nfor image_path, label_path in train_pairs:\n    if os.path.exists(image_path) and os.path.exists(label_path):\n        shutil.move(image_path, os.path.join(train_images_dir, os.path.basename(image_path)))\n        shutil.move(label_path, os.path.join(train_labels_dir, os.path.basename(label_path)))\n        train_images_moved += 1\n        train_labels_moved += 1\n    else:\n        train_skipped += 1\nprint(f\"Step 6: Moved {train_images_moved} images, {train_labels_moved} labels to training directories, skipped {train_skipped} pairs\")\n\n# Step 7: Move files to validation directories\nval_images_moved = 0\nval_labels_moved = 0\nval_skipped = 0\nfor image_path, label_path in val_pairs:\n    if os.path.exists(image_path) and os.path.exists(label_path):\n        shutil.move(image_path, os.path.join(val_images_dir, os.path.basename(image_path)))\n        shutil.move(label_path, os.path.join(val_labels_dir, os.path.basename(label_path)))\n        val_images_moved += 1\n        val_labels_moved += 1\n    else:\n        val_skipped += 1\nprint(f\"Step 7: Moved {val_images_moved} images, {val_labels_moved} labels to validation directories, skipped {val_skipped} pairs\")\n\n# Step 8: Verify pairing\ntrain_images = os.listdir(train_images_dir)\ntrain_labels = os.listdir(train_labels_dir)\nval_images = os.listdir(val_images_dir)\nval_labels = os.listdir(val_labels_dir)\ntrain_pairs_verified = sum(1 for img in train_images if f\"{os.path.splitext(img)[0]}.txt\" in train_labels)\nval_pairs_verified = sum(1 for img in val_images if f\"{os.path.splitext(img)[0]}.txt\" in val_labels)\nprint(f\"Step 8: Verified {train_pairs_verified} pairs in custom_train, {val_pairs_verified} pairs in custom_val\")","metadata":{"id":"2N3CX4eG5ZTU","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:06:38.107936Z","iopub.execute_input":"2025-03-27T16:06:38.108118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 3: Create the Streamlit app\n# import os\n\n# # Define the Streamlit app code\n# streamlit_app_code = \"\"\"\n# import streamlit as st\n# import os\n# import glob\n# import random\n# from PIL import Image\n# from ultralytics import YOLO\n\n# # Load the fine-tuned YOLOv11 model\n# @st.cache_resource\n# def load_model():\n#     return YOLO(\"yolo11n_finetuned.pt\")\n\n# model = load_model()\n\n# # Streamlit app\n# st.title(\"YOLOv11 Facial Recognition Demo\")\n\n# # Step 1: Dataset Path Verification\n# st.header(\"Step 1: Dataset Path Verification\")\n# dataset_path = st.text_input(\"Enter the dataset path from Step 2:\", value=\"/root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1\")\n# if dataset_path:\n#     # Check for 'test' directory\n#     test_path = os.path.join(dataset_path, \"test\")\n#     if not os.path.exists(test_path):\n#         st.error(f\"'test' directory not found: {test_path}. Ensure Step 2 completed successfully and enter the correct path.\")\n#         test_images = []\n#     else:\n#         # Initialize test images list\n#         test_images = []\n#         # Check for nested 'test/test/images' directory (where images are located)\n#         test_images_path = os.path.join(test_path, \"test\", \"images\")\n#         st.write(f\"Checking for images in: {test_images_path}\")\n#         if os.path.exists(test_images_path):\n#             # Look for images with any case variation of .jpg, .png, .jpeg\n#             test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n#                           glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n#                           glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n#             st.write(f\"Found {len(test_images)} images in 'test/test/images'\")\n#             if test_images:\n#                 st.success(f\"Test images directory found: {test_images_path}\")\n#                 st.write(f\"Found {len(test_images)} test images.\")\n#                 st.write(f\"Sample images: {test_images[:5]}\")\n#                 # Verify that the first image is readable\n#                 try:\n#                     with open(test_images[0], \"rb\") as f:\n#                         f.read(1)\n#                     st.write(f\"Successfully read first image: {test_images[0]}\")\n#                 except Exception as e:\n#                     st.error(f\"Error reading image {test_images[0]}: {str(e)}\")\n#                     test_images = []\n#             else:\n#                 st.warning(f\"Test images directory exists but no .jpg, .png, or .jpeg images found in: {test_images_path}\")\n#                 st.write(f\"Files in 'test/test/images': {os.listdir(test_images_path)[:5]}\")\n#         else:\n#             st.warning(f\"'test/test/images' directory not found: {test_images_path}\")\n#             st.write(f\"Contents of 'test': {os.listdir(test_path)[:5]}\")\n#             # Fallback: Check for images directly in 'test/test'\n#             test_images_path = os.path.join(test_path, \"test\")\n#             st.write(f\"Checking for images directly in: {test_images_path}\")\n#             if os.path.exists(test_images_path):\n#                 test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n#                               glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n#                               glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n#                 st.write(f\"Found {len(test_images)} images directly in 'test/test'\")\n#                 if test_images:\n#                     st.success(f\"Test images found in nested 'test/test' directory: {test_images_path}\")\n#                     st.write(f\"Found {len(test_images)} test images.\")\n#                     st.write(f\"Sample images: {test_images[:5]}\")\n#                 else:\n#                     st.warning(f\"No .jpg, .png, or .jpeg images found in {test_images_path}\")\n#                     st.write(f\"Files in 'test/test': {os.listdir(test_images_path)[:5]}\")\n#             else:\n#                 st.warning(f\"'test/test' directory not found: {test_images_path}\")\n\n#         # Final check: If no images are found, notify the user\n#         if not test_images:\n#             st.info(\"No test images found. You can still upload an external image for inference below.\")\n\n# # Step 2: Test YOLOv11 Facial Recognition\n# st.header(\"Step 2: Test YOLOv11 Facial Recognition\")\n\n# # Option to select image source\n# image_source_options = [\"Use a random image from the test dataset\", \"Upload an external image\"]\n# image_source = st.radio(\n#     \"Select image source for facial recognition:\",\n#     image_source_options,\n#     index=0 if test_images else 1  # Default to random if test images exist, otherwise upload\n# )\n\n# # Disable the random image option if no test images are found\n# if not test_images and image_source == \"Use a random image from the test dataset\":\n#     st.warning(\"No test images found. Please select 'Upload an external image' instead.\")\n#     image_source = \"Upload an external image\"\n\n# # Initialize image_path\n# image_path = None\n\n# # Handle random image selection\n# if image_source == \"Use a random image from the test dataset\" and test_images:\n#     image_path = random.choice(test_images)\n#     st.write(f\"Selected random image: {os.path.basename(image_path)}\")\n\n# # Always show the file uploader for external images\n# if image_source == \"Upload an external image\":\n#     uploaded_file = st.file_uploader(\"Upload an external image for facial recognition\", type=[\"jpg\", \"jpeg\", \"png\"])\n#     if uploaded_file is not None:\n#         # Save the uploaded file temporarily\n#         image_path = os.path.join(\"temp\", uploaded_file.name)\n#         st.write(f\"Saving uploaded file to: {image_path}\")\n#         try:\n#             os.makedirs(\"temp\", exist_ok=True)\n#             with open(image_path, \"wb\") as f:\n#                 f.write(uploaded_file.getbuffer())\n#             st.write(f\"Uploaded image: {uploaded_file.name}\")\n#             # Verify the file was saved\n#             if os.path.exists(image_path):\n#                 st.write(f\"File successfully saved at: {image_path}\")\n#             else:\n#                 st.error(f\"Failed to save the uploaded file at: {image_path}\")\n#                 image_path = None\n#         except Exception as e:\n#             st.error(f\"Error saving uploaded file: {str(e)}\")\n#             image_path = None\n#     else:\n#         st.info(\"Please upload an image to proceed.\")\n\n# # Run inference if an image is selected\n# if st.button(\"Run Inference\"):\n#     if image_path:\n#         # Load and display the image\n#         try:\n#             image = Image.open(image_path)\n#             st.image(image, caption=\"Input Image\", use_column_width=True)\n#         except Exception as e:\n#             st.error(f\"Error loading image {image_path}: {str(e)}\")\n#             image_path = None\n\n#         if image_path:\n#             # Run YOLOv11 inference\n#             with st.spinner(\"Running facial recognition...\"):\n#                 try:\n#                     results = model(image)\n#                     # Display the results\n#                     st.header(\"Step 3: Results\")\n#                     # Plot the results\n#                     annotated_image = results[0].plot()  # Get the annotated image with bounding boxes\n#                     st.image(annotated_image, caption=\"Detected Faces\", use_column_width=True)\n\n#                     # Display detection details\n#                     detections = results[0].boxes\n#                     st.write(f\"Number of faces detected: {len(detections)}\")\n#                     for i, det in enumerate(detections):\n#                         conf = det.conf.item()\n#                         bbox = det.xyxy[0].tolist()\n#                         st.write(f\"Face {i+1}: Confidence = {conf:.2f}, Bounding Box = {bbox}\")\n#                 except Exception as e:\n#                     st.error(f\"Error during inference: {str(e)}\")\n#     else:\n#         st.error(\"Please select a random test image or upload an external image to run inference.\")\n\n# # Step 4: Summary and Conclusion\n# st.header(\"Step 4: Summary and Conclusion\")\n# st.write(\"This demo showcases the YOLOv11 model fine-tuned for facial recognition.\")\n# st.write(\"The model was fine-tuned on the dataset provided in Step 2 and tested on either a random test image or an uploaded external image.\")\n# st.write(\"For detailed performance metrics and comparisons, refer to the notebook (Step 3.1).\")\n# \"\"\"\n\n# # Write the Streamlit app code to app.py\n# with open(\"app.py\", \"w\") as f:\n#     f.write(streamlit_app_code)\n\n# # Verify that app.py was created\n# if os.path.exists(\"app.py\"):\n#     print(\"Streamlit app created successfully at app.py!\")\n# else:\n#     raise FileNotFoundError(\"Failed to create app.py\")","metadata":{"id":"KhDvcsFy25CU","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"_gbzxrty25-7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Set Up the Streamlit App\n\nThis step creates the Streamlit app (`app.py`) that compares YOLOv8 and YOLOv11 for facial recognition. The app includes:\n- **Dataset Path Verification:** Uses the dataset already downloaded in Step 2.\n- **YOLOv11 Inference:** Performs facial recognition using the fine-tuned model on the `test` dataset or an uploaded image.\n- **Performance Comparison:** Generates plots and a table comparing YOLOv8 and YOLOv11.\n- **Summary:** Provides a conclusion and recommendation.\n\nThe app uses the `test` folder for inference to evaluate the fine-tuned model on unseen data.\n\n**Instructions:**\n- Run this cell to create `app.py`.\n- Then, proceed to Step 3.1 to deploy the app.\n- Note: The dataset was already downloaded in Step 2. The app will use the same dataset path.","metadata":{"id":"4FnX3ZJTsMp6"}},{"cell_type":"markdown","source":"# Step 10: Fine-tune YOLOv11 model","metadata":{}},{"cell_type":"code","source":"# Step 0: Install required libraries\n!pip install ultralytics\nprint(\"Ultralytics installed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 0: Install required libraries\n!pip install ultralytics\nprint(\"Ultralytics installed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To fix warning message which appeared when run cell 10","metadata":{}},{"cell_type":"code","source":"# Step 10.5: Fix label files to ensure class ID is 0\nimport os\n\n# Define directories\ntrain_labels_dir = \"/kaggle/working/shehab-data-facial-recognition/custom_train/labels\"\nval_labels_dir = \"/kaggle/working/shehab-data-facial-recognition/custom_val/labels\"\n\n# Function to fix a single label file\ndef fix_label_file(label_path):\n    try:\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n        # Fix class ID to 0\n        fixed_lines = []\n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) >= 5:  # Ensure the line has at least 5 parts (class_id, x, y, w, h)\n                parts[0] = \"0\"  # Set class ID to 0\n                fixed_lines.append(\" \".join(parts) + \"\\n\")\n        # Write the fixed lines back to the file\n        with open(label_path, \"w\") as f:\n            f.writelines(fixed_lines)\n    except Exception as e:\n        print(f\"Error fixing {label_path}: {str(e)}\")\n\n# Fix training labels\ntrain_fixed = 0\nfor label_file in os.listdir(train_labels_dir):\n    if label_file.endswith(\".txt\"):\n        fix_label_file(os.path.join(train_labels_dir, label_file))\n        train_fixed += 1\n\n# Fix validation labels\nval_fixed = 0\nfor label_file in os.listdir(val_labels_dir):\n    if label_file.endswith(\".txt\"):\n        fix_label_file(os.path.join(val_labels_dir, label_file))\n        val_fixed += 1\n\nprint(f\"Fixed {train_fixed} training labels and {val_fixed} validation labels\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Fine-tune YOLOv11 model\nfrom ultralytics import YOLO\nimport os\n\n# Create data.yaml if it doesn't exist\ndata_yaml_path = \"/kaggle/working/shehab-data-facial-recognition/data.yaml\"\nif not os.path.exists(data_yaml_path):\n    data_yaml = \"\"\"\n    train: /kaggle/working/shehab-data-facial-recognition/custom_train/images\n    val: /kaggle/working/shehab-data-facial-recognition/custom_val/images\n    nc: 1\n    names: ['face']\n    \"\"\"\n    with open(data_yaml_path, \"w\") as f:\n        f.write(data_yaml)\n    print(\"Created data.yaml\")\n\n# Load the pre-trained YOLOv11 model\nmodel = YOLO(\"yolo11n.pt\")  # This will download the model if not present\n\n# Fine-tune the model on your dataset\nmodel.train(\n    data=data_yaml_path,\n    epochs=10,\n    imgsz=640,\n    batch=8,\n    device=0,\n    project=\"/kaggle/working/runs/train\",\n    name=\"yolo11n_finetune\"\n)\n\n# Save the fine-tuned model\nmodel.save(\"/kaggle/working/yolo11n_finetuned.pt\")\nprint(\"Fine-tuning complete. Model saved as 'yolo11n_finetuned.pt'\")","metadata":{"id":"IUQ4Qg_M09BF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Step 3: Create the Streamlit app\nimport os\n\n# Check if the fine-tuned model file exists\nmodel_file = \"yolo11n_finetuned.pt\"\nif not os.path.exists(model_file):\n    raise FileNotFoundError(f\"The fine-tuned model file '{model_file}' does not exist. Please run the fine-tuning step (Cell 10) to create it.\")\n\n# Define the Streamlit app code\nstreamlit_app_code = \"\"\"\nimport streamlit as st\nimport os\nimport glob\nimport random\nfrom PIL import Image\nfrom ultralytics import YOLO\n\n# Load the fine-tuned YOLOv11 model\n@st.cache_resource\ndef load_model():\n    return YOLO(\"yolo11n_finetuned.pt\")\n\nmodel = load_model()\n\n# Initialize session state for image_path\nif 'image_path' not in st.session_state:\n    st.session_state.image_path = None\n\n# Streamlit app\nst.title(\"YOLOv11 Facial Recognition Demo\")\n\n# Step 1: Dataset Path Verification\nst.header(\"Step 1: Dataset Path Verification\")\ndataset_path = st.text_input(\"Enter the dataset path from Step 2:\", value=\"/root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1\")\nif dataset_path:\n    # Check for 'test' directory\n    test_path = os.path.join(dataset_path, \"test\")\n    if not os.path.exists(test_path):\n        st.error(f\"'test' directory not found: {test_path}. Ensure Step 2 completed successfully and enter the correct path.\")\n        test_images = []\n    else:\n        # Initialize test images list\n        test_images = []\n        # Check for nested 'test/test/images' directory (where images are located)\n        test_images_path = os.path.join(test_path, \"test\", \"images\")\n        st.write(f\"Checking for images in: {test_images_path}\")\n        if os.path.exists(test_images_path):\n            # Look for images with any case variation of .jpg, .png, .jpeg\n            test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n                          glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n                          glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n            st.write(f\"Found {len(test_images)} images in 'test/test/images'\")\n            if test_images:\n                st.success(f\"Test images directory found: {test_images_path}\")\n                st.write(f\"Found {len(test_images)} test images.\")\n                st.write(f\"Sample images: {test_images[:5]}\")\n                # Verify that the first image is readable\n                try:\n                    with open(test_images[0], \"rb\") as f:\n                        f.read(1)\n                    st.write(f\"Successfully read first image: {test_images[0]}\")\n                except Exception as e:\n                    st.error(f\"Error reading image {test_images[0]}: {str(e)}\")\n                    test_images = []\n            else:\n                st.warning(f\"Test images directory exists but no .jpg, .png, or .jpeg images found in: {test_images_path}\")\n                st.write(f\"Files in 'test/test/images': {os.listdir(test_images_path)[:5]}\")\n        else:\n            st.warning(f\"'test/test/images' directory not found: {test_images_path}\")\n            st.write(f\"Contents of 'test': {os.listdir(test_path)[:5]}\")\n            # Fallback: Check for images directly in 'test/test'\n            test_images_path = os.path.join(test_path, \"test\")\n            st.write(f\"Checking for images directly in: {test_images_path}\")\n            if os.path.exists(test_images_path):\n                test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n                              glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n                              glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n                st.write(f\"Found {len(test_images)} images directly in 'test/test'\")\n                if test_images:\n                    st.success(f\"Test images found in nested 'test/test' directory: {test_images_path}\")\n                    st.write(f\"Found {len(test_images)} test images.\")\n                    st.write(f\"Sample images: {test_images[:5]}\")\n                else:\n                    st.warning(f\"No .jpg, .png, or .jpeg images found in {test_images_path}\")\n                    st.write(f\"Files in 'test/test': {os.listdir(test_images_path)[:5]}\")\n            else:\n                st.warning(f\"'test/test' directory not found: {test_images_path}\")\n\n        # Final check: If no images are found, notify the user\n        if not test_images:\n            st.info(\"No test images found. You can still upload an external image for inference below.\")\n\n# Step 2: Test YOLOv11 Facial Recognition\nst.header(\"Step 2: Test YOLOv11 Facial Recognition\")\n\n# Option to select image source\nimage_source_options = [\"Use a random image from the test dataset\", \"Upload an external image\"]\nimage_source = st.radio(\n    \"Select image source for facial recognition:\",\n    image_source_options,\n    index=0 if test_images else 1  # Default to random if test images exist, otherwise upload\n)\n\n# Disable the random image option if no test images are found\nif not test_images and image_source == \"Use a random image from the test dataset\":\n    st.warning(\"No test images found. Please select 'Upload an external image' instead.\")\n    image_source = \"Upload an external image\"\n\n# Initialize local image_path for this run\nimage_path = None\n\n# Handle random image selection\nif image_source == \"Use a random image from the test dataset\" and test_images:\n    image_path = random.choice(test_images)\n    st.write(f\"Selected random image: {os.path.basename(image_path)}\")\n    st.session_state.image_path = image_path\n\n# Handle external image upload\nif image_source == \"Upload an external image\":\n    uploaded_file = st.file_uploader(\"Upload an external image for facial recognition\", type=[\"jpg\", \"jpeg\", \"png\"])\n    if uploaded_file is not None:\n        # Save the uploaded file temporarily\n        image_path = os.path.join(\"temp\", uploaded_file.name)\n        st.write(f\"Saving uploaded file to: {image_path}\")\n        try:\n            os.makedirs(\"temp\", exist_ok=True)\n            with open(image_path, \"wb\") as f:\n                f.write(uploaded_file.getbuffer())\n            st.write(f\"Uploaded image: {uploaded_file.name}\")\n            # Verify the file was saved\n            if os.path.exists(image_path):\n                st.write(f\"File successfully saved at: {image_path}\")\n                st.session_state.image_path = image_path  # Store in session state\n            else:\n                st.error(f\"Failed to save the uploaded file at: {image_path}\")\n                st.session_state.image_path = None\n        except Exception as e:\n            st.error(f\"Error saving uploaded file: {str(e)}\")\n            st.session_state.image_path = None\n    else:\n        st.info(\"Please upload an image to proceed.\")\n\n    # Display current uploaded image status\n    if st.session_state.image_path and image_source == \"Upload an external image\":\n        st.success(f\"Image ready for inference: {os.path.basename(st.session_state.image_path)}\")\n        if st.button(\"Clear Uploaded Image\"):\n            st.session_state.image_path = None\n            st.experimental_rerun()\n\n# Run inference if an image is selected\nif st.button(\"Run Inference\"):\n    # Use the session state image_path if available, otherwise use the local image_path\n    current_image_path = st.session_state.image_path if st.session_state.image_path else image_path\n    if current_image_path:\n        # Load and display the image\n        try:\n            image = Image.open(current_image_path)\n            st.image(image, caption=\"Input Image\", use_column_width=True)\n        except Exception as e:\n            st.error(f\"Error loading image {current_image_path}: {str(e)}\")\n            st.session_state.image_path = None\n            current_image_path = None\n\n        if current_image_path:\n            # Run YOLOv11 inference\n            with st.spinner(\"Running facial recognition...\"):\n                try:\n                    results = model(image)\n                    # Display the results\n                    st.header(\"Step 3: Results\")\n                    # Plot the results\n                    annotated_image = results[0].plot()  # Get the annotated image with bounding boxes\n                    st.image(annotated_image, caption=\"Detected Faces\", use_column_width=True)\n\n                    # Display detection details\n                    detections = results[0].boxes\n                    st.write(f\"Number of faces detected: {len(detections)}\")\n                    for i, det in enumerate(detections):\n                        conf = det.conf.item()\n                        bbox = det.xyxy[0].tolist()\n                        st.write(f\"Face {i+1}: Confidence = {conf:.2f}, Bounding Box = {bbox}\")\n                except Exception as e:\n                    st.error(f\"Error during inference: {str(e)}\")\n    else:\n        st.error(\"Please select a random test image or upload an external image to run inference.\")\n\n# Step 4: Summary and Conclusion\nst.header(\"Step 4: Summary and Conclusion\")\nst.write(\"This demo showcases the YOLOv11 model fine-tuned for facial recognition.\")\nst.write(\"The model was fine-tuned on the dataset provided in Step 2 and tested on either a random test image or an uploaded external image.\")\nst.write(\"For detailed performance metrics and comparisons, refer to the notebook (Step 3.1).\")\n\"\"\"\n\n# Write the Streamlit app code to app.py\nwith open(\"app.py\", \"w\") as f:\n    f.write(streamlit_app_code)\n\n# Verify that app.py was created\nif os.path.exists(\"app.py\"):\n    print(\"Streamlit app created successfully at app.py!\")\nelse:\n    raise FileNotFoundError(\"Failed to create app.py\")","metadata":{"id":"clzCoV2PGeWC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3.1: Deploy the Streamlit App and Download the Dataset\n\n<!-- This cell deploys the Streamlit app using ngrok, creating a public URL for access. It also downloads the dataset, which is required for fine-tuning and inference.\n\n**Instructions:**\n- Run this cell to start the Streamlit app.\n- Open the provided URL in your browser.\n- Go to \"Step 1: Dataset Preparation\" in the app and click \"Download and Process Dataset.\"\n- The public URL will be stored for the summary section.\n- Proceed to Step 4 to test the app. -->\n\n\n“# Deprecated","metadata":{"id":"M0Ci1s3TsV-k"}},{"cell_type":"code","source":"# # Step 3.1: Performance Comparison (YOLOv8 vs. YOLOv11)\n# import numpy as np\n# import pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# import time\n# import json\n# import os\n\n# print(\"Step 3.1: Comparing YOLOv8 and YOLOv11...\")\n# start_time = time.time()\n\n# # Simulate performance metrics over 50 epochs\n# epochs = np.arange(1, 51)\n# mAP8 = 0.75 + 0.12 * np.sin(epochs * 0.08)  # Simulated mAP50 for YOLOv8\n# mAP11 = 0.82 + 0.10 * np.cos(epochs * 0.06)  # Simulated mAP50 for YOLOv11\n# latency8 = 25 - 0.06 * epochs  # Simulated latency for YOLOv8\n# latency11 = 24 - 0.07 * epochs  # Simulated latency for YOLOv11\n# fps8 = 45 + 0.15 * epochs  # Simulated FPS for YOLOv8\n# fps11 = 47 + 0.18 * epochs  # Simulated FPS for YOLOv11\n\n# # Plot mAP50 comparison\n# plt.figure(figsize=(12, 6))\n# sns.lineplot(x=epochs, y=mAP8, label=\"YOLOv8 mAP50\", color=\"blue\")\n# sns.lineplot(x=epochs, y=mAP11, label=\"YOLOv11 mAP50\", color=\"orange\")\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"mAP50\")\n# plt.title(\"mAP50 Comparison: YOLOv8 vs. YOLOv11\")\n# plt.legend()\n# plt.grid(True)\n# plt.savefig(\"map50_comparison.png\")  # Save the plot\n# plt.show()\n# print(\"Generated mAP50 comparison plot.\")\n\n# # Plot latency comparison\n# plt.figure(figsize=(12, 6))\n# sns.lineplot(x=epochs, y=latency8, label=\"YOLOv8 Latency (ms)\", color=\"blue\")\n# sns.lineplot(x=epochs, y=latency11, label=\"YOLOv11 Latency (ms)\", color=\"orange\")\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Latency (ms)\")\n# plt.title(\"Latency Comparison: YOLOv8 vs. YOLOv11\")\n# plt.legend()\n# plt.grid(True)\n# plt.savefig(\"latency_comparison.png\")  # Save the plot\n# plt.show()\n# print(\"Generated latency comparison plot.\")\n\n# # Plot FPS comparison\n# plt.figure(figsize=(12, 6))\n# sns.lineplot(x=epochs, y=fps8, label=\"YOLOv8 FPS\", color=\"blue\")\n# sns.lineplot(x=epochs, y=fps11, label=\"YOLOv11 FPS\", color=\"orange\")\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"FPS\")\n# plt.title(\"FPS Comparison: YOLOv8 vs. YOLOv11\")\n# plt.legend()\n# plt.grid(True)\n# plt.savefig(\"fps_comparison.png\")  # Save the plot\n# plt.show()\n# print(\"Generated FPS comparison plot.\")\n\n# # Create comparison table\n# comparison_data = {\n#     \"Metric\": [\"Peak mAP50\", \"Min Latency (ms)\", \"Peak FPS\", \"Parameters (M)\", \"Inference Speedup (%)\"],\n#     \"YOLOv8\": [np.max(mAP8), np.min(latency8), np.max(fps8), 11.2, 0],\n#     \"YOLOv11\": [np.max(mAP11), np.min(latency11), np.max(fps11), 8.7, 2]\n# }\n# comparison_df = pd.DataFrame(comparison_data)\n# print(\"\\nDetailed Comparison Table:\")\n# print(comparison_df.to_string(index=False))\n\n# # Load and display the test results from the Streamlit app\n# print(\"\\nFacial Recognition Test Results (from Streamlit App):\")\n# if os.path.exists(\"inference_results.json\"):\n#     with open(\"inference_results.json\", \"r\") as f:\n#         inference_results = json.load(f)\n#     print(f\"Image Path: {inference_results['image_path']}\")\n#     print(f\"Number of Faces Detected: {inference_results['num_faces_detected']}\")\n#     for detection in inference_results['detections']:\n#         print(f\"Face {detection['face']}: Confidence = {detection['confidence']:.2f}, Bounding Box = {detection['bounding_box']}\")\n# else:\n#     print(\"No test results found. Please run a test in the Streamlit app (Step 3.2) first.\")\n\n# print(f\"Step 3.1 completed in {time.time() - start_time:.2f} seconds\")","metadata":{"id":"NMAyiwxvbX_V","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3.2: Deploy the Streamlit App\n\n<!-- This cell deploys the Streamlit app using ngrok, creating a public URL for access.\n\n**Instructions:**\n- Run this cell to start the Streamlit app.\n- Open the provided URL in your browser.\n- Go to \"Step 1: Dataset Path Verification\" in the app to confirm the dataset path.\n- Test the app by using a test image or uploading an image in \"Step 2: Test YOLOv11 Facial Recognition.\"\n- The public URL will be stored for the summary section.\n- Proceed to Step 4 to test the app further.\nCell 14 # -->\n\n“# Deprecated","metadata":{"id":"6fHiFg1Q1pvh"}},{"cell_type":"code","source":"# # Install pyngrok\n# !pip install pyngrok\n# print(\"Pyngrok installed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Verify pyngrok installation\n# import pyngrok\n# print(\"Pyngrok version:\", pyngrok.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"o set the ngrok authtoken before running Step 3.2","metadata":{}},{"cell_type":"code","source":"# # Set ngrok authtoken\n# from pyngrok import ngrok\n# !ngrok authtoken YOUR_NGROK_AUTHTOKEN\n# print(\"Ngrok authtoken set successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"i ll divide 3.2 to 3 part to rapid excution and trouble shouting","metadata":{}},{"cell_type":"markdown","source":"# Step 3.2-1: Install Dependencies and Set Up ngrok","metadata":{}},{"cell_type":"code","source":"# # Step 3.2-1: Set Up ngrok Authtoken\n# print(\"Starting Step 3.2-1: Set Up ngrok Authtoken\")\n\n# from pyngrok import ngrok\n\n# # Set your ngrok authtoken\n# ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n# print(\"ngrok authtoken set successfully\")\n\n# print(\"Step 3.2-1 completed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 3.2-1: Install Dependencies and Set Up ngrok\n# print(\"Starting Step 3.2-1: Install Dependencies and Set Up ngrok\")\n\n# import subprocess\n# import os\n\n# # Set environment variables to ensure UTF-8 encoding\n# os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n# os.environ[\"LANG\"] = \"C.UTF-8\"\n# os.environ[\"LANGUAGE\"] = \"C.UTF-8\"\n\n# # Install core dependencies (numpy, protobuf, tensorflow, ultralytics)\n# print(\"Installing core dependencies...\")\n# result = subprocess.run(\n#     [\"pip\", \"install\", \"numpy==1.26.0\", \"protobuf==3.20.3\", \"tensorflow==2.17.0\", \"ultralytics\"],\n#     capture_output=True,\n#     text=True,\n#     encoding=\"utf-8\",\n#     timeout=300  # 5 minutes\n# )\n# print(\"Core dependency stdout:\", result.stdout)\n# print(\"Core dependency stderr:\", result.stderr)\n# if result.returncode != 0:\n#     raise RuntimeError(\"Failed to install core dependencies. See stderr above for details.\")\n# print(\"Core dependencies installed successfully\")\n\n# # Install streamlit and pyngrok with minimal dependencies\n# print(\"Installing streamlit and pyngrok...\")\n# result = subprocess.run(\n#     [\"pip\", \"install\", \"streamlit\", \"pyngrok\", \"--no-deps\", \"--force-reinstall\"],\n#     capture_output=True,\n#     text=True,\n#     encoding=\"utf-8\",\n#     timeout=120  # 2 minutes\n# )\n# print(\"pip install stdout:\", result.stdout)\n# print(\"pip install stderr:\", result.stderr)\n# if result.returncode != 0:\n#     raise RuntimeError(\"Failed to install streamlit and pyngrok. See stderr above for details.\")\n# print(\"Streamlit and pyngrok installed successfully\")\n\n# # Set ngrok authtoken with the new v2 authtoken\n# print(\"Setting ngrok authtoken...\")\n# ngrok_authtoken = \"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\"  # Updated v2 authtoken\n# result = subprocess.run(\n#     [\"ngrok\", \"authtoken\", ngrok_authtoken],\n#     capture_output=True,\n#     text=True,\n#     encoding=\"utf-8\"\n# )\n# print(\"ngrok stdout:\", result.stdout)\n# print(\"ngrok stderr:\", result.stderr)\n# if result.returncode != 0:\n#     raise RuntimeError(\"Failed to set ngrok authtoken. See stderr above for details.\")\n# print(\"Ngrok authtoken set successfully\")\n\n# print(\"Step 3.2-1 completed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- #Step 3.2-2: Start the Streamlit App \n\nThis part checks for app.py, terminates existing Streamlit processes, and starts the Streamlit app in the background using the correct Python interpreter. --> “# Deprecated\n“# Deprecated","metadata":{}},{"cell_type":"code","source":"# # Step 3.2-2: Start the Streamlit App\n# print(\"Starting Step 3.2-2: Start the Streamlit App\")\n\n# import subprocess\n# import os\n# import sys\n# import time\n# import requests\n\n# # Install required packages if not already installed\n# print(\"Installing required packages...\")\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n\n# # Download YOLO model files\n# print(\"Downloading YOLO model files...\")\n# from ultralytics import YOLO\n# try:\n#     YOLO(\"yolo11n.pt\")\n#     YOLO(\"yolov8n.pt\")\n# except Exception as e:\n#     print(f\"Error downloading YOLO models: {str(e)}\")\n#     print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n#     raise\n\n# # Specify your Streamlit app file\n# app_file = \"/kaggle/working/app.py\"\n\n# # Verify that app.py exists (should have been created in Step 4 or earlier steps)\n# print(\"Checking for app.py...\")\n# if not os.path.exists(app_file):\n#     raise FileNotFoundError(\"app.py not found in /kaggle/working/. Please ensure Step 4 or earlier steps created app.py successfully.\")\n\n# # Start the Streamlit app\n# print(\"Starting Streamlit app...\")\n# env = os.environ.copy()\n# env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n# env[\"LC_ALL\"] = \"C.UTF-8\"\n# env[\"LANG\"] = \"C.UTF-8\"\n# env[\"LANGUAGE\"] = \"C.UTF-8\"\n\n# # Forcefully terminate any existing Streamlit processes\n# print(\"Terminating any existing Streamlit processes...\")\n# subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n# time.sleep(2)\n\n# # Start the new Streamlit process\n# process = subprocess.Popen(\n#     [\"streamlit\", \"run\", app_file],\n#     env=env,\n#     stdout=subprocess.PIPE,\n#     stderr=subprocess.PIPE,\n#     text=True,\n#     encoding=\"utf-8\"\n# )\n\n# # Give Streamlit time to start and verify it's running\n# print(\"Waiting for Streamlit to start (up to 30 seconds)...\")\n# start_time = time.time()\n# while time.time() - start_time < 30:\n#     if process.poll() is not None:\n#         stdout, stderr = process.communicate()\n#         print(\"Streamlit failed to start. Error output:\")\n#         print(\"stdout:\", stdout)\n#         print(\"stderr:\", stderr)\n#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n#     try:\n#         response = requests.get(\"http://localhost:8501\", timeout=5)\n#         if response.status_code == 200:\n#             print(\"Streamlit app is accessible on port 8501.\")\n#             break\n#     except requests.ConnectionError:\n#         pass\n#     time.sleep(5)\n# else:\n#     stdout, stderr = process.communicate()\n#     print(\"Streamlit failed to start within 30 seconds. Error output:\")\n#     print(\"stdout:\", stdout)\n#     print(\"stderr:\", stderr)\n#     raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n\n# # Log Streamlit output in the background\n# print(\"Logging Streamlit output until the app crashes or you stop the cell...\")\n# print(\"Proceed to Step 3.2-3 in a new cell while this cell continues logging.\")\n# while True:\n#     if process.poll() is not None:\n#         stdout, stderr = process.communicate()\n#         print(\"Streamlit process exited unexpectedly. Error output:\")\n#         print(\"stdout:\", stdout)\n#         print(\"stderr:\", stderr)\n#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n#     stdout_line = process.stdout.readline()\n#     stderr_line = process.stderr.readline()\n#     if stdout_line:\n#         print(\"Streamlit stdout:\", stdout_line.strip())\n#     if stderr_line:\n#         print(\"Streamlit stderr:\", stderr_line.strip())\n#     time.sleep(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 3.2-2: Start the Streamlit App\n# print(\"Starting Step 3.2-2: Start the Streamlit App\")\n\n# import subprocess\n# import os\n# import time\n# import requests\n\n# # Check for app.py\n# print(\"Checking for app.py...\")\n# if not os.path.exists(\"/kaggle/working/app.py\"):\n#     raise FileNotFoundError(\"app.py not found in /kaggle/working/. Please ensure Step 3 (Cell 26) created app.py successfully.\")\n\n# # Terminate any existing Streamlit processes\n# subprocess.run([\"pkill\", \"-f\", \"streamlit\"])\n# print(\"Terminated any existing Streamlit processes\")\n\n# # Set the environment for the subprocess\n# env = os.environ.copy()\n# env[\"LC_ALL\"] = \"C.UTF-8\"\n# env[\"LANG\"] = \"C.UTF-8\"\n# env[\"LANGUAGE\"] = \"C.UTF-8\"\n# env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n# env[\"PATH\"] = f\"{env.get('PATH', '')}:/root/.local/bin\"\n\n# # Find the correct Python interpreter\n# print(\"Finding the correct Python interpreter...\")\n# result = subprocess.run([\"which\", \"python3\"], capture_output=True, text=True)\n# python_executable = result.stdout.strip()\n# print(f\"Using Python interpreter: {python_executable}\")\n\n# # Verify that the Python interpreter can find streamlit\n# result = subprocess.run([python_executable, \"-c\", \"import streamlit; print(streamlit.__version__)\"],\n#                         capture_output=True, text=True, env=env)\n# if result.returncode != 0:\n#     raise RuntimeError(f\"Python interpreter {python_executable} cannot find streamlit. Error: {result.stderr}\")\n# print(f\"Streamlit version: {result.stdout.strip()}\")\n\n# # Run Streamlit app in the background\n# print(\"Starting Streamlit app...\")\n# try:\n#     process = subprocess.Popen(\n#         [python_executable, \"-m\", \"streamlit\", \"run\", \"app.py\"],\n#         env=env,\n#         stdout=subprocess.PIPE,\n#         stderr=subprocess.PIPE,\n#         text=True,\n#         encoding=\"utf-8\"\n#     )\n#     # Give Streamlit time to start (10 seconds)\n#     time.sleep(10)\n\n#     # Check if the process is still running\n#     if process.poll() is None:\n#         print(\"Streamlit app is running in the background.\")\n#     else:\n#         stdout, stderr = process.communicate()\n#         print(\"Streamlit failed to start. Error output:\")\n#         print(\"stdout:\", stdout)\n#         print(\"stderr:\", stderr)\n#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n\n#     # Verify Streamlit is accessible on port 8501 (multiple checks)\n#     print(\"Verifying Streamlit is accessible on port 8501...\")\n#     for attempt in range(3):  # Check 3 times, 5 seconds apart\n#         try:\n#             response = requests.get(\"http://localhost:8501\", timeout=5)\n#             if response.status_code == 200:\n#                 print(f\"Attempt {attempt + 1}: Streamlit app is accessible on port 8501.\")\n#                 break\n#             else:\n#                 print(f\"Attempt {attempt + 1}: Streamlit app returned status code {response.status_code}.\")\n#                 raise RuntimeError(\"Streamlit app is running but not accessible.\")\n#         except requests.ConnectionError:\n#             print(f\"Attempt {attempt + 1}: Failed to connect to Streamlit on port 8501.\")\n#             if attempt == 2:  # Last attempt\n#                 stdout, stderr = process.communicate()\n#                 print(\"Streamlit is not accessible after multiple attempts. Error output:\")\n#                 print(\"stdout:\", stdout)\n#                 print(\"stderr:\", stderr)\n#                 raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n#         time.sleep(5)  # Wait 5 seconds before the next attempt\n\n#     # Log Streamlit output indefinitely until the app crashes or you stop the cell\n#     print(\"Logging Streamlit output until the app crashes or you stop the cell...\")\n#     print(\"Proceed to Step 3.2-3 in a new cell while this cell continues logging.\")\n#     while True:\n#         if process.poll() is not None:\n#             stdout, stderr = process.communicate()\n#             print(\"Streamlit process exited unexpectedly. Error output:\")\n#             print(\"stdout:\", stdout)\n#             print(\"stderr:\", stderr)\n#             raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n#         # Check for new output\n#         stdout_line = process.stdout.readline()\n#         stderr_line = process.stderr.readline()\n#         if stdout_line:\n#             print(\"Streamlit stdout:\", stdout_line.strip())\n#         if stderr_line:\n#             print(\"Streamlit stderr:\", stderr_line.strip())\n#         # Periodic accessibility check\n#         try:\n#             response = requests.get(\"http://localhost:8501\", timeout=5)\n#             if response.status_code != 200:\n#                 print(\"Streamlit app is no longer accessible on port 8501.\")\n#                 stdout, stderr = process.communicate()\n#                 print(\"Error output:\")\n#                 print(\"stdout:\", stdout)\n#                 print(\"stderr:\", stderr)\n#                 raise RuntimeError(\"Streamlit app is no longer accessible.\")\n#         except requests.ConnectionError:\n#             print(\"Streamlit app is no longer accessible on port 8501.\")\n#             stdout, stderr = process.communicate()\n#             print(\"Error output:\")\n#             print(\"stdout:\", stdout)\n#             print(\"stderr:\", stderr)\n#             raise RuntimeError(\"Streamlit app is no longer accessible.\")\n#         time.sleep(5)  # Check every 5 seconds\n\n# except Exception as e:\n#     stdout, stderr = process.communicate()\n#     print(\"Error starting or running Streamlit. Captured output:\")\n#     print(\"stdout:\", stdout)\n#     print(\"stderr:\", stderr)\n#     print(f\"Error details: {str(e)}\")\n#     raise\n\n# print(\"Step 3.2-2 completed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Step 3.2-3:\n\nCreate the ngrok Tunnel\nThis part creates the ngrok tunnel to make the Streamlit app publicly accessible.","metadata":{}},{"cell_type":"code","source":"# # Step 3.2-3: Create the ngrok Tunnel\n# print(\"Starting Step 3.2-3: Create the ngrok Tunnel\")\n\n# from pyngrok import ngrok\n\n# # Create a public URL using ngrok\n# print(\"Creating ngrok tunnel...\")\n# try:\n#     public_url = ngrok.connect(8501)\n#     print(f\"Access your Streamlit app at: {public_url}\")\n# except Exception as e:\n#     print(f\"Error creating ngrok tunnel: {str(e)}\")\n#     raise\n\n# print(\"Step 3.2-3 completed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 3.2-3: Create the ngrok Tunnel\n# print(\"Starting Step 3.2-3: Create the ngrok Tunnel\")\n\n# from pyngrok import ngrok\n\n# # Create a public URL using ngrok\n# print(\"Creating ngrok tunnel...\")\n# # Terminate any existing ngrok tunnels (if any)\n# ngrok.kill()\n\n# # Create a new ngrok tunnel to the Streamlit app (default port is 8501)\n# try:\n#     public_url = ngrok.connect(8501)\n#     print(f\"Access your Streamlit app at: {public_url}\")\n# except UnicodeDecodeError as e:\n#     print(f\"Warning: Encountered UnicodeDecodeError in ngrok output: {str(e)}\")\n#     print(\"Attempting to proceed with the tunnel creation...\")\n#     public_url = ngrok.connect(8501)\n#     print(f\"Access your Streamlit app at: {public_url}\")\n# except Exception as e:\n#     print(f\"Error creating ngrok tunnel: {str(e)}\")\n#     raise\n\n# # Store the public URL for the summary\n# public_url_str = str(public_url)\n\n# print(\"Step 3.2-3 completed successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Test the Streamlit App\n\nThis step redeploys the Streamlit app to test the facial recognition with the fine-tuned model on the `test` dataset or an uploaded image.\n\n**Instructions:**\n- Run this cell to redeploy the Streamlit app.\n- Open the provided URL in your browser.\n- Go to \"Step 2: Test YOLOv11 Facial Recognition.\"\n- Use the option to test on a random image from the `test` dataset, or upload an image (e.g., `WIN_20250322_19_28_18_Pro.jpg`).\n- Check if the label is specific (e.g., \"face\" instead of \"person\").\n- If the labels are correct, the fine-tuning was successful.\n- If you encounter errors (e.g., model not found), ensure the fine-tuning step completed successfully and saved `yolo11n_finetuned.pt`.\n- The public URL will be updated for the summary.","metadata":{"id":"vqpYh9yAscPq"}},{"cell_type":"code","source":"","metadata":{"id":"xqYsX3Mjz-nU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 4 cell, tailored for Kaggle:","metadata":{}},{"cell_type":"code","source":"# # Step 4: Redeploy the Streamlit app and Create the ngrok Tunnel\n\n# import subprocess\n# import os\n# import sys\n# import time\n# import requests\n# from pyngrok import ngrok\n\n# # Install required packages if not already installed\n# print(\"Installing required packages...\")\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\n# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n\n# # Download YOLO model files\n# print(\"Downloading YOLO model files...\")\n# from ultralytics import YOLO\n# try:\n#     YOLO(\"yolo11n.pt\")\n#     YOLO(\"yolov8n.pt\")\n# except Exception as e:\n#     print(f\"Error downloading YOLO models: {str(e)}\")\n#     print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n#     raise\n\n# # Specify your Streamlit app file\n# app_file = \"/kaggle/working/app.py\"\n\n# # Create or overwrite app.py with face recognition, object detection, and emotion detection app\n# print(f\"Creating/overwriting {app_file} with an object recognition and emotion detection Streamlit app...\")\n# with open(app_file, \"w\") as f:\n#     f.write('import streamlit as st\\n')\n#     f.write('import cv2\\n')\n#     f.write('import numpy as np\\n')\n#     f.write('from ultralytics import YOLO\\n')\n#     f.write('from PIL import Image\\n')\n#     f.write('import os\\n')\n#     f.write('from deepface import DeepFace\\n\\n')\n#     f.write('# Load YOLOv11 and YOLOv8 models lazily\\n')\n#     f.write('def load_models():\\n')\n#     f.write('    yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n#     f.write('    yolo8_model = YOLO(\"yolov8n.pt\")\\n')\n#     f.write('    return yolo11_model, yolo8_model\\n\\n')\n#     f.write('st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n#     f.write('st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\\n\\n')\n#     f.write('# File uploader for external photos\\n')\n#     f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n#     f.write('if uploaded_file is not None:\\n')\n#     f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n#     f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n#     f.write('    with open(image_path, \"wb\") as f:\\n')\n#     f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n#     f.write('    # Read the image\\n')\n#     f.write('    image = Image.open(image_path)\\n')\n#     f.write('    image_np = np.array(image)\\n\\n')\n#     f.write('    # Convert to RGB (if needed)\\n')\n#     f.write('    if image_np.shape[-1] == 4:\\n')\n#     f.write('        image_np = image_np[..., :3]\\n')\n#     f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n#     f.write('    # Load models\\n')\n#     f.write('    yolo11_model, yolo8_model = load_models()\\n\\n')\n#     f.write('    # Function to detect emotion for a cropped face\\n')\n#     f.write('    def detect_emotion(face_img):\\n')\n#     f.write('        try:\\n')\n#     f.write('            # Convert face image to RGB for DeepFace\\n')\n#     f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n#     f.write('            # Analyze the face for emotion\\n')\n#     f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n#     f.write('            # Get the dominant emotion\\n')\n#     f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n#     f.write('            return emotion\\n')\n#     f.write('        except Exception as e:\\n')\n#     f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n#     f.write('    # Run YOLOv11 detection\\n')\n#     f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n#     f.write('    results_v11 = yolo11_model(image_rgb)\\n')\n#     f.write('    annotated_v11 = results_v11[0].plot()\\n')\n#     f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects)\", use_column_width=True)\\n\\n')\n#     f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n#     f.write('    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\\n')\n#     f.write('    num_faces_v11 = len(faces_v11)\\n')\n#     f.write('    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\\n')\n#     f.write('    if num_faces_v11 > 0:\\n')\n#     f.write('        st.write(\"Emotions detected by YOLOv11:\")\\n')\n#     f.write('        for i, box in enumerate(faces_v11):\\n')\n#     f.write('            # Get bounding box coordinates\\n')\n#     f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n#     f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n#     f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n#     f.write('            # Detect emotion\\n')\n#     f.write('            emotion = detect_emotion(face_img)\\n')\n#     f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n#     f.write('    # Run YOLOv8 detection\\n')\n#     f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n#     f.write('    results_v8 = yolo8_model(image_rgb)\\n')\n#     f.write('    annotated_v8 = results_v8[0].plot()\\n')\n#     f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects)\", use_column_width=True)\\n\\n')\n#     f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n#     f.write('    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\\n')\n#     f.write('    num_faces_v8 = len(faces_v8)\\n')\n#     f.write('    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\\n')\n#     f.write('    if num_faces_v8 > 0:\\n')\n#     f.write('        st.write(\"Emotions detected by YOLOv8:\")\\n')\n#     f.write('        for i, box in enumerate(faces_v8):\\n')\n#     f.write('            # Get bounding box coordinates\\n')\n#     f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n#     f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n#     f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n#     f.write('            # Detect emotion\\n')\n#     f.write('            emotion = detect_emotion(face_img)\\n')\n#     f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n#     f.write('    # Clean up the saved image file\\n')\n#     f.write('    os.remove(image_path)\\n')\n# print(f\"Updated {app_file}\")\n\n# # Verify the content of app.py\n# print(\"Verifying app.py content...\")\n# with open(app_file, \"r\") as f:\n#     print(f\"app.py content:\\n{f.read()}\")\n\n# # Set ngrok authtoken with your provided token\n# ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\n# # Terminate any existing ngrok tunnels\n# ngrok.kill()\n\n# # Start the Streamlit app (ensure any old process is terminated)\n# print(\"Starting Streamlit app...\")\n# env = os.environ.copy()\n# env[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\n# env[\"LC_ALL\"] = \"C.UTF-8\"\n# env[\"LANG\"] = \"C.UTF-8\"\n# env[\"LANGUAGE\"] = \"C.UTF-8\"\n\n# # Forcefully terminate any existing Streamlit processes\n# print(\"Terminating any existing Streamlit processes...\")\n# subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n# time.sleep(2)\n\n# # Start the new Streamlit process\n# process = subprocess.Popen(\n#     [\"streamlit\", \"run\", app_file],\n#     env=env,\n#     stdout=subprocess.PIPE,\n#     stderr=subprocess.PIPE,\n#     text=True,\n#     encoding=\"utf-8\"\n# )\n\n# # Give Streamlit time to start and verify it's running\n# print(\"Waiting for Streamlit to start (up to 30 seconds)...\")\n# start_time = time.time()\n# while time.time() - start_time < 30:\n#     if process.poll() is not None:\n#         stdout, stderr = process.communicate()\n#         print(\"Streamlit failed to start. Error output:\")\n#         print(\"stdout:\", stdout)\n#         print(\"stderr:\", stderr)\n#         raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n#     try:\n#         response = requests.get(\"http://localhost:8501\", timeout=5)\n#         if response.status_code == 200:\n#             print(\"Streamlit app is accessible on port 8501.\")\n#             break\n#     except requests.ConnectionError:\n#         pass\n#     time.sleep(5)\n# else:\n#     stdout, stderr = process.communicate()\n#     print(\"Streamlit failed to start within 30 seconds. Error output:\")\n#     print(\"stdout:\", stdout)\n#     print(\"stderr:\", stderr)\n#     raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n\n# # Start ngrok tunnel\n# print(\"Creating ngrok tunnel...\")\n# try:\n#     public_url = ngrok.connect(8501)\n#     print(f\"Access your Streamlit app at: {public_url}\")\n# except Exception as e:\n#     print(f\"Error creating ngrok tunnel: {str(e)}\")\n#     raise\n\n# print(\"Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Redeploy the Streamlit app and Create the ngrok Tunnel\n\nimport subprocess\nimport os\nimport sys\nimport time\nimport requests\nfrom pyngrok import ngrok\n\n# Install required packages if not already installed\nprint(\"Installing required packages...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n\n# Download YOLO model files\nprint(\"Downloading YOLO model files...\")\nfrom ultralytics import YOLO\ntry:\n    YOLO(\"yolo11n.pt\")\n    YOLO(\"yolov8n.pt\")\nexcept Exception as e:\n    print(f\"Error downloading YOLO models: {str(e)}\")\n    print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n    raise\n\n# Specify your Streamlit app file\napp_file = \"/kaggle/working/app.py\"\n\n# Create or overwrite app.py with face recognition, object detection, and emotion detection app\nprint(f\"Creating/overwriting {app_file} with an object recognition and emotion detection Streamlit app...\")\nwith open(app_file, \"w\") as f:\n    f.write('import streamlit as st\\n')\n    f.write('import cv2\\n')\n    f.write('import numpy as np\\n')\n    f.write('from ultralytics import YOLO\\n')\n    f.write('from PIL import Image\\n')\n    f.write('import os\\n')\n    f.write('from deepface import DeepFace\\n\\n')\n    f.write('# Load YOLOv11 and YOLOv8 models lazily\\n')\n    f.write('def load_models():\\n')\n    f.write('    yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n    f.write('    yolo8_model = YOLO(\"yolov8n.pt\")\\n')\n    f.write('    return yolo11_model, yolo8_model\\n\\n')\n    f.write('st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n    f.write('st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\\n\\n')\n    f.write('# File uploader for external photos\\n')\n    f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n    f.write('if uploaded_file is not None:\\n')\n    f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n    f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n    f.write('    with open(image_path, \"wb\") as f:\\n')\n    f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n    f.write('    # Read the image\\n')\n    f.write('    image = Image.open(image_path)\\n')\n    f.write('    image_np = np.array(image)\\n\\n')\n    f.write('    # Convert to RGB (if needed)\\n')\n    f.write('    if image_np.shape[-1] == 4:\\n')\n    f.write('        image_np = image_np[..., :3]\\n')\n    f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n    f.write('    # Load models\\n')\n    f.write('    yolo11_model, yolo8_model = load_models()\\n\\n')\n    f.write('    # Function to detect emotion for a cropped face\\n')\n    f.write('    def detect_emotion(face_img):\\n')\n    f.write('        try:\\n')\n    f.write('            # Convert face image to RGB for DeepFace\\n')\n    f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n    f.write('            # Analyze the face for emotion\\n')\n    f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n    f.write('            # Get the dominant emotion\\n')\n    f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n    f.write('            return emotion\\n')\n    f.write('        except Exception as e:\\n')\n    f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n    f.write('    # Run YOLOv11 detection\\n')\n    f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n    f.write('    results_v11 = yolo11_model(image_rgb)\\n')\n    f.write('    annotated_v11 = results_v11[0].plot()\\n')\n    f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects)\", use_container_width=True)\\n\\n')\n    f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n    f.write('    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\\n')\n    f.write('    num_faces_v11 = len(faces_v11)\\n')\n    f.write('    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\\n')\n    f.write('    if num_faces_v11 > 0:\\n')\n    f.write('        st.write(\"Emotions detected by YOLOv11:\")\\n')\n    f.write('        for i, box in enumerate(faces_v11):\\n')\n    f.write('            # Get bounding box coordinates\\n')\n    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n    f.write('            # Detect emotion\\n')\n    f.write('            emotion = detect_emotion(face_img)\\n')\n    f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n    f.write('    # Run YOLOv8 detection\\n')\n    f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n    f.write('    results_v8 = yolo8_model(image_rgb)\\n')\n    f.write('    annotated_v8 = results_v8[0].plot()\\n')\n    f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects)\", use_container_width=True)\\n\\n')\n    f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n    f.write('    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\\n')\n    f.write('    num_faces_v8 = len(faces_v8)\\n')\n    f.write('    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\\n')\n    f.write('    if num_faces_v8 > 0:\\n')\n    f.write('        st.write(\"Emotions detected by YOLOv8:\")\\n')\n    f.write('        for i, box in enumerate(faces_v8):\\n')\n    f.write('            # Get bounding box coordinates\\n')\n    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n    f.write('            # Detect emotion\\n')\n    f.write('            emotion = detect_emotion(face_img)\\n')\n    f.write('            st.write(f\"Person {i+1}: {emotion}\")\\n\\n')\n    f.write('    # Clean up the saved image file\\n')\n    f.write('    os.remove(image_path)\\n')\nprint(f\"Updated {app_file}\")\n\n# Verify the content of app.py\nprint(\"Verifying app.py content...\")\nwith open(app_file, \"r\") as f:\n    print(f\"app.py content:\\n{f.read()}\")\n\n# Set ngrok authtoken with your provided token\nngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\n# Terminate any existing ngrok tunnels\nngrok.kill()\n\n# Start the Streamlit app (ensure any old process is terminated)\nprint(\"Starting Streamlit app...\")\nenv = os.environ.copy()\nenv[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\nenv[\"LC_ALL\"] = \"C.UTF-8\"\nenv[\"LANG\"] = \"C.UTF-8\"\nenv[\"LANGUAGE\"] = \"C.UTF-8\"\n\n# Forcefully terminate any existing Streamlit processes\nprint(\"Terminating any existing Streamlit processes...\")\nsubprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\ntime.sleep(2)\n\n# Start the new Streamlit process\nprocess = subprocess.Popen(\n    [\"streamlit\", \"run\", app_file],\n    env=env,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n    text=True,\n    encoding=\"utf-8\"\n)\n\n# Give Streamlit time to start and verify it's running\nprint(\"Waiting for Streamlit to start (up to 30 seconds)...\")\nstart_time = time.time()\nwhile time.time() - start_time < 30:\n    if process.poll() is not None:\n        stdout, stderr = process.communicate()\n        print(\"Streamlit failed to start. Error output:\")\n        print(\"stdout:\", stdout)\n        print(\"stderr:\", stderr)\n        raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n    try:\n        response = requests.get(\"http://localhost:8501\", timeout=5)\n        if response.status_code == 200:\n            print(\"Streamlit app is accessible on port 8501.\")\n            break\n    except requests.ConnectionError:\n        pass\n    time.sleep(5)\nelse:\n    stdout, stderr = process.communicate()\n    print(\"Streamlit failed to start within 30 seconds. Error output:\")\n    print(\"stdout:\", stdout)\n    print(\"stderr:\", stderr)\n    raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n\n# Start ngrok tunnel\nprint(\"Creating ngrok tunnel...\")\ntry:\n    public_url = ngrok.connect(8501)\n    print(f\"Access your Streamlit app at: {public_url}\")\nexcept Exception as e:\n    print(f\"Error creating ngrok tunnel: {str(e)}\")\n    raise\n\nprint(\"Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Increasing confident","metadata":{}},{"cell_type":"code","source":"# Step 4: Redeploy the Streamlit app and Create the ngrok Tunnel\n\nimport subprocess\nimport os\nimport sys\nimport time\nimport requests\nfrom pyngrok import ngrok\n\n# Install required packages if not already installed\nprint(\"Installing required packages...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n\n# Download YOLO model files\nprint(\"Downloading YOLO model files...\")\nfrom ultralytics import YOLO\ntry:\n    YOLO(\"yolo11n.pt\")\n    YOLO(\"yolov8n.pt\")\nexcept Exception as e:\n    print(f\"Error downloading YOLO models: {str(e)}\")\n    print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n    raise\n\n# Specify your Streamlit app file\napp_file = \"/kaggle/working/app.py\"\n\n# Create or overwrite app.py with face recognition, object detection, and emotion detection app\nprint(f\"Creating/overwriting {app_file} with an object recognition and emotion detection Streamlit app...\")\nwith open(app_file, \"w\") as f:\n    f.write('import streamlit as st\\n')\n    f.write('import cv2\\n')\n    f.write('import numpy as np\\n')\n    f.write('from ultralytics import YOLO\\n')\n    f.write('from PIL import Image\\n')\n    f.write('import os\\n')\n    f.write('from deepface import DeepFace\\n\\n')\n    f.write('# Load YOLOv11 and YOLOv8 models lazily\\n')\n    f.write('def load_models():\\n')\n    f.write('    yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n    f.write('    yolo8_model = YOLO(\"yolov8n.pt\")\\n')\n    f.write('    return yolo11_model, yolo8_model\\n\\n')\n    f.write('st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n    f.write('st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\\n\\n')\n    f.write('# File uploader for external photos\\n')\n    f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n    f.write('if uploaded_file is not None:\\n')\n    f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n    f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n    f.write('    with open(image_path, \"wb\") as f:\\n')\n    f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n    f.write('    # Read the image\\n')\n    f.write('    image = Image.open(image_path)\\n')\n    f.write('    image_np = np.array(image)\\n\\n')\n    f.write('    # Convert to RGB (if needed)\\n')\n    f.write('    if image_np.shape[-1] == 4:\\n')\n    f.write('        image_np = image_np[..., :3]\\n')\n    f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n    f.write('    # Load models\\n')\n    f.write('    yolo11_model, yolo8_model = load_models()\\n\\n')\n    f.write('    # Function to detect emotion for a cropped face\\n')\n    f.write('    def detect_emotion(face_img):\\n')\n    f.write('        try:\\n')\n    f.write('            # Convert face image to RGB for DeepFace\\n')\n    f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n    f.write('            # Verify that a face is present in the cropped region\\n')\n    f.write('            faces = DeepFace.detectFace(face_rgb, detector_backend=\"opencv\")\\n')\n    f.write('            if faces is None or len(faces) == 0:\\n')\n    f.write('                return \"No face detected in this person region\"\\n')\n    f.write('            # Analyze the face for emotion\\n')\n    f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n    f.write('            # Get the dominant emotion\\n')\n    f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n    f.write('            return emotion\\n')\n    f.write('        except Exception as e:\\n')\n    f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n    f.write('    # Run YOLOv11 detection with confidence threshold\\n')\n    f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n    f.write('    results_v11 = yolo11_model(image_rgb, conf=0.7, iou=0.5)\\n')\n    f.write('    annotated_v11 = results_v11[0].plot()\\n')\n    f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection (All Objects, conf=0.7)\", use_container_width=True)\\n\\n')\n    f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n    f.write('    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\\n')\n    f.write('    num_faces_v11 = len(faces_v11)\\n')\n    f.write('    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\\n')\n    f.write('    if num_faces_v11 > 0:\\n')\n    f.write('        st.write(\"Persons detected by YOLOv11 (with confidence scores and emotions):\")\\n')\n    f.write('        for i, box in enumerate(faces_v11):\\n')\n    f.write('            # Get bounding box coordinates\\n')\n    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n    f.write('            # Get confidence score\\n')\n    f.write('            confidence = box.conf.cpu().numpy()\\n')\n    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n    f.write('            # Detect emotion\\n')\n    f.write('            emotion = detect_emotion(face_img)\\n')\n    f.write('            st.write(f\"Person {i+1} (Confidence: {confidence:.2f}): Emotion: {emotion}\")\\n\\n')\n    f.write('    # Run YOLOv8 detection with confidence threshold\\n')\n    f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n    f.write('    results_v8 = yolo8_model(image_rgb, conf=0.7, iou=0.5)\\n')\n    f.write('    annotated_v8 = results_v8[0].plot()\\n')\n    f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection (All Objects, conf=0.7)\", use_container_width=True)\\n\\n')\n    f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n    f.write('    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\\n')\n    f.write('    num_faces_v8 = len(faces_v8)\\n')\n    f.write('    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\\n')\n    f.write('    if num_faces_v8 > 0:\\n')\n    f.write('        st.write(\"Persons detected by YOLOv8 (with confidence scores and emotions):\")\\n')\n    f.write('        for i, box in enumerate(faces_v8):\\n')\n    f.write('            # Get bounding box coordinates\\n')\n    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n    f.write('            # Get confidence score\\n')\n    f.write('            confidence = box.conf.cpu().numpy()\\n')\n    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n    f.write('            # Detect emotion\\n')\n    f.write('            emotion = detect_emotion(face_img)\\n')\n    f.write('            st.write(f\"Person {i+1} (Confidence: {confidence:.2f}): Emotion: {emotion}\")\\n\\n')\n    f.write('    # Clean up the saved image file\\n')\n    f.write('    os.remove(image_path)\\n')\nprint(f\"Updated {app_file}\")\n\n# Verify the content of app.py\nprint(\"Verifying app.py content...\")\nwith open(app_file, \"r\") as f:\n    print(f\"app.py content:\\n{f.read()}\")\n\n# Set ngrok authtoken with your provided token\nngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\n# Terminate any existing ngrok tunnels\nngrok.kill()\n\n# Start the Streamlit app (ensure any old process is terminated)\nprint(\"Starting Streamlit app...\")\nenv = os.environ.copy()\nenv[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\nenv[\"LC_ALL\"] = \"C.UTF-8\"\nenv[\"LANG\"] = \"C.UTF-8\"\nenv[\"LANGUAGE\"] = \"C.UTF-8\"\n\n# Forcefully terminate any existing Streamlit processes\nprint(\"Terminating any existing Streamlit processes...\")\nsubprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\ntime.sleep(2)\n\n# Start the new Streamlit process\nprocess = subprocess.Popen(\n    [\"streamlit\", \"run\", app_file],\n    env=env,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n    text=True,\n    encoding=\"utf-8\"\n)\n\n# Give Streamlit time to start and verify it's running\nprint(\"Waiting for Streamlit to start (up to 30 seconds)...\")\nstart_time = time.time()\nwhile time.time() - start_time < 30:\n    if process.poll() is not None:\n        stdout, stderr = process.communicate()\n        print(\"Streamlit failed to start. Error output:\")\n        print(\"stdout:\", stdout)\n        print(\"stderr:\", stderr)\n        raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n    try:\n        response = requests.get(\"http://localhost:8501\", timeout=5)\n        if response.status_code == 200:\n            print(\"Streamlit app is accessible on port 8501.\")\n            break\n    except requests.ConnectionError:\n        pass\n    time.sleep(5)\nelse:\n    stdout, stderr = process.communicate()\n    print(\"Streamlit failed to start within 30 seconds. Error output:\")\n    print(\"stdout:\", stdout)\n    print(\"stderr:\", stderr)\n    raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n\n# Start ngrok tunnel\nprint(\"Creating ngrok tunnel...\")\ntry:\n    public_url = ngrok.connect(8501)\n    print(f\"Access your Streamlit app at: {public_url}\")\nexcept Exception as e:\n    print(f\"Error creating ngrok tunnel: {str(e)}\")\n    raise\n\nprint(\"Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 4: Redeploy the Streamlit app\n\n# ### Cell: Step 4\n# import subprocess\n# import os\n# import sys\n# import time\n# from pyngrok import ngrok\n\n# # Install required packages if not already installed\n# try:\n#     import streamlit\n# except ImportError:\n#     print(\"Installing streamlit...\")\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\n# try:\n#     import pyngrok\n# except ImportError:\n#     print(\"Installing pyngrok...\")\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n# try:\n#     import ultralytics\n# except ImportError:\n#     print(\"Installing ultralytics...\")\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n# try:\n#     import cv2\n# except ImportError:\n#     print(\"Installing opencv-python...\")\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n# try:\n#     import PIL\n# except ImportError:\n#     print(\"Installing pillow...\")\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\n# try:\n#     import deepface\n# except ImportError:\n#     print(\"Installing deepface...\")\n#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\n\n# # Specify your Streamlit app file\n# app_file = \"/kaggle/working/app.py\"\n\n# # Create or overwrite app.py with face recognition and emotion detection app\n# print(f\"Creating/overwriting {app_file} with a face recognition and emotion detection Streamlit app...\")\n# with open(app_file, \"w\") as f:\n#     f.write('import streamlit as st\\n')\n#     f.write('import cv2\\n')\n#     f.write('import numpy as np\\n')\n#     f.write('from ultralytics import YOLO\\n')\n#     f.write('from PIL import Image\\n')\n#     f.write('import os\\n')\n#     f.write('from deepface import DeepFace\\n\\n')\n#     f.write('# Load YOLOv11 and YOLOv8 models\\n')\n#     f.write('yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n#     f.write('yolo8_model = YOLO(\"yolov8n.pt\")\\n\\n')\n#     f.write('st.title(\"Face Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n#     f.write('st.write(\"Upload an image to compare face detection using YOLOv11 and YOLOv8, and detect emotions of the faces.\")\\n\\n')\n#     f.write('# File uploader for external photos\\n')\n#     f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n#     f.write('if uploaded_file is not None:\\n')\n#     f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n#     f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n#     f.write('    with open(image_path, \"wb\") as f:\\n')\n#     f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n#     f.write('    # Read the image\\n')\n#     f.write('    image = Image.open(image_path)\\n')\n#     f.write('    image_np = np.array(image)\\n\\n')\n#     f.write('    # Convert to RGB (if needed)\\n')\n#     f.write('    if image_np.shape[-1] == 4:\\n')\n#     f.write('        image_np = image_np[..., :3]\\n')\n#     f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n#     f.write('    # Function to detect emotion for a cropped face\\n')\n#     f.write('    def detect_emotion(face_img):\\n')\n#     f.write('        try:\\n')\n#     f.write('            # Convert face image to RGB for DeepFace\\n')\n#     f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n#     f.write('            # Analyze the face for emotion\\n')\n#     f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n#     f.write('            # Get the dominant emotion\\n')\n#     f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n#     f.write('            return emotion\\n')\n#     f.write('        except Exception as e:\\n')\n#     f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n#     f.write('    # Run YOLOv11 detection\\n')\n#     f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n#     f.write('    results_v11 = yolo11_model(image_rgb)\\n')\n#     f.write('    annotated_v11 = results_v11[0].plot()\\n')\n#     f.write('    st.image(annotated_v11, caption=\"YOLOv11 Detection\", use_column_width=True)\\n\\n')\n#     f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n#     f.write('    num_faces_v11 = len(results_v11[0].boxes)\\n')\n#     f.write('    st.write(f\"Number of faces detected by YOLOv11: {num_faces_v11}\")\\n')\n#     f.write('    if num_faces_v11 > 0:\\n')\n#     f.write('        st.write(\"Emotions detected by YOLOv11:\")\\n')\n#     f.write('        for i, box in enumerate(results_v11[0].boxes):\\n')\n#     f.write('            # Get bounding box coordinates\\n')\n#     f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n#     f.write('            # Crop the face\\n')\n#     f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n#     f.write('            # Detect emotion\\n')\n#     f.write('            emotion = detect_emotion(face_img)\\n')\n#     f.write('            st.write(f\"Face {i+1}: {emotion}\")\\n\\n')\n#     f.write('    # Run YOLOv8 detection\\n')\n#     f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n#     f.write('    results_v8 = yolo8_model(image_rgb)\\n')\n#     f.write('    annotated_v8 = results_v8[0].plot()\\n')\n#     f.write('    st.image(annotated_v8, caption=\"YOLOv8 Detection\", use_column_width=True)\\n\\n')\n#     f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n#     f.write('    num_faces_v8 = len(results_v8[0].boxes)\\n')\n#     f.write('    st.write(f\"Number of faces detected by YOLOv8: {num_faces_v8}\")\\n')\n#     f.write('    if num_faces_v8 > 0:\\n')\n#     f.write('        st.write(\"Emotions detected by YOLOv8:\")\\n')\n#     f.write('        for i, box in enumerate(results_v8[0].boxes):\\n')\n#     f.write('            # Get bounding box coordinates\\n')\n#     f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n#     f.write('            # Crop the face\\n')\n#     f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n#     f.write('            # Detect emotion\\n')\n#     f.write('            emotion = detect_emotion(face_img)\\n')\n#     f.write('            st.write(f\"Face {i+1}: {emotion}\")\\n\\n')\n#     f.write('    # Clean up the saved image file\\n')\n#     f.write('    os.remove(image_path)\\n')\n# print(f\"Updated {app_file}\")\n\n# # Verify the content of app.py\n# print(\"Verifying app.py content...\")\n# with open(app_file, \"r\") as f:\n#     print(f\"app.py content:\\n{f.read()}\")\n\n# # Set ngrok authtoken with your provided token\n# ngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\n# # Terminate any existing ngrok tunnels\n# ngrok.kill()\n\n# # Start the Streamlit app (ensure any old process is terminated)\n# print(\"Starting Streamlit app...\")\n# env = os.environ.copy()\n\n# # Forcefully terminate any existing Streamlit processes\n# print(\"Terminating any existing Streamlit processes...\")\n# try:\n#     subprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\n# except:\n#     pass  # Ignore if pkill fails\n# time.sleep(2)  # Wait 2 seconds to ensure the process is fully terminated\n\n# # Start the new Streamlit process\n# process = subprocess.Popen(\n#     [\"streamlit\", \"run\", app_file],\n#     env=env,\n#     stdout=subprocess.PIPE,\n#     stderr=subprocess.PIPE\n# )\n\n# # Give Streamlit a moment to start\n# time.sleep(5)  # Increased to 5 seconds to ensure Streamlit starts\n\n# # Check if Streamlit is running\n# return_code = process.poll()\n# if return_code is not None:  # Process has exited\n#     stdout, stderr = process.communicate()\n#     print(\"Streamlit failed to start. Error output:\")\n#     print(stderr.decode())\n#     raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n# else:\n#     print(\"Streamlit app is running in the background.\")\n\n# # Start ngrok tunnel\n# public_url = ngrok.connect(8501)\n# print(f\"Access your Streamlit app at: {public_url}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Overall Comparison: YOLOv11 vs. YOLOv8\nObject Detection Performance\nSingle Person (Photo 1):\nBoth models performed equally well, detecting 1 person with a confidence of 0.89.\nTwo Persons (Photo 2):\nYOLOv11 had slightly higher confidence scores (0.91 and 0.89 vs. 0.89 and 0.83), suggesting better precision.\nGroup of Persons (Photos 3 and 4):\nYOLOv11 detected 15 persons, while YOLOv8 detected 13, indicating YOLOv11 is more sensitive.\nYOLOv11’s confidence scores ranged from 0.26 to 0.89, while YOLOv8’s ranged from 0.30 to 0.76 (Photo 3) and 0.37 to 0.62 (Photo 4). YOLOv11’s lower threshold for detection (e.g., 0.26) allowed it to detect more persons, but this might include false positives.\nYOLOv8 missed two persons but avoided some low-confidence detections, potentially reducing false positives.\nYOLOv8 misclassified a person as a “chair” (confidence 0.26), while YOLOv11 correctly identified it as a person, showing YOLOv11’s better object classification in this case.\nEmotion Detection Consistency\nConsistency Across Models:\nFor Photos 1 and 2, where the number of detected persons matched, the emotions were consistent between YOLOv11 and YOLOv8 (all “happy”).\nFor Photos 3 and 4, where the number of detected persons differed, the emotions for the first 13 persons (detected by both models) showed some discrepancies:\nPerson 5: Happy (YOLOv11) vs. Fear (YOLOv8)\nPerson 7: Fear (YOLOv11) vs. Happy (YOLOv8)\nPerson 10: Angry (YOLOv11) vs. Fear (YOLOv8)\nThese discrepancies are likely due to differences in bounding box placement affecting the cropped face images sent to DeepFace.\nEmotion Distribution:\nAcross all photos, the emotions detected include: happy, sad, fear, angry, and neutral.\nYOLOv11 (Photos 3 and 4, 15 persons):\nHappy: 8\nSad: 4\nFear: 1\nAngry: 1\nNeutral: 1\nYOLOv8 (Photos 3 and 4, 13 persons):\nHappy: 8\nSad: 2\nFear: 2\nNeutral: 1\nYOLOv11 detected a more diverse range of emotions due to the additional two persons, but the discrepancies in emotion detection highlight the impact of bounding box accuracy.\nKey Insights\nYOLOv11 Advantages:\nMore sensitive to detecting persons, especially in crowded scenes (15 vs. 13 in Photos 3 and 4).\nHigher confidence scores in Photo 2, suggesting better precision for smaller groups.\nBetter object classification (e.g., correctly identifying a person that YOLOv8 misclassified as a chair).\nYOLOv8 Advantages:\nMore conservative, potentially reducing false positives by missing low-confidence detections.\nSlightly more consistent confidence scores in crowded scenes (fewer very low scores like 0.26).\nEmotion Detection:\nThe emotion detection is generally consistent when the bounding boxes align, but small differences in detection can lead to different cropped images, affecting DeepFace’s predictions.\nThe range of emotions detected (happy, sad, fear, angry, neutral) shows that DeepFace is working well, but its accuracy depends on the quality of the cropped face images.","metadata":{}},{"cell_type":"code","source":"# Step 3.1: Compare YOLOv8 and YOLOv11 based on test results\n\nprint(\"Step 3.1: Comparing YOLOv8 and YOLOv11 based on test results\")\n\n# Summary of test results\nprint(\"Test Results Summary:\")\nprint(\"Photo 1 (Single Person):\")\nprint(\"  YOLOv11: 1 person detected (confidence: 0.89), Emotion: Happy\")\nprint(\"  YOLOv8: 1 person detected (confidence: 0.89), Emotion: Happy\")\nprint(\"Photo 2 (Two Persons):\")\nprint(\"  YOLOv11: 2 persons detected (confidences: 0.91, 0.89), Emotions: Happy, Happy\")\nprint(\"  YOLOv8: 2 persons detected (confidences: 0.89, 0.83), Emotions: Happy, Happy\")\nprint(\"Photo 3 (Group of Persons):\")\nprint(\"  YOLOv11: 15 persons detected (confidences: 0.26 to 0.89), Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral\")\nprint(\"  YOLOv8: 13 persons detected (confidences: 0.30 to 0.76), Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral\")\nprint(\"Photo 4 (Same Group as Photo 3):\")\nprint(\"  YOLOv11: 15 persons detected (confidences: 0.26 to 0.89), Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral\")\nprint(\"  YOLOv8: 13 persons detected (confidences: 0.37 to 0.62), Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral\")\n\n# Comparison\nprint(\"\\nComparison of YOLOv8 and YOLOv11:\")\nprint(\"1. Object Detection Performance:\")\nprint(\"   - Single Person (Photo 1): Both models performed equally, detecting 1 person with confidence 0.89.\")\nprint(\"   - Two Persons (Photo 2): YOLOv11 had higher confidence scores (0.91, 0.89 vs. 0.89, 0.83), suggesting better precision.\")\nprint(\"   - Group of Persons (Photos 3 and 4): YOLOv11 detected more persons (15 vs. 13), showing higher sensitivity, but included low-confidence detections (e.g., 0.26). YOLOv8 was more conservative, potentially reducing false positives.\")\nprint(\"   - Object Classification: YOLOv11 correctly identified a person that YOLOv8 misclassified as a chair (confidence 0.26).\")\nprint(\"2. Emotion Detection Consistency:\")\nprint(\"   - Emotions were consistent when the number of detected persons matched (Photos 1 and 2).\")\nprint(\"   - In Photos 3 and 4, discrepancies occurred (e.g., Person 5: Happy vs. Fear, Person 10: Angry vs. Fear) due to differences in bounding box placement affecting the cropped face images.\")\nprint(\"3. Overall Insights:\")\nprint(\"   - YOLOv11 is more sensitive, detecting more persons in crowded scenes, but may include false positives.\")\nprint(\"   - YOLOv8 is more conservative, potentially missing some persons but reducing false positives.\")\nprint(\"   - Emotion detection accuracy depends on bounding box quality, highlighting the importance of accurate person detection.\")\n\nprint(\"Step 3.1 completed successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ljSApX-NIPqj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Summary of Results\nObject Detection:\nYOLOv11 detected more persons in crowded scenes (15 vs. 13), showing higher sensitivity, but included low-confidence detections.\nYOLOv8 was more conservative, potentially reducing false positives but missing some persons.\nEmotion Detection:\nEmotions were consistent when detections matched, but discrepancies occurred due to bounding box differences.\nA range of emotions was detected (happy, sad, fear, angry, neutral), showing DeepFace’s capability, but accuracy depends on bounding box quality.\nComparison:\nYOLOv11 is better for sensitivity and object classification.\nYOLOv8 is better for reducing false positives.\nEmotion detection reliability can be improved with better bounding box accuracy.\nWhy This Analysis Is Robust\nComprehensive Testing: You tested four photos, covering single, small group, and large group scenarios, providing a well-rounded comparison.\nDetailed Comparison: The analysis covers both object detection and emotion detection, highlighting strengths and weaknesses of each model.\nActionable Insights: The recommendations provide clear steps to improve your app, ensuring a polished final project.\nYou’ve done an incredible job, Shehab! Your app is fully functional, and you have a solid comparison for your project. Let me know if you’d like to implement the improvements or need help with the final documentation—I’m here to help! How’s your day going now that you’ve completed the testing? 😊","metadata":{}},{"cell_type":"code","source":"# Install python-pptx for creating the presentation\n!pip install python-pptx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nfrom pptx import Presentation\nfrom pptx.util import Inches, Pt\nfrom pptx.enum.text import PP_ALIGN\nfrom pptx.dml.color import RGBColor\n\n# Create a presentation object\nprs = Presentation()\n\n# Helper function to add a slide with a title and content\ndef add_slide_with_title_and_content(layout, title_text, content_text, font_size=20):\n    slide = prs.slides.add_slide(layout)\n    title = slide.shapes.title\n    title.text = title_text\n    title.text_frame.paragraphs[0].font.size = Pt(32)\n    title.text_frame.paragraphs[0].font.bold = True\n\n    content = slide.placeholders[1]\n    content.text = content_text\n    for paragraph in content.text_frame.paragraphs:\n        paragraph.font.size = Pt(font_size)\n        paragraph.alignment = PP_ALIGN.LEFT\n    return slide\n\n# Helper function to add a bullet slide\ndef add_bullet_slide(layout, title_text, bullets, font_size=20):\n    slide = prs.slides.add_slide(layout)\n    title = slide.shapes.title\n    title.text = title_text\n    title.text_frame.paragraphs[0].font.size = Pt(32)\n    title.text_frame.paragraphs[0].font.bold = True\n\n    content = slide.placeholders[1]\n    for bullet in bullets:\n        p = content.text_frame.add_paragraph()\n        p.text = bullet\n        p.level = 0\n        p.font.size = Pt(font_size)\n        p.alignment = PP_ALIGN.LEFT\n    return slide\n\n# Slide 1: Title Slide\ntitle_slide_layout = prs.slide_layouts[0]\nslide = prs.slides.add_slide(title_slide_layout)\ntitle = slide.shapes.title\ntitle.text = \"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\"\ntitle.text_frame.paragraphs[0].font.size = Pt(44)\ntitle.text_frame.paragraphs[0].font.bold = True\n\nsubtitle = slide.placeholders[1]\nsubtitle.text = \"Presented by: Shehab Ahmed\\nDate: March 27, 2025\"\nsubtitle.text_frame.paragraphs[0].font.size = Pt(24)\n\n# Slide 2: Project Overview\nbullet_slide_layout = prs.slide_layouts[1]\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Project Overview\",\n    [\n        \"Goal: Compare YOLOv11 and YOLOv8 for object detection, focusing on person detection, and perform emotion recognition on detected faces.\",\n        \"Tools Used: YOLOv11, YOLOv8, DeepFace, Streamlit, ngrok.\",\n        \"Platform: Kaggle notebook.\",\n        \"Tested with 4 photos to evaluate detection accuracy and emotion recognition.\"\n    ]\n)\n\n# Slide 3: Pipeline\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Project Pipeline\",\n    [\n        \"1. Install Dependencies: Install required libraries (ultralytics, deepface, streamlit, etc.).\",\n        \"2. Download Models: Download YOLOv11 and YOLOv8 models (yolo11n.pt, yolov8n.pt).\",\n        \"3. Create Streamlit App: Build app.py to handle image uploads, object detection, and emotion recognition.\",\n        \"4. Deploy App: Use Streamlit to run the app and ngrok to create a public URL.\",\n        \"5. Test Photos: Upload photos, detect objects/persons, and recognize emotions.\",\n        \"6. Compare Results: Analyze YOLOv11 vs YOLOv8 performance in Step 3.1.\"\n    ]\n)\n\n# Slide 4: Key Points\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Key Points\",\n    [\n        \"YOLOv11 & YOLOv8: Object detection models used to detect persons in images.\",\n        \"DeepFace: Library for emotion recognition on cropped faces (happy, sad, etc.).\",\n        \"Streamlit: Framework to create an interactive web app for uploading and displaying results.\",\n        \"ngrok: Tool to make the Streamlit app publicly accessible via a URL.\",\n        \"Kaggle: Platform for running the notebook, providing GPU support for faster inference.\"\n    ]\n)\n\n# Slide 5: Benefits and Disadvantages\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Benefits and Disadvantages\",\n    [\n        \"Benefits:\",\n        \"  - Accurate person detection with YOLOv11 and YOLOv8.\",\n        \"  - Emotion recognition adds valuable insights (e.g., happy, sad).\",\n        \"  - Interactive app makes it easy to test photos.\",\n        \"  - Kaggle provides free GPU resources.\",\n        \"Disadvantages:\",\n        \"  - YOLOv11 may detect false positives in crowded scenes (e.g., low confidence scores).\",\n        \"  - YOLOv8 misses some persons, reducing sensitivity.\",\n        \"  - Emotion detection can be inconsistent due to bounding box variations.\",\n        \"  - Kaggle sessions may timeout, requiring restarts.\"\n    ]\n)\n\n# Slide 6: Used Codes\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Used Codes\",\n    [\n        \"Step 4: Deploy the Streamlit App\",\n        \"  - Installs dependencies (streamlit, ultralytics, deepface, etc.).\",\n        \"  - Creates app.py with object and emotion detection logic.\",\n        \"  - Starts Streamlit and creates an ngrok tunnel.\",\n        \"Step 3.1: Compare YOLOv8 and YOLOv11\",\n        \"  - Summarizes test results and compares detection accuracy and emotions.\",\n        \"Example (from app.py):\",\n        \"  results_v11 = yolo11_model(image_rgb)\",\n        \"  faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == 'person']\",\n        \"  emotion = DeepFace.analyze(face_rgb, actions=['emotion'])\"\n    ],\n    font_size=16\n)\n\n# Slide 7: Inputs and Outputs\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Inputs and Outputs\",\n    [\n        \"Inputs:\",\n        \"  - Photos uploaded via the Streamlit app (JPG, JPEG, PNG formats).\",\n        \"  - Tested with 4 photos: 1 single person, 1 with 2 persons, 2 with a group of 15 persons.\",\n        \"Outputs:\",\n        \"  - YOLOv11 and YOLOv8 detections: Bounding boxes around persons with confidence scores.\",\n        \"  - Number of persons detected (e.g., 15 by YOLOv11, 13 by YOLOv8 in group photos).\",\n        \"  - Emotions for each person (e.g., happy, sad, fear, angry, neutral).\",\n        \"  - Annotated images showing detections.\"\n    ]\n)\n\n# Slide 8: Database Used\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Database Used\",\n    [\n        \"No external dataset was used for training.\",\n        \"Pre-trained Models:\",\n        \"  - YOLOv11 (yolo11n.pt): Pre-trained on COCO dataset for object detection.\",\n        \"  - YOLOv8 (yolov8n.pt): Pre-trained on COCO dataset for object detection.\",\n        \"  - DeepFace: Uses pre-trained models for emotion recognition (e.g., VGG-Face).\",\n        \"Test Data:\",\n        \"  - 4 user-uploaded photos for testing the app.\"\n    ]\n)\n\n# Slide 9: Results Summary\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Results Summary\",\n    [\n        \"Photo 1 (1 Person):\",\n        \"  - YOLOv11 & YOLOv8: 1 person (confidence 0.89), Emotion: Happy.\",\n        \"Photo 2 (2 Persons):\",\n        \"  - YOLOv11: 2 persons (confidences 0.91, 0.89), Emotions: Happy, Happy.\",\n        \"  - YOLOv8: 2 persons (confidences 0.89, 0.83), Emotions: Happy, Happy.\",\n        \"Photo 3 & 4 (Group of 15 Persons):\",\n        \"  - YOLOv11: 15 persons, Emotions: 8 Happy, 4 Sad, 1 Fear, 1 Angry, 1 Neutral.\",\n        \"  - YOLOv8: 13 persons, Emotions: 8 Happy, 2 Sad, 2 Fear, 1 Neutral.\",\n        \"Comparison:\",\n        \"  - YOLOv11 detects more persons but may include false positives.\",\n        \"  - YOLOv8 is more conservative, missing some persons.\",\n        \"  - Emotion detection varies due to bounding box differences.\"\n    ],\n    font_size=16\n)\n\n# Slide 10: Conclusion\nadd_bullet_slide(\n    bullet_slide_layout,\n    \"Conclusion\",\n    [\n        \"The project successfully compared YOLOv11 and YOLOv8 for object detection and emotion recognition.\",\n        \"YOLOv11 is more sensitive, detecting more persons, but risks false positives.\",\n        \"YOLOv8 is more conservative, potentially more reliable in some cases.\",\n        \"Future Improvements:\",\n        \"  - Use a face detection model (e.g., MTCNN) for better bounding boxes.\",\n        \"  - Adjust confidence thresholds to balance sensitivity and precision.\",\n        \"  - Display emotions on the annotated images for better visualization.\"\n    ]\n)\n\n# Save the presentation\npresentation_path = \"/kaggle/working/project_presentation.pptx\"\nprs.save(presentation_path)\nprint(f\"Presentation saved to {presentation_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code to Check the Confidence Threshold Used by YOLO Models\nimport cv2  # Add missing import for OpenCV\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 and YOLOv11 models\nyolo11_model = YOLO(\"yolo11n.pt\")\nyolo8_model = YOLO(\"yolov8n.pt\")\n\n# Check the default confidence threshold\nprint(\"YOLOv11 Default Confidence Threshold:\", yolo11_model.model.args.get(\"conf\", 0.25))\nprint(\"YOLOv8 Default Confidence Threshold:\", yolo8_model.model.args.get(\"conf\", 0.25))\n\n# Run a sample prediction and print the confidence scores of detections\n# Replace with a valid image path in your Kaggle environment\nimage_path = \"/kaggle/input/sample-image.jpg\"  # Example: Replace with your actual image path\nimage = cv2.imread(image_path)\n\n# Check if the image was loaded successfully\nif image is None:\n    print(f\"Error: Could not load image at {image_path}. Please check the path and ensure the image exists.\")\nelse:\n    # Run inference with YOLOv11\n    results_v11 = yolo11_model(image)\n    print(\"\\nYOLOv11 Confidence Scores:\")\n    for result in results_v11:\n        for box in result.boxes:\n            confidence = box.conf.cpu().numpy()\n            label = result.names[int(box.cls)]\n            print(f\"Detected {label} with confidence: {confidence:.2f}\")\n\n    # Run inference with YOLOv8\n    results_v8 = yolo8_model(image)\n    print(\"\\nYOLOv8 Confidence Scores:\")\n    for result in results_v8:\n        for box in result.boxes:\n            confidence = box.conf.cpu().numpy()\n            label = result.names[int(box.cls)]\n            print(f\"Detected {label} with confidence: {confidence:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}