{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11079563,"sourceType":"datasetVersion","datasetId":6905565}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shehabahmed74/graduation-project-2-final-yolo-11-vs-yolo-8-fa?scriptVersionId=232807555\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Graduation Project 2: YOLOv8 vs. YOLOv11 Facial Recognition Demo\n\nThis notebook implements a Streamlit app to compare YOLOv8 and YOLOv11 for facial recognition. The project includes the following steps:\n- **Step 1:** Install dependencies and verify GPU availability.\n- **Step 2:** Fine-tune YOLOv11 on a facial recognition dataset.\n- **Step 3:** Set up and deploy a Streamlit app to perform facial recognition and compare YOLOv8 and YOLOv11.\n- **Step 4:** Test the app and verify the results.\n- **Step 5:** Summarize the results and provide the public URL for the app.\n\nThe dataset used is `shehabahmed74/shehab-data-facial-recognition`, which contains three folders: `train`, `valid`, and `test`, each with `images` and `labels` subfolders. We will use `train` for training, `valid` for validation, and `test` for testing in the Streamlit app.\n\nThe notebook is structured to be professional and compatible with GitHub for project delivery.\n\n**Execution Time Estimate:**\n- With GPU: ~15–25 minutes (fine-tuning takes ~10–20 minutes for 10 epochs).\n- Without GPU (on CPU): ~30–60 minutes (fine-tuning takes ~20–40 minutes for 10 epochs).\n- To optimize performance, ensure a GPU runtime is selected (see Step 1.2).\n\n---\n\n## Step 1: Install Dependencies\n\nThis cell installs the required Python packages for the project, including Streamlit, ngrok, Ultralytics (for YOLOv11), and other dependencies.\n\n**Instructions:**\n- Run this cell to install the dependencies.\n- If you encounter version conflicts, you may need to restart the runtime (`Runtime > Restart runtime`) and rerun this cell.","metadata":{"id":"KHc6q_w3r1sb"}},{"cell_type":"code","source":"# Install dependencies\n!pip install streamlit -q\n!pip install pyngrok -q\n!pip install ultralytics seaborn matplotlib pandas scipy kaggle pillow -q\nprint(\"Dependencies installed successfully!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-mBpGpkr5bC","outputId":"63a93e30-6b2a-48cb-c7bc-ae7b1f49d41a","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:37:58.915986Z","iopub.execute_input":"2025-04-09T07:37:58.916255Z","iopub.status.idle":"2025-04-09T07:38:13.181828Z","shell.execute_reply.started":"2025-04-09T07:37:58.916233Z","shell.execute_reply":"2025-04-09T07:38:13.180715Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDependencies installed successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Upgrade pyngrok to the latest version using subprocess\nimport subprocess\nimport os\n\n# Set the environment for the subprocess\nenv = os.environ.copy()\nenv[\"LC_ALL\"] = \"C.UTF-8\"\nenv[\"LANG\"] = \"C.UTF-8\"\nenv[\"LANGUAGE\"] = \"C.UTF-8\"\n\nprint(\"Upgrading pyngrok...\")\ntry:\n    # Use subprocess to run the pip install command\n    process = subprocess.run(\n        [\"pip\", \"install\", \"--upgrade\", \"pyngrok\", \"-q\"],\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True\n    )\n    # Check if the command was successful\n    if process.returncode == 0:\n        print(\"pyngrok upgraded successfully!\")\n    else:\n        print(\"Failed to upgrade pyngrok. Error output:\")\n        print(process.stderr)\n        raise RuntimeError(\"pyngrok upgrade failed.\")\nexcept Exception as e:\n    print(f\"Error upgrading pyngrok: {str(e)}\")\n    raise","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlaJyjtFb2YK","outputId":"82bfe61d-8a81-4e7a-ff04-515ce06f991a","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:13.183309Z","iopub.execute_input":"2025-04-09T07:38:13.183578Z","iopub.status.idle":"2025-04-09T07:38:16.532339Z","shell.execute_reply.started":"2025-04-09T07:38:13.183554Z","shell.execute_reply":"2025-04-09T07:38:16.531457Z"}},"outputs":[{"name":"stdout","text":"Upgrading pyngrok...\npyngrok upgraded successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install dependencies\n!pip install streamlit pyngrok ultralytics kaggle","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLbT3ayZ4_m7","outputId":"efafadd2-90d0-43a1-9a2a-84bcd0acd4b5","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:16.536762Z","iopub.execute_input":"2025-04-09T07:38:16.537011Z","iopub.status.idle":"2025-04-09T07:38:20.193875Z","shell.execute_reply.started":"2025-04-09T07:38:16.536991Z","shell.execute_reply":"2025-04-09T07:38:20.193024Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.44.1)\nRequirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.3)\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.105)\nRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\nRequirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\nRequirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2025.1.31)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.3.0)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 1.1: Verify Installed Versions\n\nThis cell checks the installed versions of key packages to ensure compatibility.\n\n**Instructions:**\n- Run this cell to verify the versions of NumPy and Ultralytics.\n- Expected versions (approximate):\n  - NumPy: 2.1.x\n  - Ultralytics: 8.3.x\n- If there are compatibility issues, you may need to pin specific versions in the previous cell (e.g., `numpy==1.26.4 ultralytics==8.3.92`).","metadata":{"id":"KzF5bm-Lr7cK"}},{"cell_type":"code","source":"# Check installed versions\nimport numpy\nimport ultralytics\nprint(f\"NumPy version: {numpy.__version__}\")\nprint(f\"Ultralytics version: {ultralytics.__version__}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayaY456ar-Za","outputId":"8718a40f-85c5-4c3b-e1c2-3dde9a57eb67","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:20.194987Z","iopub.execute_input":"2025-04-09T07:38:20.195341Z","iopub.status.idle":"2025-04-09T07:38:24.361389Z","shell.execute_reply.started":"2025-04-09T07:38:20.195303Z","shell.execute_reply":"2025-04-09T07:38:24.360423Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nNumPy version: 1.26.4\nUltralytics version: 8.3.105\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 1.2: Verify GPU Availability\n\nThis cell checks if a GPU is available, as fine-tuning YOLOv11 is significantly faster on a GPU.\n\n**Instructions:**\n- Run this cell to verify GPU availability.\n- If the output shows `CUDA available: True`, a GPU is available, and you can proceed.\n- If the output shows `CUDA available: False`, switch to a GPU runtime:\n  - Go to `Runtime > Change runtime type`.\n  - Select `GPU` under \"Hardware accelerator\" (e.g., T4 GPU).\n  - Click `Save` and restart the runtime (`Runtime > Restart runtime`).\n  - Rerun all cells above this one.\n- **Note:** If a GPU is not available, the notebook will fall back to CPU, but fine-tuning will be slower (~20–40 minutes for 10 epochs vs. ~10–20 minutes on a GPU).","metadata":{"id":"6pukWCJOsAWi"}},{"cell_type":"code","source":"# Verify GPU availability\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: No GPU detected. Fine-tuning will be slower on CPU. See instructions above to switch to a GPU runtime.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Il5InHDsCE7","outputId":"6629d2a3-6c9f-45a1-f35f-7ec825876424","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:24.362774Z","iopub.execute_input":"2025-04-09T07:38:24.363197Z","iopub.status.idle":"2025-04-09T07:38:24.439556Z","shell.execute_reply.started":"2025-04-09T07:38:24.363174Z","shell.execute_reply":"2025-04-09T07:38:24.43862Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 1\nGPU Name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Step 1.3: Set Up ngrok Authtoken\n\nThis cell sets up ngrok to create a public URL for the Streamlit app.\n\n**Instructions:**\n- The ngrok authtoken is already provided.\n- Run this cell to set up ngrok.\n- If you encounter issues with the authtoken, ensure it is valid by checking your ngrok dashboard at [ngrok.com](https://ngrok.com).","metadata":{"id":"RfR9DrI4sDsC"}},{"cell_type":"code","source":"# Set up ngrok authtoken\nfrom pyngrok import ngrok\n\n# Set the ngrok authtoken\nngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\nprint(\"ngrok authtoken set up successfully!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"418wTmFisFPC","outputId":"1e4ca1fb-f20a-41b5-a1bf-ae5e909a6be0","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:24.44039Z","iopub.execute_input":"2025-04-09T07:38:24.440987Z","iopub.status.idle":"2025-04-09T07:38:24.978274Z","shell.execute_reply.started":"2025-04-09T07:38:24.440963Z","shell.execute_reply":"2025-04-09T07:38:24.977331Z"}},"outputs":[{"name":"stdout","text":"ngrok authtoken set up successfully!                                                                \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 2: Fine-Tune YOLOv11 for Facial Recognition\n\nThis step fine-tunes the YOLOv11 nano model (`yolo11n.pt`) on the facial recognition dataset (`shehabahmed74/shehab-data-facial-recognition`). The fine-tuned model will be saved as `yolo11n_finetuned.pt` and used in the Streamlit app for more specific facial recognition labels (e.g., \"face\" instead of \"person\").\n\nThe dataset contains three folders: `train`, `valid`, and `test`. We will use:\n- `train` for training.\n- `valid` for validation.\n- `test` for testing in the Streamlit app.\n\nThe dataset paths, number of classes (`nc`), and class names (`names`) are pre-configured based on prior testing. We assume:\n- There is one class (`face`), so `nc: 1` and `names: ['face']`.\n\n**This cell will:**\n- Download the dataset (`shehabahmed74/shehab-data-facial-recognition`) using `kagglehub`.\n- Unzip the dataset and verify its structure.\n- Dynamically set the `train` and `val` paths in `data.yaml` based on the actual dataset structure.\n- Fine-tune the YOLOv11 model.\n\n**Instructions:**\n- Run the next cell (Cell 10) to download the dataset and fine-tune the model.\n- This step takes the longest:\n  - With GPU: ~10–20 minutes for 10 epochs.\n  - Without GPU (on CPU): ~20–40 minutes for 10 epochs.\n- If you encounter errors (e.g., missing dataset paths), the dataset structure may differ from the expected layout. The cell will print the dataset structure to help you debug. You can also manually check the structure by adding a new code cell with:\n  ```python\n  !ls -R /root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1","metadata":{"id":"xtWeveSssH2C"}},{"cell_type":"code","source":"","metadata":{"id":"JFk7vSFGn0hl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 2: Fine-tune YOLOv11\n# from ultralytics import YOLO\n# import os\n# import torch\n# import kagglehub\n# import zipfile\n# import glob\n# import shutil\n\n# # Function to print directory structure using Python\n# def print_directory_structure(directory, indent=0, max_files=5):\n#     \"\"\"Recursively print the directory structure with a limit on the number of files displayed per directory.\"\"\"\n#     try:\n#         print(\"  \" * indent + f\"Directory: {directory}\")\n#         items = sorted(os.listdir(directory))\n#         for item in items:\n#             item_path = os.path.join(directory, item)\n#             if os.path.isdir(item_path):\n#                 print(\"  \" * (indent + 1) + f\"Subdirectory: {item}\")\n#                 print_directory_structure(item_path, indent + 2, max_files)\n#             else:\n#                 if max_files > 0:\n#                     print(\"  \" * (indent + 1) + f\"File: {item}\")\n#                     max_files -= 1\n#                 elif max_files == 0:\n#                     print(\"  \" * (indent + 1) + \"... (additional files omitted)\")\n#                     break\n#     except Exception as e:\n#         print(\"  \" * (indent + 1) + f\"Error accessing directory: {str(e)}\")\n\n# # Function to download and process dataset\n# def download_and_process_dataset():\n#     print(\"Downloading and Processing Dataset...\")\n#     try:\n#         # Download dataset\n#         dataset_path = kagglehub.dataset_download(\"shehabahmed74/shehab-data-facial-recognition\")\n#         print(f\"Dataset downloaded to: {dataset_path}\")\n\n#         # Process dataset (unzip if necessary)\n#         if os.path.isdir(dataset_path):\n#             processed_path = dataset_path\n#         elif os.path.isfile(dataset_path) and dataset_path.endswith('.zip'):\n#             unzip_dir = os.path.join(os.path.dirname(dataset_path), \"unzipped_dataset\")\n#             os.makedirs(unzip_dir, exist_ok=True)\n#             with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n#                 zip_ref.extractall(unzip_dir)\n#             processed_path = unzip_dir\n#         else:\n#             raise ValueError(\"Dataset is neither a recognized zip file nor a usable directory.\")\n\n#         print(\"Dataset processing completed!\")\n#         return processed_path\n#     except Exception as e:\n#         print(f\"Error downloading or processing dataset: {str(e)}\")\n#         raise\n\n# # Download and process the dataset\n# dataset_path = download_and_process_dataset()\n\n# # Verify the dataset structure using Python\n# print(\"\\nDataset structure:\")\n# print_directory_structure(dataset_path)\n\n# # Search for image files in the dataset\n# print(\"\\nSearching for image files in the dataset...\")\n# image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n# image_files = []\n# for ext in image_extensions:\n#     image_files.extend(glob.glob(os.path.join(dataset_path, \"**\", ext), recursive=True))\n\n# if not image_files:\n#     raise FileNotFoundError(\"No image files found in the dataset. Check the dataset structure above.\")\n# else:\n#     print(f\"Found {len(image_files)} image files. Examples:\")\n#     for img in image_files[:5]:  # Print first 5 image paths\n#         print(img)\n\n# # Search for label files in the dataset\n# print(\"\\nSearching for label files in the dataset...\")\n# label_files = glob.glob(os.path.join(dataset_path, \"**\", \"*.txt\"), recursive=True)\n# if not label_files:\n#     print(\"Warning: No label files found in the dataset. Check the dataset structure above.\")\n# else:\n#     print(f\"Found {len(label_files)} label files. Examples:\")\n#     for lbl in label_files[:5]:  # Print first 5 label paths\n#         print(lbl)\n\n# # Create a dictionary mapping image filenames (without extension) to their paths\n# image_dict = {os.path.splitext(os.path.basename(img))[0]: img for img in image_files}\n# label_dict = {os.path.splitext(os.path.basename(lbl))[0]: lbl for lbl in label_files}\n\n# # Find images that have corresponding labels\n# paired_files = []\n# for img_name in image_dict.keys():\n#     if img_name in label_dict:\n#         paired_files.append((image_dict[img_name], label_dict[img_name]))\n\n# if not paired_files:\n#     raise ValueError(\"No images have corresponding label files. Check the dataset structure.\")\n\n# print(f\"\\nFound {len(paired_files)} image-label pairs.\")\n\n# # Split the paired files into training and validation sets (80% train, 20% val)\n# paired_files.sort()  # Sort to ensure consistent splitting\n# split_idx = int(0.8 * len(paired_files))\n# train_pairs = paired_files[:split_idx]\n# val_pairs = paired_files[split_idx:]\n\n# # Create directories for training and validation\n# train_images_dir = os.path.join(dataset_path, \"custom_train\", \"images\")\n# train_labels_dir = os.path.join(dataset_path, \"custom_train\", \"labels\")\n# val_images_dir = os.path.join(dataset_path, \"custom_valid\", \"images\")\n# val_labels_dir = os.path.join(dataset_path, \"custom_valid\", \"labels\")\n\n# # Remove existing directories to avoid conflicts\n# print(\"\\nRemoving existing custom_train and custom_valid directories (if any)...\")\n# for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n#     if os.path.exists(dir_path):\n#         shutil.rmtree(dir_path)\n\n# # Recreate the directories\n# os.makedirs(train_images_dir, exist_ok=True)\n# os.makedirs(train_labels_dir, exist_ok=True)\n# os.makedirs(val_images_dir, exist_ok=True)\n# os.makedirs(val_labels_dir, exist_ok=True)\n\n# # Copy images and labels to the respective directories (symbolic links to save space)\n# print(\"\\nOrganizing images and labels into train and valid directories...\")\n# for img_path, lbl_path in train_pairs:\n#     img_name = os.path.basename(img_path)\n#     lbl_name = os.path.basename(lbl_path)\n#     img_symlink = os.path.join(train_images_dir, img_name)\n#     lbl_symlink = os.path.join(train_labels_dir, lbl_name)\n#     # Create symbolic links, overwriting if they exist\n#     if os.path.exists(img_symlink):\n#         os.remove(img_symlink)\n#     if os.path.exists(lbl_symlink):\n#         os.remove(lbl_symlink)\n#     os.symlink(img_path, img_symlink)\n#     os.symlink(lbl_path, lbl_symlink)\n\n# for img_path, lbl_path in val_pairs:\n#     img_name = os.path.basename(img_path)\n#     lbl_name = os.path.basename(lbl_path)\n#     img_symlink = os.path.join(val_images_dir, img_name)\n#     lbl_symlink = os.path.join(val_labels_dir, lbl_name)\n#     # Create symbolic links, overwriting if they exist\n#     if os.path.exists(img_symlink):\n#         os.remove(img_symlink)\n#     if os.path.exists(lbl_symlink):\n#         os.remove(lbl_symlink)\n#     os.symlink(img_path, img_symlink)\n#     os.symlink(lbl_path, lbl_symlink)\n\n# # Define the paths for data.yaml\n# train_images_path = train_images_dir\n# val_images_path = val_images_dir\n\n# # Verify the new directories\n# print(f\"\\nNew training images path: {train_images_path}\")\n# print(f\"Number of training images: {len(glob.glob(os.path.join(train_images_path, '*')))}\")\n# print(f\"New training labels path: {train_labels_dir}\")\n# print(f\"Number of training labels: {len(glob.glob(os.path.join(train_labels_dir, '*')))}\")\n# print(f\"New validation images path: {val_images_path}\")\n# print(f\"Number of validation images: {len(glob.glob(os.path.join(val_images_path, '*')))}\")\n# print(f\"New validation labels path: {val_labels_dir}\")\n# print(f\"Number of validation labels: {len(glob.glob(os.path.join(val_labels_dir, '*')))}\")\n\n# # Check if the paths exist and are not empty\n# if not os.path.exists(train_images_path) or not glob.glob(os.path.join(train_images_path, '*')):\n#     raise FileNotFoundError(f\"Training images path is empty or not found: {train_images_path}\")\n# if not os.path.exists(val_images_path) or not glob.glob(os.path.join(val_images_path, '*')):\n#     raise FileNotFoundError(f\"Validation images path is empty or not found: {val_images_path}\")\n# if not os.path.exists(train_labels_dir) or not glob.glob(os.path.join(train_labels_dir, '*')):\n#     raise FileNotFoundError(f\"Training labels path is empty or not found: {train_labels_dir}\")\n# if not os.path.exists(val_labels_dir) or not glob.glob(os.path.join(val_labels_dir, '*')):\n#     raise FileNotFoundError(f\"Validation labels path is empty or not found: {val_labels_dir}\")\n\n# # Load the pre-trained YOLOv11 nano model\n# model = YOLO(\"yolo11n.pt\")\n\n# # Define the data.yaml with the correct paths\n# data_yaml = f\"\"\"\n# train: {train_images_path}\n# val: {val_images_path}\n# nc: 1  # Number of classes (assumed to be 1 for 'face')\n# names: ['face']  # Class names (assumed to be 'face')\n# \"\"\"\n\n# # Save the data.yaml file\n# with open(\"data.yaml\", \"w\") as f:\n#     f.write(data_yaml)\n\n# # Determine the device (GPU if available, else CPU)\n# device = 0 if torch.cuda.is_available() else 'cpu'\n# print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n\n# # Fine-tune the model\n# try:\n#     model.train(\n#         data=\"data.yaml\",\n#         epochs=10,  # Use fewer epochs for a quick test; increase for better results\n#         imgsz=640,\n#         batch=16,\n#         device=device  # Use GPU if available, else CPU\n#     )\n# except Exception as e:\n#     print(f\"Error during fine-tuning: {str(e)}\")\n#     raise\n\n# # Save the fine-tuned model\n# model.save(\"yolo11n_finetuned.pt\")\n# print(\"Fine-tuning complete! Model saved as 'yolo11n_finetuned.pt'\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loUaDgbm5YU8","outputId":"c81a2db5-9c05-4579-dc9e-58938e6d83a9","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:24.981022Z","iopub.execute_input":"2025-04-09T07:38:24.981291Z","iopub.status.idle":"2025-04-09T07:38:24.98678Z","shell.execute_reply.started":"2025-04-09T07:38:24.981268Z","shell.execute_reply":"2025-04-09T07:38:24.985957Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"### Cell: Step 2\nimport os\nimport shutil\nimport random\n\n# Step 1: Copy dataset to writable directory\nsource_dataset_dir = \"/kaggle/input/shehab-data-facial-recognition\"\nworking_dataset_dir = \"/kaggle/working/shehab-data-facial-recognition\"\nif os.path.exists(working_dataset_dir):\n    shutil.rmtree(working_dataset_dir)\nshutil.copytree(source_dataset_dir, working_dataset_dir)\nprint(\"Step 1: Dataset copied to writable directory\")\n\n# Step 2: Define directory paths\ntrain_images_dir = os.path.join(working_dataset_dir, \"custom_train\", \"images\")\ntrain_labels_dir = os.path.join(working_dataset_dir, \"custom_train\", \"labels\")\nval_images_dir = os.path.join(working_dataset_dir, \"custom_val\", \"images\")\nval_labels_dir = os.path.join(working_dataset_dir, \"custom_val\", \"labels\")\nos.makedirs(train_images_dir, exist_ok=True)\nos.makedirs(train_labels_dir, exist_ok=True)\nos.makedirs(val_images_dir, exist_ok=True)\nos.makedirs(val_labels_dir, exist_ok=True)\nprint(\"Step 2: Directory paths defined\")\n\n# Step 3: Find images and labels\nimage_extensions = (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\")\nall_images = []\nall_labels = []\nfor root, dirs, files in os.walk(working_dataset_dir):\n    if \"custom_train\" in root or \"custom_val\" in root:\n        continue\n    for file in files:\n        if file.lower().endswith(image_extensions):\n            all_images.append(os.path.join(root, file))\n        elif file.endswith(\".txt\"):\n            all_labels.append(os.path.join(root, file))\nprint(f\"Step 3: Found {len(all_images)} images, {len(all_labels)} labels\")\n\n# Step 4: Pair images with labels and deduplicate\nimage_label_pairs = []\nseen_names = set()\nfor image_path in all_images:\n    image_name = os.path.splitext(os.path.basename(image_path))[0]\n    if image_name in seen_names:\n        continue  # Skip duplicates\n    for label_path in all_labels:\n        label_name = os.path.splitext(os.path.basename(label_path))[0]\n        if image_name == label_name and os.path.exists(image_path) and os.path.exists(label_path):\n            image_label_pairs.append((image_path, label_path))\n            seen_names.add(image_name)\n            break\nprint(f\"Step 4: Found {len(image_label_pairs)} image-label pairs after deduplication\")\n\n# Step 5: Split into train and validation sets\nrandom.shuffle(image_label_pairs)\nsplit_idx = int(0.8 * len(image_label_pairs))\ntrain_pairs = image_label_pairs[:split_idx]\nval_pairs = image_label_pairs[split_idx:]\nprint(f\"Step 5: Split into {len(train_pairs)} training pairs, {len(val_pairs)} validation pairs\")\n\n# Step 6: Move files to training directories\ntrain_images_moved = 0\ntrain_labels_moved = 0\ntrain_skipped = 0\nfor image_path, label_path in train_pairs:\n    if os.path.exists(image_path) and os.path.exists(label_path):\n        shutil.move(image_path, os.path.join(train_images_dir, os.path.basename(image_path)))\n        shutil.move(label_path, os.path.join(train_labels_dir, os.path.basename(label_path)))\n        train_images_moved += 1\n        train_labels_moved += 1\n    else:\n        train_skipped += 1\nprint(f\"Step 6: Moved {train_images_moved} images, {train_labels_moved} labels to training directories, skipped {train_skipped} pairs\")\n\n# Step 7: Move files to validation directories\nval_images_moved = 0\nval_labels_moved = 0\nval_skipped = 0\nfor image_path, label_path in val_pairs:\n    if os.path.exists(image_path) and os.path.exists(label_path):\n        shutil.move(image_path, os.path.join(val_images_dir, os.path.basename(image_path)))\n        shutil.move(label_path, os.path.join(val_labels_dir, os.path.basename(label_path)))\n        val_images_moved += 1\n        val_labels_moved += 1\n    else:\n        val_skipped += 1\nprint(f\"Step 7: Moved {val_images_moved} images, {val_labels_moved} labels to validation directories, skipped {val_skipped} pairs\")\n\n# Step 8: Verify pairing\ntrain_images = os.listdir(train_images_dir)\ntrain_labels = os.listdir(train_labels_dir)\nval_images = os.listdir(val_images_dir)\nval_labels = os.listdir(val_labels_dir)\ntrain_pairs_verified = sum(1 for img in train_images if f\"{os.path.splitext(img)[0]}.txt\" in train_labels)\nval_pairs_verified = sum(1 for img in val_images if f\"{os.path.splitext(img)[0]}.txt\" in val_labels)\nprint(f\"Step 8: Verified {train_pairs_verified} pairs in custom_train, {val_pairs_verified} pairs in custom_val\")","metadata":{"id":"2N3CX4eG5ZTU","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:38:24.988075Z","iopub.execute_input":"2025-04-09T07:38:24.988296Z","iopub.status.idle":"2025-04-09T07:49:15.710625Z","shell.execute_reply.started":"2025-04-09T07:38:24.988276Z","shell.execute_reply":"2025-04-09T07:49:15.709799Z"}},"outputs":[{"name":"stdout","text":"Step 1: Dataset copied to writable directory\nStep 2: Directory paths defined\nStep 3: Found 25262 images, 25262 labels\nStep 4: Found 23129 image-label pairs after deduplication\nStep 5: Split into 18503 training pairs, 4626 validation pairs\nStep 6: Moved 18503 images, 18503 labels to training directories, skipped 0 pairs\nStep 7: Moved 4626 images, 4626 labels to validation directories, skipped 0 pairs\nStep 8: Verified 18503 pairs in custom_train, 4626 pairs in custom_val\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# # Step 3: Create the Streamlit app\n# import os\n\n# # Define the Streamlit app code\n# streamlit_app_code = \"\"\"\n# import streamlit as st\n# import os\n# import glob\n# import random\n# from PIL import Image\n# from ultralytics import YOLO\n\n# # Load the fine-tuned YOLOv11 model\n# @st.cache_resource\n# def load_model():\n#     return YOLO(\"yolo11n_finetuned.pt\")\n\n# model = load_model()\n\n# # Streamlit app\n# st.title(\"YOLOv11 Facial Recognition Demo\")\n\n# # Step 1: Dataset Path Verification\n# st.header(\"Step 1: Dataset Path Verification\")\n# dataset_path = st.text_input(\"Enter the dataset path from Step 2:\", value=\"/root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1\")\n# if dataset_path:\n#     # Check for 'test' directory\n#     test_path = os.path.join(dataset_path, \"test\")\n#     if not os.path.exists(test_path):\n#         st.error(f\"'test' directory not found: {test_path}. Ensure Step 2 completed successfully and enter the correct path.\")\n#         test_images = []\n#     else:\n#         # Initialize test images list\n#         test_images = []\n#         # Check for nested 'test/test/images' directory (where images are located)\n#         test_images_path = os.path.join(test_path, \"test\", \"images\")\n#         st.write(f\"Checking for images in: {test_images_path}\")\n#         if os.path.exists(test_images_path):\n#             # Look for images with any case variation of .jpg, .png, .jpeg\n#             test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n#                           glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n#                           glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n#             st.write(f\"Found {len(test_images)} images in 'test/test/images'\")\n#             if test_images:\n#                 st.success(f\"Test images directory found: {test_images_path}\")\n#                 st.write(f\"Found {len(test_images)} test images.\")\n#                 st.write(f\"Sample images: {test_images[:5]}\")\n#                 # Verify that the first image is readable\n#                 try:\n#                     with open(test_images[0], \"rb\") as f:\n#                         f.read(1)\n#                     st.write(f\"Successfully read first image: {test_images[0]}\")\n#                 except Exception as e:\n#                     st.error(f\"Error reading image {test_images[0]}: {str(e)}\")\n#                     test_images = []\n#             else:\n#                 st.warning(f\"Test images directory exists but no .jpg, .png, or .jpeg images found in: {test_images_path}\")\n#                 st.write(f\"Files in 'test/test/images': {os.listdir(test_images_path)[:5]}\")\n#         else:\n#             st.warning(f\"'test/test/images' directory not found: {test_images_path}\")\n#             st.write(f\"Contents of 'test': {os.listdir(test_path)[:5]}\")\n#             # Fallback: Check for images directly in 'test/test'\n#             test_images_path = os.path.join(test_path, \"test\")\n#             st.write(f\"Checking for images directly in: {test_images_path}\")\n#             if os.path.exists(test_images_path):\n#                 test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n#                               glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n#                               glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n#                 st.write(f\"Found {len(test_images)} images directly in 'test/test'\")\n#                 if test_images:\n#                     st.success(f\"Test images found in nested 'test/test' directory: {test_images_path}\")\n#                     st.write(f\"Found {len(test_images)} test images.\")\n#                     st.write(f\"Sample images: {test_images[:5]}\")\n#                 else:\n#                     st.warning(f\"No .jpg, .png, or .jpeg images found in {test_images_path}\")\n#                     st.write(f\"Files in 'test/test': {os.listdir(test_images_path)[:5]}\")\n#             else:\n#                 st.warning(f\"'test/test' directory not found: {test_images_path}\")\n\n#         # Final check: If no images are found, notify the user\n#         if not test_images:\n#             st.info(\"No test images found. You can still upload an external image for inference below.\")\n\n# # Step 2: Test YOLOv11 Facial Recognition\n# st.header(\"Step 2: Test YOLOv11 Facial Recognition\")\n\n# # Option to select image source\n# image_source_options = [\"Use a random image from the test dataset\", \"Upload an external image\"]\n# image_source = st.radio(\n#     \"Select image source for facial recognition:\",\n#     image_source_options,\n#     index=0 if test_images else 1  # Default to random if test images exist, otherwise upload\n# )\n\n# # Disable the random image option if no test images are found\n# if not test_images and image_source == \"Use a random image from the test dataset\":\n#     st.warning(\"No test images found. Please select 'Upload an external image' instead.\")\n#     image_source = \"Upload an external image\"\n\n# # Initialize image_path\n# image_path = None\n\n# # Handle random image selection\n# if image_source == \"Use a random image from the test dataset\" and test_images:\n#     image_path = random.choice(test_images)\n#     st.write(f\"Selected random image: {os.path.basename(image_path)}\")\n\n# # Always show the file uploader for external images\n# if image_source == \"Upload an external image\":\n#     uploaded_file = st.file_uploader(\"Upload an external image for facial recognition\", type=[\"jpg\", \"jpeg\", \"png\"])\n#     if uploaded_file is not None:\n#         # Save the uploaded file temporarily\n#         image_path = os.path.join(\"temp\", uploaded_file.name)\n#         st.write(f\"Saving uploaded file to: {image_path}\")\n#         try:\n#             os.makedirs(\"temp\", exist_ok=True)\n#             with open(image_path, \"wb\") as f:\n#                 f.write(uploaded_file.getbuffer())\n#             st.write(f\"Uploaded image: {uploaded_file.name}\")\n#             # Verify the file was saved\n#             if os.path.exists(image_path):\n#                 st.write(f\"File successfully saved at: {image_path}\")\n#             else:\n#                 st.error(f\"Failed to save the uploaded file at: {image_path}\")\n#                 image_path = None\n#         except Exception as e:\n#             st.error(f\"Error saving uploaded file: {str(e)}\")\n#             image_path = None\n#     else:\n#         st.info(\"Please upload an image to proceed.\")\n\n# # Run inference if an image is selected\n# if st.button(\"Run Inference\"):\n#     if image_path:\n#         # Load and display the image\n#         try:\n#             image = Image.open(image_path)\n#             st.image(image, caption=\"Input Image\", use_column_width=True)\n#         except Exception as e:\n#             st.error(f\"Error loading image {image_path}: {str(e)}\")\n#             image_path = None\n\n#         if image_path:\n#             # Run YOLOv11 inference\n#             with st.spinner(\"Running facial recognition...\"):\n#                 try:\n#                     results = model(image)\n#                     # Display the results\n#                     st.header(\"Step 3: Results\")\n#                     # Plot the results\n#                     annotated_image = results[0].plot()  # Get the annotated image with bounding boxes\n#                     st.image(annotated_image, caption=\"Detected Faces\", use_column_width=True)\n\n#                     # Display detection details\n#                     detections = results[0].boxes\n#                     st.write(f\"Number of faces detected: {len(detections)}\")\n#                     for i, det in enumerate(detections):\n#                         conf = det.conf.item()\n#                         bbox = det.xyxy[0].tolist()\n#                         st.write(f\"Face {i+1}: Confidence = {conf:.2f}, Bounding Box = {bbox}\")\n#                 except Exception as e:\n#                     st.error(f\"Error during inference: {str(e)}\")\n#     else:\n#         st.error(\"Please select a random test image or upload an external image to run inference.\")\n\n# # Step 4: Summary and Conclusion\n# st.header(\"Step 4: Summary and Conclusion\")\n# st.write(\"This demo showcases the YOLOv11 model fine-tuned for facial recognition.\")\n# st.write(\"The model was fine-tuned on the dataset provided in Step 2 and tested on either a random test image or an uploaded external image.\")\n# st.write(\"For detailed performance metrics and comparisons, refer to the notebook (Step 3.1).\")\n# \"\"\"\n\n# # Write the Streamlit app code to app.py\n# with open(\"app.py\", \"w\") as f:\n#     f.write(streamlit_app_code)\n\n# # Verify that app.py was created\n# if os.path.exists(\"app.py\"):\n#     print(\"Streamlit app created successfully at app.py!\")\n# else:\n#     raise FileNotFoundError(\"Failed to create app.py\")","metadata":{"id":"KhDvcsFy25CU","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:49:15.711573Z","iopub.execute_input":"2025-04-09T07:49:15.711872Z","iopub.status.idle":"2025-04-09T07:49:15.718875Z","shell.execute_reply.started":"2025-04-09T07:49:15.711849Z","shell.execute_reply":"2025-04-09T07:49:15.718102Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"id":"_gbzxrty25-7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3: Fine-tune YOLOv11 model","metadata":{}},{"cell_type":"code","source":"# Step 0: Install required libraries\n!pip install ultralytics\nprint(\"Ultralytics installed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:49:15.719692Z","iopub.execute_input":"2025-04-09T07:49:15.72001Z","iopub.status.idle":"2025-04-09T07:49:19.223215Z","shell.execute_reply.started":"2025-04-09T07:49:15.71999Z","shell.execute_reply":"2025-04-09T07:49:19.221819Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.105)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nUltralytics installed successfully\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Step 0: Install required libraries\n!pip install ultralytics\nprint(\"Ultralytics installed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:49:19.224592Z","iopub.execute_input":"2025-04-09T07:49:19.224984Z","iopub.status.idle":"2025-04-09T07:49:22.805227Z","shell.execute_reply.started":"2025-04-09T07:49:19.224948Z","shell.execute_reply":"2025-04-09T07:49:22.804169Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.105)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nUltralytics installed successfully\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"To fix warning message which appeared when run cell 3","metadata":{}},{"cell_type":"code","source":"# Step 10.5: Fix label files to ensure class ID is 0\nimport os\n\n# Define directories\ntrain_labels_dir = \"/kaggle/working/shehab-data-facial-recognition/custom_train/labels\"\nval_labels_dir = \"/kaggle/working/shehab-data-facial-recognition/custom_val/labels\"\n\n# Function to fix a single label file\ndef fix_label_file(label_path):\n    try:\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n        # Fix class ID to 0\n        fixed_lines = []\n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) >= 5:  # Ensure the line has at least 5 parts (class_id, x, y, w, h)\n                parts[0] = \"0\"  # Set class ID to 0\n                fixed_lines.append(\" \".join(parts) + \"\\n\")\n        # Write the fixed lines back to the file\n        with open(label_path, \"w\") as f:\n            f.writelines(fixed_lines)\n    except Exception as e:\n        print(f\"Error fixing {label_path}: {str(e)}\")\n\n# Fix training labels\ntrain_fixed = 0\nfor label_file in os.listdir(train_labels_dir):\n    if label_file.endswith(\".txt\"):\n        fix_label_file(os.path.join(train_labels_dir, label_file))\n        train_fixed += 1\n\n# Fix validation labels\nval_fixed = 0\nfor label_file in os.listdir(val_labels_dir):\n    if label_file.endswith(\".txt\"):\n        fix_label_file(os.path.join(val_labels_dir, label_file))\n        val_fixed += 1\n\nprint(f\"Fixed {train_fixed} training labels and {val_fixed} validation labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:49:22.806318Z","iopub.execute_input":"2025-04-09T07:49:22.806669Z","iopub.status.idle":"2025-04-09T07:49:25.407978Z","shell.execute_reply.started":"2025-04-09T07:49:22.806631Z","shell.execute_reply":"2025-04-09T07:49:25.407161Z"}},"outputs":[{"name":"stdout","text":"Fixed 18503 training labels and 4626 validation labels\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Step 10: Fine-tune YOLOv11 model\nfrom ultralytics import YOLO\nimport os\n\n# Create data.yaml if it doesn't exist\ndata_yaml_path = \"/kaggle/working/shehab-data-facial-recognition/data.yaml\"\nif not os.path.exists(data_yaml_path):\n    data_yaml = \"\"\"\n    train: /kaggle/working/shehab-data-facial-recognition/custom_train/images\n    val: /kaggle/working/shehab-data-facial-recognition/custom_val/images\n    nc: 1\n    names: ['face']\n    \"\"\"\n    with open(data_yaml_path, \"w\") as f:\n        f.write(data_yaml)\n    print(\"Created data.yaml\")\n\n# Load the pre-trained YOLOv11 model\nmodel = YOLO(\"yolo11n.pt\")  # This will download the model if not present\n\n# Fine-tune the model on your dataset\nmodel.train(\n    data=data_yaml_path,\n    epochs=10,\n    imgsz=640,\n    batch=8,\n    device=0,\n    project=\"/kaggle/working/runs/train\",\n    name=\"yolo11n_finetune\"\n)\n\n# Save the fine-tuned model\nmodel.save(\"/kaggle/working/yolo11n_finetuned.pt\")\nprint(\"Fine-tuning complete. Model saved as 'yolo11n_finetuned.pt'\")","metadata":{"id":"IUQ4Qg_M09BF","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:49:25.408687Z","iopub.execute_input":"2025-04-09T07:49:25.40895Z","iopub.status.idle":"2025-04-09T08:47:08.857179Z","shell.execute_reply.started":"2025-04-09T07:49:25.408931Z","shell.execute_reply":"2025-04-09T08:47:08.856207Z"}},"outputs":[{"name":"stdout","text":"Created data.yaml\nUltralytics 8.3.105 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/kaggle/working/shehab-data-facial-recognition/data.yaml, epochs=10, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=/kaggle/working/runs/train, name=yolo11n_finetune6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/kaggle/working/runs/train/yolo11n_finetune6\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 755k/755k [00:00<00:00, 17.3MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nYOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n\nTransferred 448/499 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /kaggle/working/runs/train/yolo11n_finetune6', view at http://localhost:6006/\nFreezing layer 'model.23.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/shehab-data-facial-recognition/custom_train/labels... 18503 images, 0 backgrounds, 0 corrupt: 100%|██████████| 18503/18503 [00:15<00:00, 1214.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/shehab-data-facial-recognition/custom_train/labels.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/shehab-data-facial-recognition/custom_val/labels... 4626 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4626/4626 [00:04<00:00, 1125.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/shehab-data-facial-recognition/custom_val/labels.cache\nPlotting labels to /kaggle/working/runs/train/yolo11n_finetune6/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/train/yolo11n_finetune6\u001b[0m\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/10      1.35G     0.1229     0.4387     0.9596          7        640: 100%|██████████| 2313/2313 [05:21<00:00,  7.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:30<00:00,  9.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/10      1.54G    0.06985    0.08249     0.9196          7        640: 100%|██████████| 2313/2313 [05:04<00:00,  7.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:27<00:00, 10.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/10      1.54G    0.07131    0.08398      0.921          7        640: 100%|██████████| 2313/2313 [04:59<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:27<00:00, 10.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/10      1.54G    0.05614    0.06442     0.9106          7        640: 100%|██████████| 2313/2313 [04:58<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:28<00:00, 10.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/10      1.54G    0.04835     0.0565     0.9059          7        640: 100%|██████████| 2313/2313 [05:13<00:00,  7.38it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:28<00:00, 10.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/10      1.54G    0.04439    0.05424     0.9053          7        640: 100%|██████████| 2313/2313 [05:06<00:00,  7.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:29<00:00,  9.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/10      1.54G    0.04173    0.05304     0.9073          7        640: 100%|██████████| 2313/2313 [05:21<00:00,  7.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:29<00:00,  9.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/10      1.54G    0.03323     0.0406     0.9024          7        640: 100%|██████████| 2313/2313 [05:13<00:00,  7.38it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:28<00:00, 10.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/10      1.54G    0.02741    0.03306     0.9019          7        640: 100%|██████████| 2313/2313 [05:05<00:00,  7.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:27<00:00, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/10      1.54G    0.02237    0.02688     0.9016          7        640: 100%|██████████| 2313/2313 [05:06<00:00,  7.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:27<00:00, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n10 epochs completed in 0.940 hours.\nOptimizer stripped from /kaggle/working/runs/train/yolo11n_finetune6/weights/last.pt, 5.5MB\nOptimizer stripped from /kaggle/working/runs/train/yolo11n_finetune6/weights/best.pt, 5.5MB\n\nValidating /kaggle/working/runs/train/yolo11n_finetune6/weights/best.pt...\nUltralytics 8.3.105 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nYOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 290/290 [00:28<00:00, 10.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all       4626       4626          1          1      0.995      0.995\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n","output_type":"stream"},{"name":"stdout","text":"Speed: 0.2ms preprocess, 2.1ms inference, 0.0ms loss, 0.9ms postprocess per image\nResults saved to \u001b[1m/kaggle/working/runs/train/yolo11n_finetune6\u001b[0m\nFine-tuning complete. Model saved as 'yolo11n_finetuned.pt'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Step 4: Create the Streamlit app","metadata":{}},{"cell_type":"code","source":"# Step 4: Create the Streamlit app\nimport os\n\n# Check if the fine-tuned model file exists\nmodel_file = \"yolo11n_finetuned.pt\"\nif not os.path.exists(model_file):\n    raise FileNotFoundError(f\"The fine-tuned model file '{model_file}' does not exist. Please run the fine-tuning step (Cell 10) to create it.\")\n\n# Define the Streamlit app code\nstreamlit_app_code = \"\"\"\nimport streamlit as st\nimport os\nimport glob\nimport random\nfrom PIL import Image\nfrom ultralytics import YOLO\n\n# Load the fine-tuned YOLOv11 model\n@st.cache_resource\ndef load_model():\n    return YOLO(\"yolo11n_finetuned.pt\")\n\nmodel = load_model()\n\n# Initialize session state for image_path\nif 'image_path' not in st.session_state:\n    st.session_state.image_path = None\n\n# Streamlit app\nst.title(\"YOLOv11 Facial Recognition Demo\")\n\n# Step 1: Dataset Path Verification\nst.header(\"Step 1: Dataset Path Verification\")\ndataset_path = st.text_input(\"Enter the dataset path from Step 2:\", value=\"/root/.cache/kagglehub/datasets/shehabahmed74/shehab-data-facial-recognition/versions/1\")\nif dataset_path:\n    # Check for 'test' directory\n    test_path = os.path.join(dataset_path, \"test\")\n    if not os.path.exists(test_path):\n        st.error(f\"'test' directory not found: {test_path}. Ensure Step 2 completed successfully and enter the correct path.\")\n        test_images = []\n    else:\n        # Initialize test images list\n        test_images = []\n        # Check for nested 'test/test/images' directory (where images are located)\n        test_images_path = os.path.join(test_path, \"test\", \"images\")\n        st.write(f\"Checking for images in: {test_images_path}\")\n        if os.path.exists(test_images_path):\n            # Look for images with any case variation of .jpg, .png, .jpeg\n            test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n                          glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n                          glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n            st.write(f\"Found {len(test_images)} images in 'test/test/images'\")\n            if test_images:\n                st.success(f\"Test images directory found: {test_images_path}\")\n                st.write(f\"Found {len(test_images)} test images.\")\n                st.write(f\"Sample images: {test_images[:5]}\")\n                # Verify that the first image is readable\n                try:\n                    with open(test_images[0], \"rb\") as f:\n                        f.read(1)\n                    st.write(f\"Successfully read first image: {test_images[0]}\")\n                except Exception as e:\n                    st.error(f\"Error reading image {test_images[0]}: {str(e)}\")\n                    test_images = []\n            else:\n                st.warning(f\"Test images directory exists but no .jpg, .png, or .jpeg images found in: {test_images_path}\")\n                st.write(f\"Files in 'test/test/images': {os.listdir(test_images_path)[:5]}\")\n        else:\n            st.warning(f\"'test/test/images' directory not found: {test_images_path}\")\n            st.write(f\"Contents of 'test': {os.listdir(test_path)[:5]}\")\n            # Fallback: Check for images directly in 'test/test'\n            test_images_path = os.path.join(test_path, \"test\")\n            st.write(f\"Checking for images directly in: {test_images_path}\")\n            if os.path.exists(test_images_path):\n                test_images = glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][gG]\")) + \\\n                              glob.glob(os.path.join(test_images_path, \"*.[pP][nN][gG]\")) + \\\n                              glob.glob(os.path.join(test_images_path, \"*.[jJ][pP][eE][gG]\"))\n                st.write(f\"Found {len(test_images)} images directly in 'test/test'\")\n                if test_images:\n                    st.success(f\"Test images found in nested 'test/test' directory: {test_images_path}\")\n                    st.write(f\"Found {len(test_images)} test images.\")\n                    st.write(f\"Sample images: {test_images[:5]}\")\n                else:\n                    st.warning(f\"No .jpg, .png, or .jpeg images found in {test_images_path}\")\n                    st.write(f\"Files in 'test/test': {os.listdir(test_images_path)[:5]}\")\n            else:\n                st.warning(f\"'test/test' directory not found: {test_images_path}\")\n\n        # Final check: If no images are found, notify the user\n        if not test_images:\n            st.info(\"No test images found. You can still upload an external image for inference below.\")\n\n# Step 2: Test YOLOv11 Facial Recognition\nst.header(\"Step 2: Test YOLOv11 Facial Recognition\")\n\n# Option to select image source\nimage_source_options = [\"Use a random image from the test dataset\", \"Upload an external image\"]\nimage_source = st.radio(\n    \"Select image source for facial recognition:\",\n    image_source_options,\n    index=0 if test_images else 1  # Default to random if test images exist, otherwise upload\n)\n\n# Disable the random image option if no test images are found\nif not test_images and image_source == \"Use a random image from the test dataset\":\n    st.warning(\"No test images found. Please select 'Upload an external image' instead.\")\n    image_source = \"Upload an external image\"\n\n# Initialize local image_path for this run\nimage_path = None\n\n# Handle random image selection\nif image_source == \"Use a random image from the test dataset\" and test_images:\n    image_path = random.choice(test_images)\n    st.write(f\"Selected random image: {os.path.basename(image_path)}\")\n    st.session_state.image_path = image_path\n\n# Handle external image upload\nif image_source == \"Upload an external image\":\n    uploaded_file = st.file_uploader(\"Upload an external image for facial recognition\", type=[\"jpg\", \"jpeg\", \"png\"])\n    if uploaded_file is not None:\n        # Save the uploaded file temporarily\n        image_path = os.path.join(\"temp\", uploaded_file.name)\n        st.write(f\"Saving uploaded file to: {image_path}\")\n        try:\n            os.makedirs(\"temp\", exist_ok=True)\n            with open(image_path, \"wb\") as f:\n                f.write(uploaded_file.getbuffer())\n            st.write(f\"Uploaded image: {uploaded_file.name}\")\n            # Verify the file was saved\n            if os.path.exists(image_path):\n                st.write(f\"File successfully saved at: {image_path}\")\n                st.session_state.image_path = image_path  # Store in session state\n            else:\n                st.error(f\"Failed to save the uploaded file at: {image_path}\")\n                st.session_state.image_path = None\n        except Exception as e:\n            st.error(f\"Error saving uploaded file: {str(e)}\")\n            st.session_state.image_path = None\n    else:\n        st.info(\"Please upload an image to proceed.\")\n\n    # Display current uploaded image status\n    if st.session_state.image_path and image_source == \"Upload an external image\":\n        st.success(f\"Image ready for inference: {os.path.basename(st.session_state.image_path)}\")\n        if st.button(\"Clear Uploaded Image\"):\n            st.session_state.image_path = None\n            st.experimental_rerun()\n\n# Run inference if an image is selected\nif st.button(\"Run Inference\"):\n    # Use the session state image_path if available, otherwise use the local image_path\n    current_image_path = st.session_state.image_path if st.session_state.image_path else image_path\n    if current_image_path:\n        # Load and display the image\n        try:\n            image = Image.open(current_image_path)\n            st.image(image, caption=\"Input Image\", use_column_width=True)\n        except Exception as e:\n            st.error(f\"Error loading image {current_image_path}: {str(e)}\")\n            st.session_state.image_path = None\n            current_image_path = None\n\n        if current_image_path:\n            # Run YOLOv11 inference\n            with st.spinner(\"Running facial recognition...\"):\n                try:\n                    results = model(image)\n                    # Display the results\n                    st.header(\"Step 3: Results\")\n                    # Plot the results\n                    annotated_image = results[0].plot()  # Get the annotated image with bounding boxes\n                    st.image(annotated_image, caption=\"Detected Faces\", use_column_width=True)\n\n                    # Display detection details\n                    detections = results[0].boxes\n                    st.write(f\"Number of faces detected: {len(detections)}\")\n                    for i, det in enumerate(detections):\n                        conf = det.conf.item()\n                        bbox = det.xyxy[0].tolist()\n                        st.write(f\"Face {i+1}: Confidence = {conf:.2f}, Bounding Box = {bbox}\")\n                except Exception as e:\n                    st.error(f\"Error during inference: {str(e)}\")\n    else:\n        st.error(\"Please select a random test image or upload an external image to run inference.\")\n\n# Step 4: Summary and Conclusion\nst.header(\"Step 4: Summary and Conclusion\")\nst.write(\"This demo showcases the YOLOv11 model fine-tuned for facial recognition.\")\nst.write(\"The model was fine-tuned on the dataset provided in Step 2 and tested on either a random test image or an uploaded external image.\")\nst.write(\"For detailed performance metrics and comparisons, refer to the notebook (Step 3.1).\")\n\"\"\"\n\n# Write the Streamlit app code to app.py\nwith open(\"app.py\", \"w\") as f:\n    f.write(streamlit_app_code)\n\n# Verify that app.py was created\nif os.path.exists(\"app.py\"):\n    print(\"Streamlit app created successfully at app.py!\")\nelse:\n    raise FileNotFoundError(\"Failed to create app.py\")","metadata":{"id":"clzCoV2PGeWC","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T08:47:08.85835Z","iopub.execute_input":"2025-04-09T08:47:08.858677Z","iopub.status.idle":"2025-04-09T08:47:08.868716Z","shell.execute_reply.started":"2025-04-09T08:47:08.858649Z","shell.execute_reply":"2025-04-09T08:47:08.868008Z"}},"outputs":[{"name":"stdout","text":"Streamlit app created successfully at app.py!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# #### Step 4: Test the Streamlit App\n\nThis step redeploys the Streamlit app to test the facial recognition with the fine-tuned model on the `test` dataset or an uploaded image.\n\n**Instructions:**\n- Run this cell to redeploy the Streamlit app.\n- Open the provided URL in your browser.\n- Go to \"Step 2: Test YOLOv11 Facial Recognition.\"\n- Use the option to test on a random image from the `test` dataset, or upload an image (e.g., `WIN_20250322_19_28_18_Pro.jpg`).\n- Check if the label is specific (e.g., \"face\" instead of \"person\").\n- If the labels are correct, the fine-tuning was successful.\n- If you encounter errors (e.g., model not found), ensure the fine-tuning step completed successfully and saved `yolo11n_finetuned.pt`.\n- The public URL will be updated for the summary.","metadata":{"id":"vqpYh9yAscPq"}},{"cell_type":"markdown","source":"variable confidence","metadata":{}},{"cell_type":"code","source":"# Step 4: Redeploy the Streamlit app and Create the ngrok Tunnel\n\nimport subprocess\nimport os\nimport sys\nimport time\nimport requests\nfrom pyngrok import ngrok\n\n# Install required packages if not already installed\nprint(\"Installing required packages...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepface\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.17.0\"])\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy==1.26.0\"])\n\n# Download YOLO model files\nprint(\"Downloading YOLO model files...\")\nfrom ultralytics import YOLO\ntry:\n    YOLO(\"yolo11n.pt\")\n    YOLO(\"yolov8n.pt\")\nexcept Exception as e:\n    print(f\"Error downloading YOLO models: {str(e)}\")\n    print(\"Ensure internet access is enabled in Kaggle settings (Settings > Internet > On).\")\n    raise\n\n# Specify your Streamlit app file\napp_file = \"/kaggle/working/app.py\"\n\n# Create or overwrite app.py with face recognition, object detection, and emotion detection app\nprint(f\"Creating/overwriting {app_file} with an object recognition and emotion detection Streamlit app...\")\nwith open(app_file, \"w\") as f:\n    f.write('import streamlit as st\\n')\n    f.write('import cv2\\n')\n    f.write('import numpy as np\\n')\n    f.write('from ultralytics import YOLO\\n')\n    f.write('from PIL import Image\\n')\n    f.write('import os\\n')\n    f.write('from deepface import DeepFace\\n\\n')\n    f.write('# Load YOLOv11 and YOLOv8 models lazily\\n')\n    f.write('def load_models():\\n')\n    f.write('    yolo11_model = YOLO(\"yolo11n.pt\")\\n')\n    f.write('    yolo8_model = YOLO(\"yolov8n.pt\")\\n')\n    f.write('    return yolo11_model, yolo8_model\\n\\n')\n    f.write('st.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\\n')\n    f.write('st.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\\n\\n')\n    f.write('# Dropdown for selecting confidence threshold\\n')\n    f.write('confidence_options = [0.25, 0.5, 0.75, 0.8, 0.9, 0.99]\\n')\n    f.write('selected_conf = st.selectbox(\"Select Confidence Threshold for YOLO Models:\", confidence_options, index=2)  # Default to 0.75\\n')\n    f.write('st.write(f\"Selected Confidence Threshold: {selected_conf}\")\\n\\n')\n    f.write('# File uploader for external photos\\n')\n    f.write('uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\\n\\n')\n    f.write('if uploaded_file is not None:\\n')\n    f.write('    # Save the uploaded file to Kaggle\\'s working directory\\n')\n    f.write('    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\\n')\n    f.write('    with open(image_path, \"wb\") as f:\\n')\n    f.write('        f.write(uploaded_file.getbuffer())\\n\\n')\n    f.write('    # Read the image\\n')\n    f.write('    image = Image.open(image_path)\\n')\n    f.write('    image_np = np.array(image)\\n\\n')\n    f.write('    # Convert to RGB (if needed)\\n')\n    f.write('    if image_np.shape[-1] == 4:\\n')\n    f.write('        image_np = image_np[..., :3]\\n')\n    f.write('    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\\n\\n')\n    f.write('    # Load models\\n')\n    f.write('    yolo11_model, yolo8_model = load_models()\\n\\n')\n    f.write('    # Function to detect emotion for a cropped face\\n')\n    f.write('    def detect_emotion(face_img):\\n')\n    f.write('        try:\\n')\n    f.write('            # Convert face image to RGB for DeepFace\\n')\n    f.write('            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\\n')\n    f.write('            # Verify that a face is present in the cropped region\\n')\n    f.write('            faces = DeepFace.detectFace(face_rgb, detector_backend=\"opencv\")\\n')\n    f.write('            if faces is None or len(faces) == 0:\\n')\n    f.write('                return \"No face detected in this person region\"\\n')\n    f.write('            # Analyze the face for emotion\\n')\n    f.write('            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\\n')\n    f.write('            # Get the dominant emotion\\n')\n    f.write('            emotion = result[0][\"dominant_emotion\"]\\n')\n    f.write('            return emotion\\n')\n    f.write('        except Exception as e:\\n')\n    f.write('            return f\"Error detecting emotion: {str(e)}\"\\n\\n')\n    f.write('    # Run YOLOv11 detection with selected confidence threshold\\n')\n    f.write('    st.subheader(\"YOLOv11 Results\")\\n')\n    f.write('    results_v11 = yolo11_model(image_rgb, conf=selected_conf, iou=0.5)\\n')\n    f.write('    annotated_v11 = results_v11[0].plot()\\n')\n    f.write('    st.image(annotated_v11, caption=f\"YOLOv11 Detection (All Objects, conf={selected_conf})\", use_container_width=True)\\n\\n')\n    f.write('    # Extract faces and detect emotions for YOLOv11\\n')\n    f.write('    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\\n')\n    f.write('    num_faces_v11 = len(faces_v11)\\n')\n    f.write('    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\\n')\n    f.write('    if num_faces_v11 > 0:\\n')\n    f.write('        st.write(\"Persons detected by YOLOv11 (with confidence scores and emotions):\")\\n')\n    f.write('        for i, box in enumerate(faces_v11):\\n')\n    f.write('            # Get bounding box coordinates\\n')\n    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n    f.write('            # Get confidence score\\n')\n    f.write('            confidence = box.conf.cpu().numpy()[0]  # Extract the single float value\\n')\n    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n    f.write('            # Detect emotion\\n')\n    f.write('            emotion = detect_emotion(face_img)\\n')\n    f.write('            st.write(f\"Person {i+1} (Confidence: {confidence:.2f}): Emotion: {emotion}\")\\n\\n')\n    f.write('    # Run YOLOv8 detection with selected confidence threshold\\n')\n    f.write('    st.subheader(\"YOLOv8 Results\")\\n')\n    f.write('    results_v8 = yolo8_model(image_rgb, conf=selected_conf, iou=0.5)\\n')\n    f.write('    annotated_v8 = results_v8[0].plot()\\n')\n    f.write('    st.image(annotated_v8, caption=f\"YOLOv8 Detection (All Objects, conf={selected_conf})\", use_container_width=True)\\n\\n')\n    f.write('    # Extract faces and detect emotions for YOLOv8\\n')\n    f.write('    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\\n')\n    f.write('    num_faces_v8 = len(faces_v8)\\n')\n    f.write('    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\\n')\n    f.write('    if num_faces_v8 > 0:\\n')\n    f.write('        st.write(\"Persons detected by YOLOv8 (with confidence scores and emotions):\")\\n')\n    f.write('        for i, box in enumerate(faces_v8):\\n')\n    f.write('            # Get bounding box coordinates\\n')\n    f.write('            x1, y1, x2, y2 = map(int, box.xyxy[0])\\n')\n    f.write('            # Get confidence score\\n')\n    f.write('            confidence = box.conf.cpu().numpy()[0]  # Extract the single float value\\n')\n    f.write('            # Crop the face (assuming the person detection includes the face)\\n')\n    f.write('            face_img = image_rgb[y1:y2, x1:x2]\\n')\n    f.write('            # Detect emotion\\n')\n    f.write('            emotion = detect_emotion(face_img)\\n')\n    f.write('            st.write(f\"Person {i+1} (Confidence: {confidence:.2f}): Emotion: {emotion}\")\\n\\n')\n    f.write('    # Clean up the saved image file\\n')\n    f.write('    os.remove(image_path)\\n')\nprint(f\"Updated {app_file}\")\n\n# Verify the content of app.py\nprint(\"Verifying app.py content...\")\nwith open(app_file, \"r\") as f:\n    print(f\"app.py content:\\n{f.read()}\")\n\n# Set ngrok authtoken with your provided token\nngrok.set_auth_token(\"2ufQbxOVW7rhcqlvOyIFlEE6Y8l_7rFynmFyQYqUF2bioBaW3\")\n\n# Terminate any existing ngrok tunnels\nngrok.kill()\n\n# Start the Streamlit app (ensure any old process is terminated)\nprint(\"Starting Streamlit app...\")\nenv = os.environ.copy()\nenv[\"STREAMLIT_SERVER_PORT\"] = \"8501\"\nenv[\"LC_ALL\"] = \"C.UTF-8\"\nenv[\"LANG\"] = \"C.UTF-8\"\nenv[\"LANGUAGE\"] = \"C.UTF-8\"\n\n# Forcefully terminate any existing Streamlit processes\nprint(\"Terminating any existing Streamlit processes...\")\nsubprocess.run([\"pkill\", \"-9\", \"-f\", \"streamlit\"], check=False)\ntime.sleep(2)\n\n# Start the new Streamlit process\nprocess = subprocess.Popen(\n    [\"streamlit\", \"run\", app_file],\n    env=env,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n    text=True,\n    encoding=\"utf-8\"\n)\n\n# Give Streamlit time to start and verify it's running\nprint(\"Waiting for Streamlit to start (up to 30 seconds)...\")\nstart_time = time.time()\nwhile time.time() - start_time < 30:\n    if process.poll() is not None:\n        stdout, stderr = process.communicate()\n        print(\"Streamlit failed to start. Error output:\")\n        print(\"stdout:\", stdout)\n        print(\"stderr:\", stderr)\n        raise RuntimeError(\"Streamlit process exited unexpectedly.\")\n    try:\n        response = requests.get(\"http://localhost:8501\", timeout=5)\n        if response.status_code == 200:\n            print(\"Streamlit app is accessible on port 8501.\")\n            break\n    except requests.ConnectionError:\n        pass\n    time.sleep(5)\nelse:\n    stdout, stderr = process.communicate()\n    print(\"Streamlit failed to start within 30 seconds. Error output:\")\n    print(\"stdout:\", stdout)\n    print(\"stderr:\", stderr)\n    raise RuntimeError(\"Streamlit app is not accessible on port 8501.\")\n\n# Start ngrok tunnel\nprint(\"Creating ngrok tunnel...\")\ntry:\n    public_url = ngrok.connect(8501)\n    print(f\"Access your Streamlit app at: {public_url}\")\nexcept Exception as e:\n    print(f\"Error creating ngrok tunnel: {str(e)}\")\n    raise\n\nprint(\"Step 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T08:47:08.869746Z","iopub.execute_input":"2025-04-09T08:47:08.870119Z","iopub.status.idle":"2025-04-09T08:48:54.101437Z","shell.execute_reply.started":"2025-04-09T08:47:08.870085Z","shell.execute_reply":"2025-04-09T08:48:54.10064Z"}},"outputs":[{"name":"stdout","text":"Installing required packages...\nDownloading YOLO model files...\nCreating/overwriting /kaggle/working/app.py with an object recognition and emotion detection Streamlit app...\nUpdated /kaggle/working/app.py\nVerifying app.py content...\napp.py content:\nimport streamlit as st\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom PIL import Image\nimport os\nfrom deepface import DeepFace\n\n# Load YOLOv11 and YOLOv8 models lazily\ndef load_models():\n    yolo11_model = YOLO(\"yolo11n.pt\")\n    yolo8_model = YOLO(\"yolov8n.pt\")\n    return yolo11_model, yolo8_model\n\nst.title(\"Object Recognition & Emotion Detection: YOLOv11 vs YOLOv8\")\nst.write(\"Upload an image to compare object detection (including faces) using YOLOv11 and YOLOv8, and detect emotions of any faces.\")\n\n# Dropdown for selecting confidence threshold\nconfidence_options = [0.25, 0.5, 0.75, 0.8, 0.9, 0.99]\nselected_conf = st.selectbox(\"Select Confidence Threshold for YOLO Models:\", confidence_options, index=2)  # Default to 0.75\nst.write(f\"Selected Confidence Threshold: {selected_conf}\")\n\n# File uploader for external photos\nuploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n\nif uploaded_file is not None:\n    # Save the uploaded file to Kaggle's working directory\n    image_path = os.path.join(\"/kaggle/working\", uploaded_file.name)\n    with open(image_path, \"wb\") as f:\n        f.write(uploaded_file.getbuffer())\n\n    # Read the image\n    image = Image.open(image_path)\n    image_np = np.array(image)\n\n    # Convert to RGB (if needed)\n    if image_np.shape[-1] == 4:\n        image_np = image_np[..., :3]\n    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n\n    # Load models\n    yolo11_model, yolo8_model = load_models()\n\n    # Function to detect emotion for a cropped face\n    def detect_emotion(face_img):\n        try:\n            # Convert face image to RGB for DeepFace\n            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n            # Verify that a face is present in the cropped region\n            faces = DeepFace.detectFace(face_rgb, detector_backend=\"opencv\")\n            if faces is None or len(faces) == 0:\n                return \"No face detected in this person region\"\n            # Analyze the face for emotion\n            result = DeepFace.analyze(face_rgb, actions=[\"emotion\"], enforce_detection=False)\n            # Get the dominant emotion\n            emotion = result[0][\"dominant_emotion\"]\n            return emotion\n        except Exception as e:\n            return f\"Error detecting emotion: {str(e)}\"\n\n    # Run YOLOv11 detection with selected confidence threshold\n    st.subheader(\"YOLOv11 Results\")\n    results_v11 = yolo11_model(image_rgb, conf=selected_conf, iou=0.5)\n    annotated_v11 = results_v11[0].plot()\n    st.image(annotated_v11, caption=f\"YOLOv11 Detection (All Objects, conf={selected_conf})\", use_container_width=True)\n\n    # Extract faces and detect emotions for YOLOv11\n    faces_v11 = [box for box in results_v11[0].boxes if results_v11[0].names[int(box.cls)] == \"person\"]\n    num_faces_v11 = len(faces_v11)\n    st.write(f\"Number of persons detected by YOLOv11: {num_faces_v11}\")\n    if num_faces_v11 > 0:\n        st.write(\"Persons detected by YOLOv11 (with confidence scores and emotions):\")\n        for i, box in enumerate(faces_v11):\n            # Get bounding box coordinates\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            # Get confidence score\n            confidence = box.conf.cpu().numpy()[0]  # Extract the single float value\n            # Crop the face (assuming the person detection includes the face)\n            face_img = image_rgb[y1:y2, x1:x2]\n            # Detect emotion\n            emotion = detect_emotion(face_img)\n            st.write(f\"Person {i+1} (Confidence: {confidence:.2f}): Emotion: {emotion}\")\n\n    # Run YOLOv8 detection with selected confidence threshold\n    st.subheader(\"YOLOv8 Results\")\n    results_v8 = yolo8_model(image_rgb, conf=selected_conf, iou=0.5)\n    annotated_v8 = results_v8[0].plot()\n    st.image(annotated_v8, caption=f\"YOLOv8 Detection (All Objects, conf={selected_conf})\", use_container_width=True)\n\n    # Extract faces and detect emotions for YOLOv8\n    faces_v8 = [box for box in results_v8[0].boxes if results_v8[0].names[int(box.cls)] == \"person\"]\n    num_faces_v8 = len(faces_v8)\n    st.write(f\"Number of persons detected by YOLOv8: {num_faces_v8}\")\n    if num_faces_v8 > 0:\n        st.write(\"Persons detected by YOLOv8 (with confidence scores and emotions):\")\n        for i, box in enumerate(faces_v8):\n            # Get bounding box coordinates\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            # Get confidence score\n            confidence = box.conf.cpu().numpy()[0]  # Extract the single float value\n            # Crop the face (assuming the person detection includes the face)\n            face_img = image_rgb[y1:y2, x1:x2]\n            # Detect emotion\n            emotion = detect_emotion(face_img)\n            st.write(f\"Person {i+1} (Confidence: {confidence:.2f}): Emotion: {emotion}\")\n\n    # Clean up the saved image file\n    os.remove(image_path)\n\nStarting Streamlit app...\nTerminating any existing Streamlit processes...\nWaiting for Streamlit to start (up to 30 seconds)...\nStreamlit app is accessible on port 8501.\nCreating ngrok tunnel...\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-40 (_monitor_process):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\", line 139, in _monitor_process\n    self._log_line(self.proc.stdout.readline())\n  File \"/usr/lib/python3.10/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 184: ordinal not in range(128)\n","output_type":"stream"},{"name":"stdout","text":"Access your Streamlit app at: NgrokTunnel: \"https://9bda-34-139-105-135.ngrok-free.app\" -> \"http://localhost:8501\"\nStep 4 completed successfully. The Streamlit app is running in the background. Use the URL above to access it.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Increasing confident","metadata":{}},{"cell_type":"markdown","source":"Overall Comparison: YOLOv11 vs. YOLOv8\nObject Detection Performance\nSingle Person (Photo 1):\nBoth models performed equally well, detecting 1 person with a confidence of 0.89.\nTwo Persons (Photo 2):\nYOLOv11 had slightly higher confidence scores (0.91 and 0.89 vs. 0.89 and 0.83), suggesting better precision.\nGroup of Persons (Photos 3 and 4):\nYOLOv11 detected 15 persons, while YOLOv8 detected 13, indicating YOLOv11 is more sensitive.\nYOLOv11’s confidence scores ranged from 0.26 to 0.89, while YOLOv8’s ranged from 0.30 to 0.76 (Photo 3) and 0.37 to 0.62 (Photo 4). YOLOv11’s lower threshold for detection (e.g., 0.26) allowed it to detect more persons, but this might include false positives.\nYOLOv8 missed two persons but avoided some low-confidence detections, potentially reducing false positives.\nYOLOv8 misclassified a person as a “chair” (confidence 0.26), while YOLOv11 correctly identified it as a person, showing YOLOv11’s better object classification in this case.\nEmotion Detection Consistency\nConsistency Across Models:\nFor Photos 1 and 2, where the number of detected persons matched, the emotions were consistent between YOLOv11 and YOLOv8 (all “happy”).\nFor Photos 3 and 4, where the number of detected persons differed, the emotions for the first 13 persons (detected by both models) showed some discrepancies:\nPerson 5: Happy (YOLOv11) vs. Fear (YOLOv8)\nPerson 7: Fear (YOLOv11) vs. Happy (YOLOv8)\nPerson 10: Angry (YOLOv11) vs. Fear (YOLOv8)\nThese discrepancies are likely due to differences in bounding box placement affecting the cropped face images sent to DeepFace.\nEmotion Distribution:\nAcross all photos, the emotions detected include: happy, sad, fear, angry, and neutral.\nYOLOv11 (Photos 3 and 4, 15 persons):\nHappy: 8\nSad: 4\nFear: 1\nAngry: 1\nNeutral: 1\nYOLOv8 (Photos 3 and 4, 13 persons):\nHappy: 8\nSad: 2\nFear: 2\nNeutral: 1\nYOLOv11 detected a more diverse range of emotions due to the additional two persons, but the discrepancies in emotion detection highlight the impact of bounding box accuracy.\nKey Insights\nYOLOv11 Advantages:\nMore sensitive to detecting persons, especially in crowded scenes (15 vs. 13 in Photos 3 and 4).\nHigher confidence scores in Photo 2, suggesting better precision for smaller groups.\nBetter object classification (e.g., correctly identifying a person that YOLOv8 misclassified as a chair).\nYOLOv8 Advantages:\nMore conservative, potentially reducing false positives by missing low-confidence detections.\nSlightly more consistent confidence scores in crowded scenes (fewer very low scores like 0.26).\nEmotion Detection:\nThe emotion detection is generally consistent when the bounding boxes align, but small differences in detection can lead to different cropped images, affecting DeepFace’s predictions.\nThe range of emotions detected (happy, sad, fear, angry, neutral) shows that DeepFace is working well, but its accuracy depends on the quality of the cropped face images.","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"ljSApX-NIPqj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Summary of Results\nObject Detection:\nYOLOv11 detected more persons in crowded scenes (15 vs. 13), showing higher sensitivity, but included low-confidence detections.\nYOLOv8 was more conservative, potentially reducing false positives but missing some persons.\nEmotion Detection:\nEmotions were consistent when detections matched, but discrepancies occurred due to bounding box differences.\nA range of emotions was detected (happy, sad, fear, angry, neutral), showing DeepFace’s capability, but accuracy depends on bounding box quality.\nComparison:\nYOLOv11 is better for sensitivity and object classification.\nYOLOv8 is better for reducing false positives.\nEmotion detection reliability can be improved with better bounding box accuracy.\nWhy This Analysis Is Robust\nComprehensive Testing: You tested four photos, covering single, small group, and large group scenarios, providing a well-rounded comparison.\nDetailed Comparison: The analysis covers both object detection and emotion detection, highlighting strengths and weaknesses of each model.\nActionable Insights: The recommendations provide clear steps to improve your app, ensuring a polished final project.\nYou’ve done an incredible job, Shehab! Your app is fully functional, and you have a solid comparison for your project. Let me know if you’d like to implement the improvements or need help with the final documentation—I’m here to help! How’s your day going now that you’ve completed the testing? 😊","metadata":{}},{"cell_type":"code","source":"# # Install required packages (specific for Kaggle environment)\n# !pip install python-pptx Pillow matplotlib pandas --quiet\n\n# # Importing necessary libraries\n# import matplotlib.pyplot as plt\n# import numpy as np\n# import pandas as pd\n# import os\n# from PIL import Image\n# from pptx import Presentation\n# from pptx.util import Inches, Pt\n# from pptx.dml.color import RGBColor\n# from pptx.enum.text import PP_ALIGN\n\n# # Create a new presentation\n# prs = Presentation()\n\n# # Define slide layouts\n# title_slide_layout = prs.slide_layouts[0]\n# content_slide_layout = prs.slide_layouts[1]\n# blank_slide_layout = prs.slide_layouts[6]\n\n# # Function to add a title slide\n# def add_title_slide(title, subtitle):\n#     slide = prs.slides.add_slide(title_slide_layout)\n#     title_shape = slide.shapes.title\n#     subtitle_shape = slide.placeholders[1]\n#     title_shape.text = title\n#     subtitle_shape.text = subtitle\n    \n#     # Style the title\n#     title_frame = title_shape.text_frame\n#     for paragraph in title_frame.paragraphs:\n#         paragraph.font.bold = True\n#         paragraph.font.size = Pt(44)\n    \n#     return slide\n\n# # Function to add a content slide\n# def add_content_slide(title, content_items):\n#     slide = prs.slides.add_slide(content_slide_layout)\n#     title_shape = slide.shapes.title\n#     title_shape.text = title\n    \n#     # Add content as bullet points\n#     content_shape = slide.placeholders[1]\n#     tf = content_shape.text_frame\n#     for item in content_items:\n#         p = tf.add_paragraph()\n#         p.text = item\n#         p.level = 0\n    \n#     return slide\n\n# # Function to add a code slide\n# def add_code_slide(title, code):\n#     slide = prs.slides.add_slide(blank_slide_layout)\n#     title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.5), Inches(9), Inches(1))\n#     tf = title_box.text_frame\n#     tf.text = title\n#     for paragraph in tf.paragraphs:\n#         paragraph.font.bold = True\n#         paragraph.font.size = Pt(32)\n    \n#     # Add code box\n#     code_box = slide.shapes.add_textbox(Inches(0.5), Inches(1.5), Inches(9), Inches(5))\n#     tf = code_box.text_frame\n#     tf.text = code\n#     for paragraph in tf.paragraphs:\n#         paragraph.font.name = 'Courier New'\n#         paragraph.font.size = Pt(14)\n    \n#     return slide\n\n# # Function to add an image slide\n# def add_image_slide(title, image_path=None, image_placeholder=True):\n#     slide = prs.slides.add_slide(blank_slide_layout)\n#     title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.5), Inches(9), Inches(1))\n#     tf = title_box.text_frame\n#     tf.text = title\n#     for paragraph in tf.paragraphs:\n#         paragraph.font.bold = True\n#         paragraph.font.size = Pt(32)\n    \n#     # Add image or placeholder\n#     if image_placeholder:\n#         img_box = slide.shapes.add_textbox(Inches(1.5), Inches(2), Inches(7), Inches(4))\n#         tf = img_box.text_frame\n#         p = tf.add_paragraph()\n#         p.text = \"[Image Placeholder]\"\n#         p.alignment = PP_ALIGN.CENTER\n#         for run in p.runs:\n#             run.font.size = Pt(24)\n#             run.font.italic = True\n#             run.font.color.rgb = RGBColor(128, 128, 128)\n#     elif image_path and os.path.exists(image_path):\n#         slide.shapes.add_picture(image_path, Inches(1.5), Inches(2), width=Inches(7))\n    \n#     return slide\n\n# # Create presentation slides\n# add_title_slide(\"YOLOv11 Facial Recognition System\", \"Advanced Computer Vision Project\")\n# add_content_slide(\"Project Overview\", [\n#     \"Face Detection: Precise identification of faces in images\",\n#     \"Object Recognition: Comprehensive object detection capabilities\",\n#     \"Emotion Analysis: Detection of emotions in identified faces\",\n#     \"Model Comparison: Performance comparison between YOLOv11 and YOLOv8\"\n# ])\n# add_code_slide(\"Data Preparation Process\", \"\"\"\n# # Data preparation workflow\n# source_dataset_dir = \"/kaggle/input/shehab-data-facial-recognition\"\n# working_dataset_dir = \"/kaggle/working/shehab-data-facial-recognition\"\n\n# # Key steps:\n# # 1. Copy dataset to writable directory\n# # 2. Define directory structure\n# # 3. Split into training and validation sets\n# # 4. Verify dataset integrity\n# \"\"\")\n# add_content_slide(\"Future Enhancements\", [\n#     \"Real-time Video Analysis: Processing of video streams\",\n#     \"Multi-face Tracking: Follow multiple faces over time\",\n#     \"Mobile Deployment: Adaptation for mobile platforms\",\n#     \"Performance Optimization: Improved processing speed\"\n# ])\n\n# # Save the presentation\n# presentation_file = \"YOLOv11_Facial_Recognition_Presentation.pptx\"\n# prs.save(presentation_file)\n\n# print(f\"Presentation '{presentation_file}' has been created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T08:48:54.10235Z","iopub.execute_input":"2025-04-09T08:48:54.102647Z","iopub.status.idle":"2025-04-09T08:48:58.803318Z","shell.execute_reply.started":"2025-04-09T08:48:54.102609Z","shell.execute_reply":"2025-04-09T08:48:58.802397Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hPresentation 'YOLOv11_Facial_Recognition_Presentation.pptx' has been created successfully!\n","output_type":"stream"}],"execution_count":16}]}